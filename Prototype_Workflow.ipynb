{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6c0ebc12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/huanxing/Documents/GitHub/Analogical-LLM/analogical-lc-env/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9f00062d-0f67-46b5-aca0-b7857d11c4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing import TypedDict\n",
    "import openai \n",
    "# from langgraph_supervisor import create_supervisor\n",
    "from langchain.chat_models import init_chat_model\n",
    "from textwrap import dedent\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c9c75eb6-69bf-40c9-8ed4-8b96c4d1f285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts\n",
    "TARGET_DOMAIN = dedent(\"\"\"\n",
    "        As a Domain Analysis Specialist, extract all the core innovation domains from the user query. It could be a single one for a simple query, or multiple ones for a complex query.\n",
    "        Instructions:\n",
    "        1. Analyze the user's input\n",
    "        2. Identify the primary domain(s) requiring innovation\n",
    "        3. Classify it within standard innovation categories\n",
    "        Output Format:\n",
    "        Target Domain: [Clear, specific domain label]\n",
    "        Be very detailed and specific in your response and do not generalize. Respond ONLY with the name of the domain, do NOT include ANY other text like 'Target Domain:'.\n",
    "        User query: {user_query}\n",
    "\"\"\").strip()\n",
    "\n",
    "PROBLEM_LANDSCAPE = dedent(\"\"\"\n",
    "        You are a Problem Landscape Analyst. Your task is to map out the concrete challenges within the target domain identified.\n",
    "        Instructions:\n",
    "        1. Identify all the core problems or challenges currently present in these domains. Aim for at least 3 problems per domain\n",
    "        2. For each problem, provide:\n",
    "        - Problem: A short, clear title.\n",
    "        - Description: 2-3 sentences explaining what the problem is and why it matters.\n",
    "        - Context: Briefly state the circumstances or environment where this problem occurs.\n",
    "        - Stakeholders: List the main groups or individuals affected.\n",
    "        - Root Causes: Identify 1-3 underlying causes, if known.\n",
    "        - Impact: State the significance of the problem (e.g., social, economic, technical).\n",
    "        - Current Approaches: How is this problem currently addressed?\n",
    "        - Limitations: What are the shortcomings of current approaches?\n",
    "        - Success Metrics: How would you measure if this problem is solved?\n",
    "        - Interconnections: Note if this problem is linked to or influenced by other problems.\n",
    "        Output Format:\n",
    "        Present your findings as a structured list or JSON array, with each problem fully described as above.\n",
    "        Important:\n",
    "        - Focus on clarity and completeness.\n",
    "        - Avoid abstracting or generalizing; stay concrete and domain-specific.\n",
    "        - Do not propose solutions; only describe the current problem landscape.\n",
    "        Target domain: {target_domain}\n",
    "\"\"\").strip()\n",
    "\n",
    "ABSTRACTION = dedent(\"\"\"\n",
    "You are a TRIZ Methodology Expert. Transform domain-specific problems into universal contradictions.\n",
    "        Process:\n",
    "        1. Read up on TRIZ - the contradiction matrix, and the inventive principles\n",
    "        2. For each problem in the problem landscape:\n",
    "        - Abstract to universal parameters (what improves vs. what worsens)\n",
    "        - Express as 'When we improve X, Y worsens'\n",
    "        - Ensure parameters are domain-agnostic\n",
    "        3. Analyze all the abstracted universal parameters, and identify all the core TRIZ contradictions present:\n",
    "        - Select the most fundamental tensions\n",
    "        - Map to TRIZ contradiction matrix\n",
    "        - Note applicable inventive principles\n",
    "        Output:\n",
    "        # List all the core contradictions in form of:\n",
    "        - Improving [parameter] vs. Worsening [parameter]\n",
    "        - TRIZ Principles: [1-3 relevant principles]\n",
    "        - Innovation Potential: [High/Medium/Low]\n",
    "        Focus on contradictions that, if resolved, would create breakthrough value.\n",
    "\n",
    "        Problem landscape: {problem_landscape}\n",
    "\"\"\").strip()\n",
    "\n",
    "BASE_DOMAIN = dedent(\"\"\"\n",
    "        You are a Cross-Domain Search Specialist. Do the following:\n",
    "        - For each contradiction provided, identify 3 distinct source domains (fields or industries) where this contradiction has been successfully addressed.\n",
    "        - Experiment with different subsets of the list of contradictions, and see if you could identify 3 distinct source domains for each of these subsets identified as well. You should find at least 3 different subsets.\n",
    "        Note: The domains should have A CONCEPTUAL DISTANCE OF AT LEAST 3 DISTINCT HOPS FROM WHAT IMMEDIATELY COMES TO MIND. Be creative! It can be domains within spheres like natural, phsyical, social, artistic, or anything.\n",
    "        For each domain identified, briefly explain why it is relevant to the single contradiction or the subset of contradictions identified. Do not describe specific solutions just yet-only list the domains and your rationale.\n",
    "        Output:\n",
    "        A list for each contradiction and subset of contradictions identified, naming 3 relevant domains with a 2 sentence rationale for each.\n",
    "        Aim for a total of at least 20 relevant base domains. \n",
    "        Contradictions: {contradictions}\n",
    "\"\"\").strip()\n",
    "\n",
    "BASE_SOLUTIONS = dedent(\"\"\"\n",
    "        You are a Solution Pattern Extractor. You are provided with an input with 3 base domains identified per TRIZ (Theory of Inventive Problem Solving) contradiction or a set of contradictions, as well as the contradictions themselves.\n",
    "        For each of these identified base domains, identify one specific, well-documented solution pattern within the domain that effectively resolves the contradiction (or the set of contradictions).\n",
    "        For each solution pattern, return:\n",
    "        - Identify the base domain it's corresponding to\n",
    "        - Recall the contradiction or the set of contradictions that this base domain faces\n",
    "        - The name or label of the solution pattern for resolving these contradiction(s) in the base domain\n",
    "        - A detailed description of the core mechanism or principle involved and how it addressed the domain's contradiction(s)\n",
    "        - The context or situation in the domain where this pattern is applied\\n\"\n",
    "        Do not generalize or adapt the solution-simply describe how the contradiction is addressed within each source domain.\n",
    "        Output:\n",
    "        For each of the provided domain, list the base domain name, contradiction(s) faced, solution pattern name, the detailed description of the mechanism of the solution pattern, and the context in which it is used. Articulate the contradictions as problems and considerations faced, through framing them as a tension.\n",
    "\n",
    "        Input: {input}\n",
    "\"\"\").strip()\n",
    "\n",
    "ANALOGICAL_TRANSFER = dedent(\"\"\"\n",
    "        You are a very innovative Analogical Transfer Specialist.\n",
    "        You are provided with list the base domain name, tensions faced, solution pattern name, the detailed description of the mechanism of the solution pattern, and the context in which it is used.\n",
    "        Your task is to propose how solution patterns used to resolve these tensions in various base domains might inspire solution framings for the original target domain.\n",
    "\n",
    "        Input Overview:\n",
    "        1. A list the base domains identified, the tensions these domains faced, the name of solution patterns that helped addressed these tensions in these base domains, the detailed description of the mechanism of the solution pattern, and the context in which it is used.\n",
    "        2. The original target domain.\n",
    "\n",
    "        Instructions:\n",
    "        For each pair of base domain and the corresponding tensions identified, review the solution patterns that worked for the base domain. For each pattern:\n",
    "        - Analyze the core mechanism or principle behind the solution.\n",
    "        - Map and adapt this mechanism conceptually to the target domain, considering the specific context and needs of the target domain.\n",
    "        - Clearly describe how this analogical transfer could frame a potential solution in the target domain.\n",
    "        - Highlight any key adaptations, considerations, or limitations that would be relevant when applying this pattern to the target domain.\n",
    "\n",
    "        Your expected Output:\n",
    "        For each base domain, provide a comprehensive description of a proposed solution framing for the target domain, including:\n",
    "        - The original tension addressed\n",
    "        - The source domain and solution pattern\n",
    "        - A detailed explanation of how the pattern could inspire or inform a solution in the target domain\n",
    "        - Any important adaptations or considerations for successful transfer\n",
    "\n",
    "        Here are the actualinputs:\n",
    "        - A list the base domains identified, the tensions these domains faced, the name of solution patterns that helped addressed these tensions in these base domains, the detailed description of the mechanism of the solution pattern, and the context in which it is used.: {contradictions_solutions}\n",
    "        - Original target domain: {target_domain}\n",
    "\"\"\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8e573d7e-e43f-4c7d-8752-ae11a6198c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for openai API\n",
    "def _set_if_undefined(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"Please provide your {var}\")\n",
    "\n",
    "\n",
    "_set_if_undefined(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a188a9e3-c538-487f-9dde-8dfe295575db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph state: workflow\n",
    "class ReasoningState(TypedDict):\n",
    "    user_query: str\n",
    "    target_domain: str\n",
    "    problem_landscape: str\n",
    "    abstraction: str\n",
    "    abstraction_feedback: str\n",
    "    base_domain: str\n",
    "    base_solutions: str\n",
    "    analogical_transfer: str\n",
    "    transfer_feedback: str\n",
    "    solution: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "306439ab-4b94-4084-8a1c-8c53ecf1a0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can switch between different LLMs \n",
    "llms = init_chat_model(\"openai:gpt-4.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "54ea64a9-96a1-42ab-bb3d-1a1c1d14b1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st Agent: Identify Target Domain from User Input\n",
    "def target_domain_agent(state: ReasoningState):\n",
    "    u = state['user_query']\n",
    "\n",
    "    msg = llms.invoke(TARGET_DOMAIN.format(user_query=u))\n",
    "\n",
    "    return {\"target_domain\": msg.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d327c06e-9618-4463-b19e-2be74a21d14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2st Agent: Conduct comprehensive research into problem landscape for target domain \n",
    "# (i.e. What specific challenges exist in this domain?)\n",
    "def problem_landscape_agent(state: ReasoningState):\n",
    "    t = state['target_domain']\n",
    "\n",
    "    msg = llms.invoke(PROBLEM_LANDSCAPE.format(target_domain=t))\n",
    "\n",
    "    return {\"problem_landscape\": msg.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "26b96ec9-7bdb-4803-9bab-dcc82d6a4c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3rd Agent: Abstract Problems Identified into Generalized Principles and TRIZ Contradiction\n",
    "def abstraction_agent(state: ReasoningState):\n",
    "    p = state['problem_landscape']\n",
    "    feedback = state.get('abstraction_feedback', '')\n",
    "    \n",
    "    # If there's feedback, include it in the prompt\n",
    "    if feedback:\n",
    "        msg = llms.invoke(ABSTRACTION.format(\n",
    "            problem_landscape=p,\n",
    "            feedback=f\"\\nPrevious attempt feedback: {feedback}\\nPlease address these issues in your new abstraction.\"\n",
    "        ))\n",
    "    else:\n",
    "        msg = llms.invoke(ABSTRACTION.format(problem_landscape=p))\n",
    "    \n",
    "    return {\"abstraction\": msg.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4b63464e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3.5th agent: CRIT Quality Control for TRIZ Abstraction\n",
    "def CRIT_Control_Abstraction(target_domain: str, problem_landscape: str, triz_contradictions: str) -> dict:\n",
    "    \"\"\"Quality control function that evaluates the abstraction output.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "        # CRITICAL READING INQUISITIVE TEMPLATE (CRIT) - TRIZ VALIDATION\n",
    "        You are a TRIZ ABSTRACTION VALIDATOR. Apply Socratic methods to evaluate contradiction quality.\n",
    "\n",
    "        ## INPUTS\n",
    "        Target Domain: {target_domain}\n",
    "        Problem Landscape: {problem_landscape}\n",
    "        TRIZ Contradictions: {triz_contradictions}\n",
    "\n",
    "        ## VALIDATION PROTOCOL [Pass Threshold: 7.0/10]\n",
    "\n",
    "        ### 1. METHOD OF DEFINITION\n",
    "        - Does each contradiction follow \"Improving [X] vs. Worsening [Y]\" format?\n",
    "        - Are parameters from Altshuller's 39 Engineering Parameters?\n",
    "        - Is abstraction at first-principles level (not solution-biased)?\n",
    "\n",
    "        ### 2. METHOD OF ELENCHUS (Cross-Examination)\n",
    "        - Trace each contradiction back to originating problem\n",
    "        - Verify causal chain: Problem → Root Cause → Contradiction\n",
    "        - Rate abstraction mapping strength (1-10)\n",
    "\n",
    "        ### 3. METHOD OF DIALECTIC (Counter-Arguments)\n",
    "        - Generate 2-3 alternative contradictions for same problem\n",
    "        - Compare which better captures essential tension\n",
    "        - Eliminate weaker contradictions\n",
    "\n",
    "        ### 4. SCORING CRITERIA\n",
    "        - **TRIZ Compliance (40%):** Format, parameter accuracy\n",
    "        - **Abstraction Quality (35%):** First-principles, non-solution-biased\n",
    "        - **Problem Coverage (25%):** All major problems represented\n",
    "\n",
    "        ### OUTPUT FORMAT\n",
    "        {{\n",
    "            \"score\": float (1.0-10.0),\n",
    "            \"pass\": boolean,\n",
    "            \"reason\": str (brief explanation if fail)\n",
    "        }}\n",
    "\n",
    "        **FAILURE CONDITIONS:** Format violations, non-TRIZ parameters, solution bias, logical gaps, incomplete coverage.\n",
    "        \n",
    "        IMPORTANT: Your response must be a valid JSON object with the exact keys shown above.\n",
    "    \"\"\"\n",
    "    response = llms.invoke(prompt)\n",
    "    \n",
    "    # Parse the response into a dictionary\n",
    "    try:\n",
    "        # Extract JSON from the response\n",
    "        response_text = response.content.strip()\n",
    "        # Find the JSON object in the response\n",
    "        start_idx = response_text.find('{')\n",
    "        end_idx = response_text.rfind('}') + 1\n",
    "        if start_idx >= 0 and end_idx > start_idx:\n",
    "            json_str = response_text[start_idx:end_idx]\n",
    "            result = json.loads(json_str)\n",
    "            return result\n",
    "        else:\n",
    "            # If no JSON found, return a default fail response\n",
    "            return {\n",
    "                \"score\": 0.0,\n",
    "                \"pass\": False,\n",
    "                \"reason\": \"Invalid response format from validator\"\n",
    "            }\n",
    "    except json.JSONDecodeError:\n",
    "        # If JSON parsing fails, return a default fail response\n",
    "        return {\n",
    "            \"score\": 0.0,\n",
    "            \"pass\": False,\n",
    "            \"reason\": \"Failed to parse validator response\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a230bf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3.5th Agent Logic Flow: if quality control passes, continue to next agent. Else, loop back to retry TRIZ abstraction (taking rationale for why it failed as additional input)\n",
    "def should_continue_to_base(state: ReasoningState) -> str:\n",
    "    \"\"\"Determine whether to continue to base domain or retry abstraction based on CRIT score.\"\"\"\n",
    "    # Get the CRIT score from the state\n",
    "    crit_result = CRIT_Control_Abstraction(\n",
    "        state['target_domain'],\n",
    "        state['problem_landscape'],\n",
    "        state['abstraction']\n",
    "    )\n",
    "    \n",
    "    # Parse the result to get the score and pass status\n",
    "    score = crit_result.get('score', 0)\n",
    "    passed = crit_result.get('pass', False)\n",
    "    \n",
    "    if passed:\n",
    "        return \"base\"  # Continue to base domain agent\n",
    "    else:\n",
    "        # Add the feedback to the state for the abstraction agent to use\n",
    "        state['abstraction_feedback'] = crit_result.get('reason', '')\n",
    "        return \"abstract\"  # Retry abstraction agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e95a5d2f-1773-4339-ba89-995d185885c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4th Agent: Search for Appropriate Base Domains\n",
    "def base_domain_agent(state: ReasoningState):\n",
    "    a = state['abstraction']\n",
    "\n",
    "    msg = llms.invoke(BASE_DOMAIN.format(contradictions=a))\n",
    "\n",
    "    return {\"base_domain\": msg.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f7db11f3-e388-48b4-8661-cde636b1ba50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5th Agent: Identify Solution in Base Domain\n",
    "def base_solution_agent(state: ReasoningState):\n",
    "    b = state['base_domain']\n",
    "\n",
    "    msg = llms.invoke(BASE_SOLUTIONS.format(input=b))\n",
    "\n",
    "    return {\"base_solutions\": msg.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5dc1b395-b154-493e-9c57-7e5a6926cf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6th Agent: Base Domain Solution Informing Target Domain Solution\n",
    "def analogical_transfer_agent(state: ReasoningState):\n",
    "    if \"base_solutions\" not in state:\n",
    "        raise ValueError(\"Missing 'base_solutions' key. Check if previous node returned it.\")\n",
    "    b = state['base_solutions']\n",
    "    t = state['target_domain']\n",
    "    feedback = state.get('transfer_feedback', '')\n",
    "    \n",
    "    # If there's feedback, include it in the prompt\n",
    "    if feedback:\n",
    "        msg = llms.invoke(ANALOGICAL_TRANSFER.format(\n",
    "            contradictions_solutions=b,\n",
    "            target_domain=t,\n",
    "            feedback=f\"\\nPrevious attempt feedback: {feedback}\\nPlease address these issues in your new analogical transfer.\"\n",
    "        ))\n",
    "    else:\n",
    "        msg = llms.invoke(ANALOGICAL_TRANSFER.format(\n",
    "            contradictions_solutions=b,\n",
    "            target_domain=t\n",
    "        ))\n",
    "    \n",
    "    return {\"analogical_transfer\": msg.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d8dc7e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.5th Agent: CRIT Quality Control for Analogical Reasoning \n",
    "def CRIT_Control_Analogical_Transfer(base_solutions: str, analogical_transfers: str, original_contradictions: str, target_domain: str) -> dict:\n",
    "    prompt = f\"\"\"\n",
    "    # CRITICAL READING INQUISITIVE TEMPLATE (CRIT) - ANALOGICAL TRANSFER VALIDATION\n",
    "    You are an ANALOGICAL TRANSFER VALIDATOR. Apply Socratic methods to evaluate transfer quality.\n",
    "\n",
    "    ## INPUTS\n",
    "    Base Solutions: {base_solutions}\n",
    "    Analogical Transfers: {analogical_transfers}\n",
    "    Original Contradictions: {original_contradictions}\n",
    "    Target Domain: {target_domain}\n",
    "\n",
    "    ## VALIDATION PROTOCOL [Pass Threshold: 7.5/10]\n",
    "\n",
    "    ### 1. METHOD OF ELENCHUS (Cross-Examination)\n",
    "    - Does each transfer preserve the CORE MECHANISM from base domain?\n",
    "    - Trace: Base Solution → Transfer Logic → Target Application\n",
    "    - Rate mechanism preservation fidelity (1-10)\n",
    "\n",
    "    ### 2. METHOD OF DIALECTIC (Counter-Arguments)\n",
    "    - Generate 2-3 alternative transfer approaches for same base solution\n",
    "    - Identify potential failure modes in target domain\n",
    "    - Test against target domain constraints\n",
    "\n",
    "    ### 3. METHOD OF COUNTERFACTUAL\n",
    "    - \"What if this transfer were applied in [alternative context]?\"\n",
    "    - Predict 2-3 unintended consequences in target domain\n",
    "    - Validate scalability and implementation feasibility\n",
    "\n",
    "    ### 4. SCORING CRITERIA\n",
    "    - **Mechanism Fidelity (35%):** Core solution pattern preserved\n",
    "    - **Domain Adaptation (30%):** Properly adapted to target constraints  \n",
    "    - **Contradiction Resolution (25%):** Addresses original TRIZ tensions\n",
    "    - **Implementation Feasibility (10%):** Realistic in target context\n",
    "\n",
    "    ### OUTPUT FORMAT\n",
    "    {{\n",
    "        \"score\": float (1.0-10.0),\n",
    "        \"pass\": boolean,\n",
    "        \"reason\": str (brief explanation if fail)\n",
    "    }}\n",
    "\n",
    "    **FAILURE CONDITIONS:** Mechanism distortion, poor domain adaptation, contradiction mismatch, implementation impossibility.\n",
    "    \n",
    "    IMPORTANT: Your response must be a valid JSON object with the exact keys shown above.\n",
    "    \"\"\"\n",
    "    response = llms.invoke(prompt)\n",
    "    \n",
    "    # Parse the response into a dictionary\n",
    "    try:\n",
    "        # Extract JSON from the response\n",
    "        response_text = response.content.strip()\n",
    "        # Find the JSON object in the response\n",
    "        start_idx = response_text.find('{')\n",
    "        end_idx = response_text.rfind('}') + 1\n",
    "        if start_idx >= 0 and end_idx > start_idx:\n",
    "            json_str = response_text[start_idx:end_idx]\n",
    "            result = json.loads(json_str)\n",
    "            return result\n",
    "        else:\n",
    "            # If no JSON found, return a default fail response\n",
    "            return {\n",
    "                \"score\": 0.0,\n",
    "                \"pass\": False,\n",
    "                \"reason\": \"Invalid response format from validator\"\n",
    "            }\n",
    "    except json.JSONDecodeError:\n",
    "        # If JSON parsing fails, return a default fail response\n",
    "        return {\n",
    "            \"score\": 0.0,\n",
    "            \"pass\": False,\n",
    "            \"reason\": \"Failed to parse validator response\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "faecca01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.5th Agent Logic Flow: \n",
    "def should_continue_to_synthesis(state: ReasoningState) -> str:\n",
    "    \"\"\"Determine whether to continue to synthesis or retry analogical transfer based on CRIT score.\"\"\"\n",
    "    crit_result = CRIT_Control_Analogical_Transfer(\n",
    "        state['base_solutions'],\n",
    "        state['analogical_transfer'], \n",
    "        state['abstraction'],\n",
    "        state['target_domain']\n",
    "    )\n",
    "\n",
    "    score = crit_result.get('score', 0)\n",
    "    passed = crit_result.get('pass', False)\n",
    "    \n",
    "    if passed:\n",
    "        return \"synthesis\"  # Continue to final synthesis\n",
    "    else:\n",
    "        state['transfer_feedback'] = crit_result.get('reason', '')\n",
    "        return \"analogical_transfer\"  # Retry transfer with feedback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e3626041-fcd5-4bf3-a32b-f164352e788f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7th Agent: Summarize everything and respond to the question\n",
    "def synthesis_agent(state: ReasoningState):\n",
    "    msg = llms.invoke(\n",
    "        f\"Evaluate the proposed analogical solutions. Find the best ones that balances practicality with innovation. Then, provide a detailed, well-structured response that addresses all aspects of the query.\\n\\n\"\n",
    "        f\"Problem: {state['user_query']}\\n\"\n",
    "        f\"Analogical Solutions: {state['analogical_transfer']}\"\n",
    "        f\"In your output, remember to abstract away the analogy itself such that it is focused on responding to the user input.\"\n",
    "        f\"Also, check if the users are requesting a specific number of possible solutions. Make sure to answer the user's query in full and provide what is requested.\"\n",
    "    )\n",
    "    return {\"solution\": msg.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "68994430-5388-4c63-84be-919d5978d5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the workflow \n",
    "workflow = StateGraph(ReasoningState)\n",
    "\n",
    "workflow.add_node(\"target\", target_domain_agent)\n",
    "workflow.add_node(\"landscape\", problem_landscape_agent)\n",
    "workflow.add_node(\"abstract\", abstraction_agent)\n",
    "workflow.add_node(\"base\", base_domain_agent)\n",
    "workflow.add_node(\"base_soln\", base_solution_agent)\n",
    "workflow.add_node(\"analogy\", analogical_transfer_agent)\n",
    "workflow.add_node(\"synthesis\", synthesis_agent)\n",
    "\n",
    "workflow.set_entry_point(\"target\")\n",
    "\n",
    "# Define edges\n",
    "workflow.add_edge(\"target\", \"landscape\")\n",
    "workflow.add_edge(\"landscape\", \"abstract\")\n",
    "# workflow.add_edge(\"abstract\", \"base\")\n",
    "\n",
    "# Define conditional edges - Use CRIT TO control quality for Abstraction TRIZ\n",
    "workflow.add_conditional_edges(\n",
    "    \"abstract\",\n",
    "    should_continue_to_base,\n",
    "    path_map=[\"base\", \"abstract\"]\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"base\", \"base_soln\")\n",
    "workflow.add_edge(\"base_soln\", \"analogy\")\n",
    "#workflow.add_edge(\"analogy\", \"synthesis\")\n",
    "\n",
    "# Define conditional edges - Use CRIT TO control quality for Analogical Transfer\n",
    "workflow.add_conditional_edges(\n",
    "    \"analogy\",\n",
    "    should_continue_to_synthesis,\n",
    "    path_map=[\"synthesis\", \"analogy\"]\n",
    ")\n",
    "\n",
    "workflow.set_finish_point(\"synthesis\")\n",
    "\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea24580",
   "metadata": {},
   "source": [
    "### Initialize Different Functions for Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c1ed3fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Output\n",
    "def basic_output(user_input: str):\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    {user_input}\n",
    "    \"\"\"\n",
    "\n",
    "    response = llms.invoke(prompt) \n",
    "\n",
    "    # Parse and format the response\n",
    "    formatted_response = parse_solution(response.content)\n",
    "\n",
    "    return(formatted_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "174da814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Output with COT\n",
    "## TO EDIT WITH FULL PROMPT?\n",
    "def basic_output_COT(user_input: str):\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    {user_input}\n",
    "    Think step by step.\n",
    "    \"\"\"\n",
    "\n",
    "    response = llms.invoke(prompt) \n",
    "\n",
    "    # Parse and format the response\n",
    "    formatted_response = parse_solution(response.content)\n",
    "\n",
    "    return(formatted_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4d4f4149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Output with Prompt Engineering\n",
    "def basic_output_prompt_engin(user_input: str):\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    {user_input}\n",
    "    Try to balance practicality with innovation.\n",
    "    \"\"\"\n",
    "\n",
    "    response = llms.invoke(prompt) \n",
    "\n",
    "    # Parse and format the response\n",
    "    formatted_response = parse_solution(response.content)\n",
    "\n",
    "    return(formatted_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7fe14c",
   "metadata": {},
   "source": [
    "### Test prompt 1 - Education"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6fb723",
   "metadata": {},
   "source": [
    "#### User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "848e42be",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_q = \"Teachers introduced structured group projects using peer evaluations and rotating roles to boost collaboration in science classes. Despite these measures, anonymous student surveys revealed that individual quiz scores post-project dropped compared to solo assignments, signaling diluted accountability. How can educators redesign design a system to ensure measurable individual mastery while preserving the benefits of teamwork?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cd473f",
   "metadata": {},
   "source": [
    "#### With Analogical Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5235345-734c-4aae-a32b-82acc97ddcf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GPT-4.1 Response WITH Analogical Reasoning Pipeline ===\n",
      "\n",
      "{'user_query': 'Teachers introduced structured group projects using peer evaluations and rotating roles to boost collaboration in science classes. Despite these measures, anonymous student surveys revealed that individual quiz scores post-project dropped compared to solo assignments, signaling diluted accountability. How can educators redesign design a system to ensure measurable individual mastery while preserving the benefits of teamwork?', 'target_domain': 'Assessment Systems Innovation in Collaborative Learning', 'problem_landscape': '[\\n  {\\n    \"Problem\": \"Lack of Valid and Reliable Assessment Tools for Collaboration\",\\n    \"Description\": \"Current assessment tools often fail to accurately measure individual contributions and collaborative skills in group learning contexts. This leads to potential unfairness and does not reliably reflect individual or group learning outcomes.\",\\n    \"Context\": \"K-12 and higher education institutions, online collaborative learning platforms, and corporate training programs.\",\\n    \"Stakeholders\": [\"Students\", \"Teachers\", \"Instructional Designers\", \"Administrators\", \"Researchers\"],\\n    \"Root Causes\": [\"Measurement tools designed for individual assessment, not collaboration\", \"Complexity of assessing group dynamics\", \"Limited empirical validation of existing tools\"],\\n    \"Impact\": \"Educational outcomes may be misrepresented, leading to decreased motivation and trust in the assessment process; can affect grading and student progression.\",\\n    \"Current Approaches\": [\"Use of peer assessments\", \"Rubrics outlining collaborative skills\", \"Group grades based on overall output\"],\\n    \"Limitations\": \"Peer assessments can be biased or inconsistent; rubrics are often subjective and generic; group grades neglect individual effort and accountability.\",\\n    \"Success Metrics\": [\"Demonstrable validity and reliability of assessment data\", \"Stakeholder trust in fairness\", \"Clear link between assessment outcomes and actual collaborative behaviors\"],\\n    \"Interconnections\": \"Related to transparency in assessment, equity issues, and the use of technology for data collection and analysis.\"\\n  },\\n  {\\n    \"Problem\": \"Inequitable Recognition of Individual Contributions\",\\n    \"Description\": \"Assessment systems often fail to distinguish between differing levels of effort and contribution within collaborative groups, resulting in some members receiving unjust credit or penalty.\",\\n    \"Context\": \"Student group projects, professional team-based training, online group discussions and collaborative assignments.\",\\n    \"Stakeholders\": [\"Students\", \"Teachers\", \"Project Managers\", \"HR Training Coordinators\"],\\n    \"Root Causes\": [\"Difficulty tracking individual input\", \"Groupthink and social loafing\", \"Insufficient tools to monitor process as well as product\"],\\n    \"Impact\": \"Demotivates high-performing members and enables free riding; undermines the credibility of collaborative assignments.\",\\n    \"Current Approaches\": [\"Self and peer evaluations\", \"Periodic check-ins\", \"Fixed-role assignments within teams\"],\\n    \"Limitations\": \"Self and peer reports can be inaccurate or manipulated; role assignments may not reflect actual division of labor; lack of continuous monitoring.\",\\n    \"Success Metrics\": [\"Accurate attribution of grades to individuals\", \"Reduction in reported incidents of free riding or inequity\", \"Stakeholder satisfaction\"],\\n    \"Interconnections\": \"Overlaps with fairness and transparency issues, as well as the absence of robust tracking tools.\"\\n  },\\n  {\\n    \"Problem\": \"Limited Scalability of Authentic Collaborative Assessment\",\\n    \"Description\": \"Scaling rigorous, authentic assessment of collaboration to large classes or massive open online courses (MOOCs) is logistically complex and labor-intensive.\",\\n    \"Context\": \"Large university courses, MOOCs, corporate e-learning platforms with high enrollment.\",\\n    \"Stakeholders\": [\"Instructors\", \"Course Designers\", \"Educational Technology Providers\", \"Students\"],\\n    \"Root Causes\": [\"Assessment processes requiring manual review\", \"Lack of automated or AI-driven assessment tools\", \"Resource constraints (time, personnel)\"],\\n    \"Impact\": \"Assessment quality is often compromised for scalability; students receive less meaningful feedback; educators face increased workloads.\",\\n    \"Current Approaches\": [\"Sampling group outputs\", \"Automated quizzes disconnected from collaboration\", \"Peer grading with limited oversight\"],\\n    \"Limitations\": \"Sampling may miss key dynamics; automated assessments do not gauge true collaboration; peer grading may be inconsistent and unreliable.\",\\n    \"Success Metrics\": [\"Number of students who can be fairly and accurately assessed\", \"Time required per assessment\", \"Stakeholder satisfaction with scale and quality\"],\\n    \"Interconnections\": \"Connects with the problems of data validity, equitable individual recognition, and technological limitations.\"\\n  },\\n  {\\n    \"Problem\": \"Lack of Transparency and Clarity in Assessment Criteria\",\\n    \"Description\": \"Students and educators often do not fully understand what collaborative behaviors or outcomes are being assessed, leading to confusion and disengagement.\",\\n    \"Context\": \"All educational levels where collaboration is part of the assessment, especially in multicultural or online environments.\",\\n    \"Stakeholders\": [\"Students\", \"Teachers\", \"Curriculum Designers\", \"Parents\"],\\n    \"Root Causes\": [\"Unclear or inconsistent assessment rubrics\", \"Misalignment between stated and actual criteria\", \"Insufficient communication from educators\"],\\n    \"Impact\": \"Students may focus efforts on the wrong tasks; disputes arise about grades; overall learning objectives are undermined.\",\\n    \"Current Approaches\": [\"Provision of generalized rubrics\", \"Occasional assignment briefings\", \"Post-assessment feedback\"],\\n    \"Limitations\": \"Rubrics may be vague or not contextualized; briefings are often insufficient or sporadic; feedback is retrospective, not formative.\",\\n    \"Success Metrics\": [\"Improvement in student understanding of assessment targets\", \"Reduction in assessment disputes\", \"Performance alignment with learning objectives\"],\\n    \"Interconnections\": \"Closely tied to issues of fairness, validity, and learning analytics used in collaborative environments.\"\\n  },\\n  {\\n    \"Problem\": \"Insufficient Integration of Process Data into Assessment\",\\n    \"Description\": \"Most collaborative assessments focus on final products rather than the collaborative process, missing critical insights into how teams function and learn together.\",\\n    \"Context\": \"Project-based learning, design thinking activities, remote team collaborations.\",\\n    \"Stakeholders\": [\"Students\", \"Educators\", \"Instructional Technologists\"],\\n    \"Root Causes\": [\"Technical difficulty in capturing process data\", \"Ethical/privacy concerns with data collection\", \"Assessment culture focused on outcomes\"],\\n    \"Impact\": \"Important aspects of learning and contribution remain invisible; learning interventions cannot be targeted effectively.\",\\n    \"Current Approaches\": [\"Occasional status reports\", \"Process-oriented reflection assignments\", \"Manual logs\"],\\n    \"Limitations\": \"Reflection is subjective and can be fabricated; manual logs are tedious and incomplete; process data often ignored in final assessment.\",\\n    \"Success Metrics\": [\"Proportion of assessment based on process\", \"Improved understanding of group dynamics\", \"Early identification of dysfunctional collaboration\"],\\n    \"Interconnections\": \"Links to data privacy, the lack of suitable digital tools, and the need for richer learning analytics.\"\\n  }\\n]', 'abstraction': 'Certainly! Here is a TRIZ-based abstraction and contradiction analysis based on your problem landscape.\\n\\n---\\n\\n# Core TRIZ Contradictions for Collaborative Assessment\\n\\n---\\n\\n### 1. Improving Measurement Accuracy vs. Increasing System Complexity\\n\\n- **When we improve**: Accuracy or reliability of assessing collaboration (Parameter: Measurement Accuracy)\\n- **It worsens**: Complexity of the assessment system (Parameter: System Complexity)\\n- **Formulated as**: Improving Measurement Accuracy worsens System Complexity.\\n- **TRIZ Principles**: \\n  - #10 Prior Action (preparing data/tools up front)\\n  - #28 Mechanics Substitution (automate/manual → digital/AI solutions)\\n  - #1 Segmentation (break down assessment into smaller, manageable components)\\n- **Innovation Potential**: **High**\\n  - (Solving this tension enables valid, scalable collaborative assessment and forms the foundation for breakthrough value in learning analytics and fairness.)\\n\\n---\\n\\n### 2. Improving Fairness/Recognition of Individual Contribution vs. Increasing Monitoring Burden\\n\\n- **When we improve**: Fair attribution of individual efforts in group work (Parameter: Degree of Individualization/Fairness)\\n- **It worsens**: Administrative/Monitoring Effort (Parameter: System Control Complexity, or Monitoring Burden)\\n- **Formulated as**: Improving Fairness in Attribution worsens the Monitoring Burden on stakeholders.\\n- **TRIZ Principles**: \\n  - #24 Intermediary (use a mediator or tool—e.g., analytics dashboard)\\n  - #23 Feedback (continuous, real-time tracking for transparency)\\n  - #25 Self-Service (enable self and peer monitoring with built-in guidance)\\n- **Innovation Potential**: **High**\\n  - (Addressing this enables both equity and scalability, a key block for mass collaborative platforms.)\\n\\n---\\n\\n### 3. Increasing Scalability vs. Decreasing Assessment Quality/Authenticity\\n\\n- **When we improve**: Scalability (Parameter: Number of Objects—students/groups—with reliable assessment)\\n- **It worsens**: Depth and authenticity of assessing collaboration (Parameter: Assessment Quality)\\n- **Formulated as**: Improving Scalability of assessment worsens its Authenticity and Quality.\\n- **TRIZ Principles**: \\n  - #10 Prior Action (automate preparation/collection)\\n  - #35 Parameter Changes (shift assessment parameters—e.g., representative sampling, adaptive assessment)\\n  - #2 Extraction (separate core collaborative indicators for scalable measurement)\\n- **Innovation Potential**: **High**\\n  - (A breakthrough here would enable authentic collaborative assessment at massive scale—MOOCs, e-learning, enterprise training.)\\n\\n---\\n\\n### 4. Improving Transparency/Clarity vs. Reducing Flexibility/Adaptivity\\n\\n- **When we improve**: Transparency and clarity in assessment criteria and process (Parameter: Visibility/Clarity)\\n- **It worsens**: Ability to adapt assessment to diverse, dynamic contexts (Parameter: Flexibility)\\n- **Formulated as**: Improving Transparency in Criteria worsens Flexibility and Adaptivity.\\n- **TRIZ Principles**: \\n  - #13 The Other Way Round (design co-created or customizable rubrics)\\n  - #3 Local Quality (contextualize clarity to group needs)\\n  - #32 Color Changes (use visualizations for contextual/real-time transparency)\\n- **Innovation Potential**: **Medium**\\n  - (Solving this helps trust and motivation but is less fundamental than fairness/scalability.)\\n\\n---\\n\\n### 5. Enhancing Process Data Integration vs. Respecting Privacy/Ethics\\n\\n- **When we improve**: Degree to which process data informs assessment (Parameter: Information Completeness)\\n- **It worsens**: Data privacy and ethical exposure (Parameter: Harmful Effects)\\n- **Formulated as**: Improving Integration of Process Data in Assessment worsens Privacy/Ethical Risks.\\n- **TRIZ Principles**: \\n  - #11 Cushion in Advance (anonymization, secure consent protocols)\\n  - #26 Copying (simulate/double process data for review before actual use)\\n  - #35 Parameter Changes (vary level or type of data collected depending on sensitivity)\\n- **Innovation Potential**: **Medium**\\n  - (Unlocks richer, formative assessment but must be resolved with trust and ethical design.)\\n\\n---\\n\\n# Summary Table\\n\\n| Contradiction | Improving... | Worsening... | TRIZ Principles | Innovation Potential |\\n|-----|-------------------------|------------------|----------------------|---------------------|\\n| 1   | Measurement Accuracy    | System Complexity| 10, 28, 1           | High                |\\n| 2   | Fair Individualization  | Monitoring Burden| 24, 23, 25           | High                |\\n| 3   | Scalability             | Assessment Quality| 10, 35, 2           | High                |\\n| 4   | Transparency/Clarity    | Flexibility      | 13, 3, 32            | Medium              |\\n| 5   | Process Data Integration| Privacy/Ethics   | 11, 26, 35           | Medium              |\\n\\n---\\n\\n## Recommendation\\n\\n**Prioritize contradictions 1, 2, and 3** for research, development, and systemic re-design, as their resolution promises the greatest breakthrough value in fair, scalable, and valid collaborative assessment. Leverage the suggested TRIZ principles as ideation ‘starting points’ for solution designs or technology requirements.\\n\\nIf you provide additional domain challenges or detailed solution attempts, I can further refine the contradiction analysis or suggest targeted inventive strategies.', 'base_domain': 'Certainly! I’ll select each single contradiction and then craft three “conceptually distant” domains for each, **explaining why each is relevant**. Then, I’ll experiment with **three different “subsets”** of the contradictions, and for each identify three new domains (which may overlap with, but should go beyond, those in the singles). The explanations will highlight how these unrelated fields have significant conceptual analogies or solutions to such contradictions.\\n\\nHere’s the **structured output** as requested:\\n\\n---\\n\\n## 1. Improving Measurement Accuracy vs. Increasing System Complexity\\n\\n**a. Music Production (Mastering & Signal Processing):**  \\nMusic engineers must deliver highly accurate sound reproduction but contend with rising complexity from added tools and filters. Insightful for balancing precision with workflow simplicity in audio engineering—paralleling the measurement-complexity dilemma.\\n\\n**b. Wildlife Population Ecology:**  \\nEcologists use increasingly sophisticated, multi-source methods (tracking, genetic markers) to achieve precise animal population counts, but must develop field protocols that don’t become unmanageable. This field offers approaches for accuracy without unsustainable complexity in real-world data gathering.\\n\\n**c. Aviation Cockpit Design:**  \\nDesigners balance the need for reliably accurate flight instrumentation with the imperative to avoid overly complex cockpit layouts, which can cause human error. The ways aviators resolve precision versus system burden is relevant to collaborative assessment tooling.\\n\\n---\\n\\n## 2. Improving Fairness/Recognition of Individual Contribution vs. Increasing Monitoring Burden\\n\\n**a. Scientific Authorship & Credit Systems:**  \\nDisciplines like particle physics handle large collaborations and credit assignment with minimal administrative overhead, using systems like contributorship matrices. Their mechanisms for fair recognition without overwhelming bureaucracy directly mirror this contradiction.\\n\\n**b. Prison Sentencing and Behavioral Monitoring:**  \\nCorrectional systems strive for individualized, fair parole/probation assessments without overburdening case managers with intrusive, continuous surveillance. This domain addresses similar fairness versus administrative load issues.\\n\\n**c. Open-Source Software Community Governance:**  \\nLarge open-source projects often structure recognition (e.g., commit logs, merit systems) to track contributions fairly, while minimizing the need for centralized, resource-heavy oversight. Their lightweight, transparent peer governance is conceptually akin to this contradiction.\\n\\n---\\n\\n## 3. Increasing Scalability vs. Decreasing Assessment Quality/Authenticity\\n\\n**a. Art Conservation (Mass vs. Individualized Approaches):**  \\nInstitutions face pressure to ‘scale up’ restoration and cataloguing, yet maintaining authenticity for each unique piece is paramount. Strategies balancing mass processes with individual attention relate to the scale/authenticity tension in collaborative assessment.\\n\\n**b. Epidemiology (Mass Screening vs. Diagnostic Precision):**  \\nPublic health systems develop ways to screen at scale (e.g., rapid tests), but face the challenge of not sacrificing the depth and authenticity of individual diagnoses. This mirrors the tension between mass assessment and personalized, authentic appraisal.\\n\\n**c. Theatre Education/Performance Festivals:**  \\nFestival organizers must assess hundreds of performances yet retain meaningful, authentic feedback for each. Their solutions to scale and individualized judgment in creative arts are directly relevant.\\n\\n---\\n\\n## 4. Improving Transparency/Clarity vs. Reducing Flexibility/Adaptivity\\n\\n**a. Constitutional Law (Legal Codification vs. Judicial Discretion):**  \\nLegal frameworks rigorously codify rules for clarity and transparency, but must preserve flexibility for contextual application by judges. The tension between codification and adaptive justice parallels this contradiction.\\n\\n**b. Fashion Design (Trend Openness vs. Designer Adaptivity):**  \\nDesigners balance the dangers of making processes/standards \"too transparent\" (limiting creative pivots) with the need for clarity in production or marketing. The interplay of openness and creative flexibility in fashion is analogous.\\n\\n**c. Emergency Medicine Protocols:**  \\nProtocols must be clear and transparent for all practitioners, yet flexible enough for adaptation in chaotic, diverse medical settings. Their balancing acts echo issues in collaborative assessment criteria.\\n\\n---\\n\\n## 5. Enhancing Process Data Integration vs. Respecting Privacy/Ethics\\n\\n**a. Urban Surveillance and Smart Cities:**  \\nCity planners seek the fullest integration of citizen behavioral/process data for analytics, efficiency, and safety, while facing acute privacy and ethical challenges. This industry’s solutions inform the data-integration/ethics dilemma.\\n\\n**b. Personal Digital Health Devices (Wearables):**  \\nWearable health tech aggregates sensitive data to inform diagnostics and wellness, yet must navigate privacy regulations and ethical data use. Their balancing of rich data insights and personal privacy maps onto this contradiction.\\n\\n**c. Anonymous Artistic Patronage (Cryptic Donors and Recipients):**  \\nIn cultural and creative spheres, artists and patrons often design systems to recognize support without revealing private identities—integrating process data while honoring privacy imperatives.\\n\\n---\\n\\n# Contradiction Subsets\\n\\n---\\n\\n### Subset A: Contradictions 1 & 2 (Measurement Accuracy vs. System Complexity AND Fair Individualization vs. Monitoring Burden)\\n\\n**a. Ecosystem Restoration Planning:**  \\nPlanners need accurate, individualized intervention data (accuracy/fairness) but can\\'t let monitoring/management grow too complex. Their integrative approaches speak directly to multifaceted accuracy and simplicity/fairness tradeoffs.\\n\\n**b. Ancient Manuscript Authentication (Provenance & Attribution Studies):**  \\nScholars seek highly accurate, individualized authorship/ageing while keeping analytic processes as simple and scalable as possible, mirroring this subset’s contradiction.\\n\\n**c. Corporate Performance Management (KPIs & Employee Recognition):**  \\nOrganizations strive for fair, accurate employee appraisal without overwhelming managers with complex evaluation systems or unfair, opaque methods—paralleling both contradictions.\\n\\n---\\n\\n### Subset B: Contradictions 3 & 4 (Scalability vs. Quality/Authenticity AND Transparency vs. Flexibility/Adaptivity)\\n\\n**a. International Development Aid (Program Evaluation):**  \\nAgencies must transparently assess massive, diverse projects while maintaining locally flexible criteria and authentic, insightful evaluation—directly reflecting this ethos.\\n\\n**b. Jazz Ensemble Improvisation:**  \\nLarge groups or jam sessions must be scalable and transparent in cues/structure, but also preserve authentic expression and adaptivity for each player, akin to artistic collaborative assessment.\\n\\n**c. Weather Forecasting for Diverse User Bases:**  \\nMeteorological systems seek to present massive-scale, data-driven, transparent predictions, while allowing local adjustments and ensuring authenticity for specialized users (e.g., pilots, farmers).\\n\\n---\\n\\n### Subset C: Contradictions 1, 2 & 5 (Accuracy vs. Complexity, Fair Individualization vs. Monitoring Burden, Data Integration vs. Privacy/Ethics)\\n\\n**a. Political Polling & Voter Analysis:**  \\nPollsters aggregate rich, individualized voter data for accuracy and fairness but must avoid overwhelming complexity and navigate privacy/ethical red lines—a triply analogous situation.\\n\\n**b. Pharmaceutical Clinical Trials Management:**  \\nTrials need accurate, personalized health assessment, data integration, but must avoid undue complexity (for coordinators/patients) and must maintain strict ethics and privacy.\\n\\n**c. Religious Confession and Accountability Systems:**  \\nFaith communities sometimes adopt structured processes for individualized fairness, accurate spiritual measurement, and core privacy/ethical protections—a unique domain navigating this trio of tensions.\\n\\n---\\n\\n# Domain Tally\\n\\n1. Music Production/Engineering  \\n2. Wildlife Population Ecology  \\n3. Aviation Cockpit Design  \\n4. Scientific Authorship & Credit  \\n5. Correctional Behavior Monitoring  \\n6. Open-source Community Governance  \\n7. Art Conservation  \\n8. Epidemiology  \\n9. Theatre Festival Assessment  \\n10. Constitutional Law  \\n11. Fashion Design  \\n12. Emergency Medicine  \\n13. Urban Surveillance/Smart Cities  \\n14. Digital Health Wearables  \\n15. Anonymous Artistic Patronage  \\n16. Ecosystem Restoration Planning  \\n17. Ancient Manuscript Authentication  \\n18. Corporate Performance Management  \\n19. International Development Aid  \\n20. Jazz Ensemble Improvisation  \\n21. Weather Forecasting Systems  \\n22. Political Polling/Voter Analytics  \\n23. Clinical Trials (Pharma)  \\n24. Religious Confession/Accountability  \\n\\n---\\n\\n## (Total: 24 distinctly rationalized source domains; per domain, relevant explanation provided as above.)', 'base_solutions': 'Certainly. Below, for **each of your provided source domains**, I extract a **specific, real solution pattern** that directly addresses the given contradiction(s). I name the pattern (as typically known in the domain), then give the contradiction(s) faced, describe the mechanism in detail, and spell out its domain application context.\\n\\n---\\n\\n## 1. **Music Production (Mastering & Signal Processing)**\\n- **Contradiction:** Need for highly accurate (faithful) sound measurement/reproduction vs. rapidly increasing system complexity (too many processing tools and options can overburden workflow and introduce errors).\\n- **Solution Pattern:** **Standardized Signal Chain (e.g., “Limiter-First” Approach)**\\n- **Mechanism:** In professional mastering, engineers often structure signal flow so that critical, accuracy-preserving tools—most notably, a *limiter*—are standardized as the final processor in the chain. By always placing a single high-quality limiter at the end, they ensure loudness constraints and peak accuracy without repeatedly adjusting parameters at every previous stage, thus avoiding cumulative complexity from nested processing. This modularizes accuracy assurance while keeping overall system complexity contained.\\n- **Context:** Common in commercial music mastering, especially when handling high volumes of tracks or running mastering houses with multiple engineers needing repeatable, high-accuracy outputs.\\n\\n---\\n\\n## 2. **Wildlife Population Ecology**\\n- **Contradiction:** Achieving accurate (granular, individualized) animal counts vs. protocol/system complexity (multi-source methods, excessive data handling).\\n- **Solution Pattern:** **Mark-Recapture with Minimal Recapture Events**\\n- **Mechanism:** Ecologists use the “mark-recapture” approach, where only a fraction of the population is marked in an initial event, then a second (sometimes just one) minimal-effort recapture round is used to calculate full population size via mathematical inference (Lincoln-Petersen estimator). This allows accuracy with limited, simple field efforts, rather than complex, continuous monitoring systems.\\n- **Context:** Applied when estimating elusive or highly mobile animal populations in field studies with logistic/resource limits.\\n\\n---\\n\\n## 3. **Aviation Cockpit Design**\\n- **Contradiction:** Need for accurate, reliable flight data available instantly to pilots vs. potential for complex instrumentation layouts leading to overload and errors.\\n- **Solution Pattern:** **Primary Flight Display (PFD) Integration**\\n- **Mechanism:** Designers consolidate numerous gauges and dials (altimeter, airspeed, horizon, etc.) into a single multi-function Primary Flight Display. By integrating the most critical indicators into one intuitive screen, pilots receive all key data accurately with much less complexity than juggling legacy layouts. This preserves measurement fidelity but drastically reduces cognitive/system overload.\\n- **Context:** Standard in modern commercial and military aviation cockpits, especially since glass cockpit technologies became ubiquitous.\\n\\n---\\n\\n## 4. **Scientific Authorship & Credit Systems**\\n- **Contradiction:** Need for fair, individualized recognition of every contributor (especially in huge collaborations) vs. the risk of an oppressive monitoring/admin burden.\\n- **Solution Pattern:** **Contributor Roles Taxonomy (CRediT) Matrix**\\n- **Mechanism:** Journals adopt the CRediT taxonomy: a matrix of contributor roles (e.g., data curation, formal analysis, visualization) wherein each author self-selects and is listed for their actual contributions. This transparent self-declaration bypasses the need for central and continuous monitoring, offering individualized yet lightweight credit.\\n- **Context:** Used in particle physics, genomics, and other “big team” science fields where papers may list hundreds or thousands of authors.\\n\\n---\\n\\n## 5. **Correctional Behavior Monitoring**\\n- **Contradiction:** Fair, individualized assessment for parole/probation decisions vs. overwhelming casework if monitoring is too granular or intrusive.\\n- **Solution Pattern:** **Risk Assessment Instruments with Periodic Self-Reporting**\\n- **Mechanism:** Many parole systems employ standardized risk assessment scoring—where parolees periodically self-report via questionnaires and only outlier scores trigger deeper investigation. This balances fairness and individualization with a lightweight, scalable method, reducing the ongoing monitoring burden except where truly warranted.\\n- **Context:** Widely adopted across the US and EU correctional/parole systems to manage large caseloads fairly.\\n\\n---\\n\\n## 6. **Open-Source Software Community Governance**\\n- **Contradiction:** Ensuring fair attribution for every contribution vs. the impracticality of heavy, centralized monitoring of all activities.\\n- **Solution Pattern:** **Distributed Commit Log with Pull Request Review**\\n- **Mechanism:** Version control systems (like Git) record every contributor’s code changes automatically, while peer-review of pull requests ensures recognition without central managerial tracking. Contribution is transparent and persistent in logs; minimal bureaucracy suffices.\\n- **Context:** Ubiquitous in major open-source software projects (e.g., Linux Kernel, Apache), where hundreds/thousands contribute asynchronously.\\n\\n---\\n\\n## 7. **Art Conservation (Mass vs. Individualized Approaches)**\\n- **Contradiction:** Pressure to scale up conservation efforts across large collections vs. need for authentic, individualized attention to each artifact.\\n- **Solution Pattern:** **Triage and Modular Restoration**\\n- **Mechanism:** Conservators implement triage: preliminary classification sorts items by risk/importance so that standardized, scalable interventions are applied to lower-tier items, while masterpieces/rarities get bespoke, in-depth attention. This partitions mass-process from authenticity-preserving craftsmanship.\\n- **Context:** Used in national museums (e.g., The Louvre, The Met) for managing thousands of cultural objects on limited budgets.\\n\\n---\\n\\n## 8. **Epidemiology (Mass Screening vs. Diagnostic Precision)**\\n- **Contradiction:** Provide mass, scalable health screening vs. not sacrificing accuracy or authenticity of individual diagnoses.\\n- **Solution Pattern:** **Two-Stage Screening (Rapid Test + Confirmatory Gold Standard)**\\n- **Mechanism:** Public health campaigns employ rapid, low-cost tests for initial screening across large populations, only referring positives to gold-standard diagnostics (e.g., PCR or imaging). This minimizes deep assessment to authentic, likely cases, preserving diagnostic detail for individuals while screening scales up.\\n- **Context:** Routine in TB, HIV, COVID-19, and cancer screening programs worldwide.\\n\\n---\\n\\n## 9. **Theatre Education/Performance Festivals**\\n- **Contradiction:** Need for scalable, efficient assessment of many performances vs. providing authentic, meaningful individualized feedback.\\n- **Solution Pattern:** **Rotating Panel with Structured Comment Sheets**\\n- **Mechanism:** Festivals recruit a rotating panel of specialists who use structured feedback forms designed to elicit at least one individualized comment per act, while core assessment criteria remain centralized. Shared workload + anchored forms ensure authenticity despite scale.\\n- **Context:** Used in Edinburgh Fringe, Speech/Debate circuits, youth theatre festivals.\\n\\n---\\n\\n## 10. **Constitutional Law (Legal Codification vs. Judicial Discretion)**\\n- **Contradiction:** Codify clear, transparent rules for justice vs. preserving judicial adaptability for case-specific circumstances.\\n- **Solution Pattern:** **“Balancing Tests” in Jurisprudence**\\n- **Mechanism:** Courts employ “balancing tests” whereby statutes provide clear, enumerated factors, but judges are empowered to weigh these flexibly depending on particulars. This mix of codified transparency and situational adjustability mediates the contradiction.\\n- **Context:** Used in US constitutional law (e.g., the Lemon Test on church/state separation).\\n\\n---\\n\\n## 11. **Fashion Design (Trend Openness vs. Designer Adaptivity)**\\n- **Contradiction:** Need for transparent, open communication of style trends/processes vs. not constraining designers’ flexibility or adaptivity.\\n- **Solution Pattern:** **Mood Boards with Modular Components**\\n- **Mechanism:** Designers present highly transparent “mood boards” showing core inspirations/materials/colors, but keep the modular components flexible and recombinable, retaining agility for last-minute creative adaptation. Mood boards guide, not confine.\\n- **Context:** Standard in design houses and fashion institutes globally.\\n\\n---\\n\\n## 12. **Emergency Medicine Protocols**\\n- **Contradiction:** Protocols must be clear and transparent for practitioners at all levels, but not so rigid as to obstruct adaptivity during medical emergencies/novel cases.\\n- **Solution Pattern:** **Algorithmic Protocol with Branch Points (e.g., ACLS)**  \\n- **Mechanism:** Life support protocols are structured as branching algorithms: they mandate clear steps, but at key clinical markers, practitioners can diverge (branch) based on context. This ensures clarity, yet embeds space for professional adaptation.\\n- **Context:** Used in Advanced Cardiac Life Support (ACLS), trauma response, field medicine.\\n\\n---\\n\\n## 13. **Urban Surveillance and Smart Cities**\\n- **Contradiction:** Maximal integration of urban data (for safety, services, analytics) vs. maintaining individual privacy and addressing ethical concerns.\\n- **Solution Pattern:** **Edge Data Anonymization**\\n- **Mechanism:** Surveillance and sensor systems preprocess data “at the edge” by automatically blurring faces, removing unique identifiers, or hashing location data before integrating into centralized analytics. This enables aggregate insights while stripping out privacy threats.\\n- **Context:** Pilot programs in cities including Toronto, Singapore, Barcelona.\\n\\n---\\n\\n## 14. **Personal Digital Health Devices (Wearables)**\\n- **Contradiction:** Rich, integrated data streams for precise health monitoring vs. regulatory/personal privacy requirements.\\n- **Solution Pattern:** **On-Device Data Aggregation with User-Controlled Sync**\\n- **Mechanism:** Wearables aggregate/process raw sensory data locally, producing only summary metrics (step count, heart rate trends) and allow the *user* to selectively sync/share to external platforms. Raw, identifying data remains private unless explicitly shared.\\n- **Context:** Standard in consumer wearables (Apple Watch, Fitbit), as required by GDPR and HIPAA.\\n\\n---\\n\\n## 15. **Anonymous Artistic Patronage**\\n- **Contradiction:** Need to integrate “support/process data” for network effects and recognition vs. respecting anonymity/privacy of patrons/artists.\\n- **Solution Pattern:** **Blind Trust with Public Attribution Tokens**\\n- **Mechanism:** Gifts are routed via anonymous “blind trusts” (legal proxies), which then issue public ‘attribution tokens’—anonymous acknowledgments on public records or works—thus integrating funding data for recognition without personal revelation.\\n- **Context:** Used by major museums (e.g., anonymous donors), literary prize systems.\\n\\n---\\n\\n## 16. **Ecosystem Restoration Planning**\\n- **Contradiction:** Accurate, individualized data for each intervention vs. unmanageable complexity in monitoring/management.\\n- **Solution Pattern:** **Standardized Monitoring Plots with Representative Sampling**\\n- **Mechanism:** Restoration managers establish carefully placed monitoring plots as statistically representative samples. Detailed data are only gathered at these carefully selected sites; broader interventions are informed by extrapolation from them, balancing accuracy and logistic simplicity.\\n- **Context:** Used in landscape-wide restoration (e.g., prairies, reforestation) by organizations like The Nature Conservancy.\\n\\n---\\n\\n## 17. **Ancient Manuscript Authentication**\\n- **Contradiction:** Desired: Highly accurate, individualized analyses (authorship, age, provenance) vs. risk of overly complex, resource-intensive analytic systems.\\n- **Solution Pattern:** **Non-invasive Multispectral Imaging with Software-assisted Analysis**\\n- **Mechanism:** High-resolution multispectral imaging enables capture of nearly all forensic data in a single pass, with software extracting layered information (inks, palimpsests, etc.) clarity. This avoids serial, additive testing and reduces overall process complexity.\\n- **Context:** Employed by libraries (Vatican, British Library), research teams for rare manuscripts.\\n\\n---\\n\\n## 18. **Corporate Performance Management**\\n- **Contradiction:** Accurate + fair, individualized employee appraisal vs. overwhelming complexity if monitoring/evaluating is too granular or bureaucratic.\\n- **Solution Pattern:** **360-Degree Appraisal with Forced Ranking**\\n- **Mechanism:** Performance is scored using limited, standardized peer/supervisor feedback, with staff then slotted in forced performance tiers (e.g., top 20%, middle 70%). This provides individualized, fair assessment through peer input, but avoids continuous, complex metric tracking.\\n- **Context:** Used in major corporations (GE, Amazon, Microsoft), often alongside bonus/retention decisions.\\n\\n---\\n\\n## 19. **International Development Aid (Program Evaluation)**\\n- **Contradiction:** Need for scalable, transparent program evaluation (as demanded by funders/oversight) vs. maintaining locally adaptive, authentic assessments for impactful work.\\n- **Solution Pattern:** **Logical Framework Approach (Logframe Matrix)**\\n- **Mechanism:** Aid agencies use logframes: a matrix mapping clear, transparent input/output/impact metrics, but allow country teams to locally interpret/adjust alignments (“indicators of success”) based on context. This standardizes transparency, but authorizes flexible authenticity in assessment.\\n- **Context:** Mandated in World Bank, UN, USAID project grants.\\n\\n---\\n\\n## 20. **Jazz Ensemble Improvisation**\\n- **Contradiction:** Large/jam ensembles require scalable and clear coordination structures, but must preserve individual performers’ authentic, adaptive expression.\\n- **Solution Pattern:** **Guided Head Arrangement with Solo Choruses**\\n- **Mechanism:** The ensemble plays a fixed “head” (intro/outro theme), with clear signals for solo turns. Within solos, full flexibility and authenticity is granted, while the group remains tethered to the transparent head structure for cueing and resolution.\\n- **Context:** Backbone of jazz big band and jam session performance (Duke Ellington, Mingus, Marsalis ensembles).\\n\\n---\\n\\n## 21. **Weather Forecasting for Diverse User Bases**\\n- **Contradiction:** Providing scaled, highly transparent data to wide audiences vs. flexible refinement/personalization for niche users (e.g., pilots, farmers).\\n- **Solution Pattern:** **Multi-Layered Forecast Products**\\n- **Mechanism:** Providers issue core, headline (“public”) forecasts in clear, accessible formats, while advanced layers (raw models, API feeds) are made accessible for professional adaptation and deeper use. This combines mass clarity with expert flexibility.\\n- **Context:** National Weather Service, ECMWF, MetOffice—standard approach in modern forecasting.\\n\\n---\\n\\n## 22. **Political Polling & Voter Analysis**\\n- **Contradiction:** Need for accurate, individualized profiling; integration of large, diverse data streams; avoidance of analytical complexity and bureaucratic bloat; and strict privacy/ethics adherence.\\n- **Solution Pattern:** **Randomized Response Technique**\\n- **Mechanism:** Polling uses statistical methods (randomized response, anonymized sampling, data aggregation) to infer group- and individual-level attitudes without ever directly tracking or connecting identities to specific responses; analytics operate on privacy-preserved aggregate layers.\\n- **Context:** Used in social science/marketing polling to study sensitive issues/voting patterns.\\n\\n---\\n\\n## 23. **Pharmaceutical Clinical Trials Management**\\n- **Contradiction:** Precise, individualized patient data; integrated analytics (across sites, populations); minimized complexity for coordinators/participants; and full privacy/ethics compliance.\\n- **Solution Pattern:** **Master Protocol Trials with Centralized Data De-identification**\\n- **Mechanism:** Multi-therapy trials deploy “master protocols” to standardize core data collection/analysis across drug arms, with data routed through secure, centralized anonymization hubs. Local sites enter data in a harmonized, simplified system, upholding both accuracy and privacy.\\n- **Context:** NIH/industry mega-trials (e.g., I-SPY 2 in cancer, COVID-19 vaccine trials).\\n\\n---\\n\\n## 24. **Religious Confession & Accountability Systems**\\n- **Contradiction:** Faith communities want fair, individualized spiritual “assessment”; accurate reckoning; protecting privacy/ethics; and avoiding intrusive complexity.\\n- **Solution Pattern:** **Anonymous Confessional Booth with Self-Reporting**\\n- **Mechanism:** The confessional booth enables individualized, honest spiritual accounting (“assessment”) without the confessor knowing the penitent’s identity (respects privacy/ethics); the priest applies standard, fair rituals without keeping records (avoids complexity and surveillance).\\n- **Context:** Found in Roman Catholic, Orthodox, and some Protestant traditions for centuries.\\n\\n---\\n\\n**[End of detailed solution pattern extraction per provided base domain and contradiction]**', 'analogical_transfer': 'Certainly. Below, for each base domain and contradiction, I propose how that source solution pattern can inspire novel solution framings for **Assessment Systems Innovation in Collaborative Learning**. For each, I: (1) name the original tension, (2) recap the base pattern and its mechanism, and (3) explain the analogical transfer to collaborative learning assessment, with adaptations and caveats.\\n\\n---\\n\\n### 1. **Music Production (Mastering & Signal Processing)**\\n- **Original Tension:** Desire for accurate assessment of learners\\' skills while collaborative activities generate workflow/tool complexity.\\n- **Base Solution Pattern:** *Standardized Signal Chain (“Limiter-First” Approach)*\\n    - **Mechanism:** Always place a crucial, accuracy-guaranteeing tool (the limiter), as the standard final stage, modularizing accuracy without hyper-complexity elsewhere.\\n- **Analogical Solution Framing:**\\n    - **Proposal:** In collaborative learning assessment, standardize the use of a “capstone” or “summative anchor” assessment for each project (e.g., team peer review, audit rubric, or critical reflection) that is always administered as the last stage. This reliably ensures core accuracy in skill measurement, independent of the variation/complexity in prior collaborative stages or tools used.\\n    - **Adaptations:** The “capstone” tool must be robust and modular, tolerating variation upstream while providing consistent, interpretable assessment results.\\n    - **Limitations/Considerations:** Requires careful calibration—overemphasis could contrain collaboration if too rigid; must integrate smoothly with preceding collaborative processes.\\n\\n---\\n\\n### 2. **Wildlife Population Ecology**\\n- **Original Tension:** High-fidelity, individual-level assessment of learning vs. system complexity in evaluating all collaborative interactions.\\n- **Base Solution Pattern:** *Mark-Recapture with Minimal Recapture Events*\\n    - **Mechanism:** Infer global properties (population size) from carefully sampled individuals/events, minimizing continuous complexity.\\n- **Analogical Solution Framing:**\\n    - **Proposal:** Implement intermittent “spot checks” in collaborative groups—have only a subset of participants complete detailed self- and peer-evaluations at random points, then statistically infer overall collaborative effectiveness and individual contributions using these data.\\n    - **Adaptations:** Sampling must be representative (e.g., randomized across roles, tasks, or time); requires a validated inference model.\\n    - **Limitations:** May miss rare or unevenly sampled learner issues; supplementary strategies may be needed for edge cases.\\n\\n---\\n\\n### 3. **Aviation Cockpit Design**\\n- **Original Tension:** Accurate, real-time assessment feedback to learners vs. complex, cumbersome assessment system layouts.\\n- **Base Solution Pattern:** *Primary Flight Display (PFD) Integration*\\n    - **Mechanism:** Integrate main indicators into a single, unified display for instant access and reduced cognitive overload.\\n- **Analogical Solution Framing:**\\n    - **Proposal:** Consolidate multiple collaborative assessment metrics (e.g., teamwork, communication, content mastery) into a unified “collaborative dashboard” visible to all learners and instructors, displaying real-time status and progress.\\n    - **Adaptations:** Must be intuitively designed, prioritizing actionable, aggregated indicators without oversimplifying.\\n    - **Limitations:** Risk of losing nuanced insights from individual metrics; requires good interface design and validation.\\n\\n---\\n\\n### 4. **Scientific Authorship & Credit Systems**\\n- **Original Tension:** Fair, individual-level credit in joint projects vs. burdensome, continuous oversight.\\n- **Base Solution Pattern:** *Contributor Roles Taxonomy (CRediT) Matrix*\\n    - **Mechanism:** Participants declare/claim specific contribution roles in a structured, transparent matrix at project’s close.\\n- **Analogical Solution Framing:**\\n    - **Proposal:** At the conclusion of collaborative learning tasks, have each student select and self-certify their contributions along a lightweight, standardized roles taxonomy (e.g., “idea generation”, “facilitation”, “editing”). Peer verification can optionally be layered in.\\n    - **Adaptations:** Taxonomy must cover relevant educational practices; process should be brief and resist gaming.\\n    - **Limitations:** Potential for inflation or misrepresentation unless cross-rated by peers.\\n\\n---\\n\\n### 5. **Correctional Behavior Monitoring**\\n- **Original Tension:** Detailed, individualized progress monitoring vs. overwhelming educator workload if too granular.\\n- **Base Solution Pattern:** *Risk Assessment Instruments with Periodic Self-Reporting*\\n    - **Mechanism:** Periodic, standardized self-reports trigger further scrutiny only when anomalous.\\n- **Analogical Solution Framing:**\\n    - **Proposal:** Employ brief, standardized self- and/or group progress check-ins at set intervals during collaborative projects. Escalate to in-depth reviews or interventions only when concerning patterns or self-reported issues are detected.\\n    - **Adaptations:** Thresholds for escalation must be carefully tuned.\\n    - **Limitations:** Some problems may evade detection if not self-reported or recognized by students.\\n\\n---\\n\\n### 6. **Open-Source Software Community Governance**\\n- **Original Tension:** Recognizing all contributions in collaborative work without impractical central monitoring.\\n- **Base Solution Pattern:** *Distributed Commit Log with Pull Request Review*\\n    - **Mechanism:** Every individual action is logged and peer-reviewed in a transparent, distributed system.\\n- **Analogical Solution Framing:**\\n    - **Proposal:** Integrate a collaborative work platform where all student edits, comments, and resource uploads are logged automatically. Periodic “pull requests” (for assessment milestones) can then be peer-reviewed, ensuring recognition and oversight with minimal instructor intervention.\\n    - **Adaptations:** Privacy must be maintained (especially in educational settings); peer review rubrics required.\\n    - **Limitations:** Some contributions (e.g., offline discussions) may be missed unless system usage is universal.\\n\\n---\\n\\n### 7. **Art Conservation (Mass vs. Individualized Approaches)**\\n- **Original Tension:** Scaling up collaborative assessment while ensuring authentic attention to unique efforts.\\n- **Base Solution Pattern:** *Triage and Modular Restoration*\\n    - **Mechanism:** Use triage to identify which items need individualized vs. standardized treatment.\\n- **Analogical Solution Framing:**\\n    - **Proposal:** Classify collaborative projects or interventions into tiers (e.g., high-risk, at-risk, routine); reserve deep, personalized assessment for “masterpieces” (complex or struggling teams), while applying streamlined, modularized evaluations to routine group work.\\n    - **Adaptations:** Classification criteria must be transparent and dynamic; students can be involved in self-categorization.\\n    - **Limitations:** Risk of “labeling” or missing latent struggles in standardized tiers.\\n\\n---\\n\\n### 8. **Epidemiology (Mass Screening vs. Diagnostic Precision)**\\n- **Original Tension:** Scalable assessment methods vs. precision and authenticity for every learner’s experience.\\n- **Base Solution Pattern:** *Two-Stage Screening: Rapid Test + Confirmatory Gold Standard*\\n    - **Mechanism:** Fast, mass screen to flag outliers for deeper, individualized analysis.\\n- **Analogical Solution Framing:**\\n    - **Proposal:** Deploy quick, standardized formative mini-assessments or peer checks throughout collaborative sessions; anyone flagged (low performance, non-participation, group concerns) receives a secondary, detailed skills or contribution assessment.\\n    - **Adaptations:** Needs careful tuning to avoid unnecessary extra assessment (“false positives”).\\n    - **Limitations:** Reliant on initial screen’s sensitivity and specificity.\\n\\n---\\n\\n### 9. **Theatre Education/Performance Festivals**\\n- **Original Tension:** Efficient mass assessment vs. meaningful individualized feedback in collaborative exhibitions.\\n- **Base Solution Pattern:** *Rotating Panel with Structured Comment Sheets*\\n    - **Mechanism:** Rotating expert panels use semi-structured forms for individualized, actionable feedback.\\n- **Analogical Solution Framing:**\\n    - **Proposal:** For class showcases or group projects, rotate external assessors or advanced students to review teams using focused rubrics that require at least one unique feedback point per learner or team.\\n    - **Adaptations:** Pre-training assessors and refining rubrics for educational relevance.\\n    - **Limitations:** Rotating experts must have sufficient context on group goals.\\n\\n---\\n\\n### 10. **Constitutional Law (Legal Codification vs. Judicial Discretion)**\\n- **Original Tension:** Desire for transparent, fair rules in collaborative assessment vs. need for flexible educator judgment for each group.\\n- **Base Solution Pattern:** *Balancing Tests in Jurisprudence*\\n    - **Mechanism:** Enumerate key factors to assess, but allow flexible weighting per case.\\n- **Analogical Solution Framing:**\\n    - **Proposal:** Create a standardized set of assessment criteria (e.g., contribution, teamwork, reflection, learning growth), but empower instructors to transparently adjust the weightings or prioritize criteria based on team-specific needs/challenges.\\n    - **Adaptations:** Adjustment rationale should be documented; students can participate in weighting decisions.\\n    - **Limitations:** Potential for perceived inconsistency unless process is open and justified.\\n\\n---\\n\\n### 11. **Fashion Design (Trend Openness vs. Designer Adaptivity)**\\n- **Original Tension:** Clear guidance for collaborative project goals/process vs. not constraining creativity/adaptivity.\\n- **Base Solution Pattern:** *Mood Boards with Modular Components*\\n    - **Mechanism:** Present open-ended, transparent parameters via modular, recombinable guides.\\n- **Analogical Solution Framing:**\\n    - **Proposal:** Use “assessment mood boards”—visual guides illustrating potential criteria, success exemplars, and collaborative process options, but allow students flexibility in assembling which and how components are emphasized in their final submission.\\n    - **Adaptations:** Board elements must map onto clearly assessable outcomes.\\n    - **Limitations:** Overly open boards may challenge some learners; optional scaffolding needed.\\n\\n---\\n\\n### 12. **Emergency Medicine Protocols**\\n- **Original Tension:** Transparent collaborative assessment rubrics vs. flexibility for novel or unexpected group challenges.\\n- **Base Solution Pattern:** *Algorithmic Protocol with Branch Points*\\n    - **Mechanism:** Standard protocol flows, divergence at context-specific “branch” markers.\\n- **Analogical Solution Framing:**\\n    - **Proposal:** Develop algorithmic, flowchart-like assessment protocols for group projects that allow predictable, explicit process steps, but have built-in branching for instructors (or teams) to adapt when flags or markers of unanticipated complexity/challenges appear.\\n    - **Adaptations:** Clear criteria for branch triggers; must train educators and teams in protocol use.\\n    - **Limitations:** Risk of “branching” too early/late—protocol calibration needed.\\n\\n---\\n\\n### 13. **Urban Surveillance and Smart Cities**\\n- **Original Tension:** Integrate collaborative learning analytics for insight while preserving student privacy.\\n- **Base Solution Pattern:** *Edge Data Anonymization*\\n    - **Mechanism:** Preprocess data locally to strip identifying details before aggregation.\\n- **Analogical Solution Framing:**\\n    - **Proposal:** Design assessment/analytics systems where collaboration logs and data are anonymized or pseudonymized on the group’s device before being accessed by instructors or aggregated for research, ensuring actionable insights without exposing identities.\\n    - **Adaptations:** Secure data pipelines; anonymization must preserve useful distinctions (e.g., roles, types of contributions).\\n    - **Limitations:** Some individual-level assessment granularity may be sacrificed.\\n\\n---\\n\\n### 14. **Personal Digital Health Devices (Wearables)**\\n- **Original Tension:** Rich collaborative data for accurate assessment vs. privacy/ethical concerns.\\n- **Base Solution Pattern:** *On-Device Data Aggregation with User-Controlled Sync*\\n    - **Mechanism:** Aggregate/process data privately; user controls what’s shared externally.\\n- **Analogical Solution Framing:**\\n    - **Proposal:** Equip collaborative learning platforms with settings that allow each team (or individual within teams) to review and choose which performance data they wish to share with educators/assessors, defaulting to only necessary summary metrics.\\n    - **Adaptations:** Ensure consent and informed data sharing; default summaries should meet key assessment needs.\\n    - **Limitations:** Students may withhold data of educational interest; balance between privacy and accountability.\\n\\n---\\n\\n### 15. **Anonymous Artistic Patronage**\\n- **Original Tension:** Desire for recognition/network effect in collaborative work, while some desire anonymity.\\n- **Base Solution Pattern:** *Blind Trust with Public Attribution Tokens*\\n    - **Mechanism:** Route contribution/recognition through an abstract, anonymizing proxy.\\n- **Analogical Solution Framing:**\\n    - **Proposal:** Enable teams or individuals in collaborative learning to opt for anonymous credit—contributions are logged via pseudonyms or tokens, actual identities held privately by system admin or “blind trust,” but aggregate outputs and recognition are attributed to the token.\\n    - **Adaptations:** Must prevent misuse (e.g., evading responsibility).\\n    - **Limitations:** Less useful for high-stakes, summative/credentialing contexts unless true identity can be revealed if necessary.\\n\\n---\\n\\n### 16. **Ecosystem Restoration Planning**\\n- **Original Tension:** Need for individualized progress data per team vs. assessment burden.\\n- **Base Solution Pattern:** *Standardized Monitoring Plots with Representative Sampling*\\n    - **Mechanism:** Sample intensively at representative sites, extrapolate findings system-wide.\\n- **Analogical Solution Framing:**\\n    - **Proposal:** Assign in-depth, rotating “assessment audits” to a subset of collaborative teams each cycle; detailed feedback and calibration checks happen here, with findings/lessons disseminated and, if appropriate, extrapolated for light-touch assessment of other teams.\\n    - **Adaptations:** Rotational and representative sampling design critical.\\n    - **Limitations:** Non-audited teams may diverge in quality without detection.\\n\\n---\\n\\n### 17. **Ancient Manuscript Authentication**\\n- **Original Tension:** In-depth, fine-grained insight into each group’s process/contributions vs. analytic overload.\\n- **Base Solution Pattern:** *Non-invasive Multispectral Imaging with Software-Assisted Analysis*\\n    - **Mechanism:** Capture all relevant signals in a single pass, analyze different layers via software.\\n- **Analogical Solution Framing:**\\n    - **Proposal:** Encourage collaborative teams to submit portfolios or recordings encapsulating all phases of the group process in a single artifact (e.g., collaborative doc, project \"audit trail\"). Educators use smart analytic tools/rubrics to extract multiple assessment layers from one submission.\\n    - **Adaptations:** Requires robust tools for extracting multiple dimensions of contribution/dynamics from single evidence stream.\\n    - **Limitations:** Upfront setup effort; possible learning curve for smart analytic tools.\\n\\n---\\n\\n### 18. **Corporate Performance Management**\\n- **Original Tension:** Individualized group member appraisal without intricate metric tracking.\\n- **Base Solution Pattern:** *360-Degree Appraisal with Forced Ranking*\\n    - **Mechanism:** Use focused peer/supervisor feedback and ranking to quickly triage individual performance.\\n- **Analogical Solution Framing:**\\n    - **Proposal:** Integrate periodic, structured peer-feedback cycles within collaborative groups, with contributions scored against anchored criteria; follow-up with “forced tiering” if needed to differentiate performance for awards or interventions.\\n    - **Adaptations:** Peer reviews must guard against bias or collusion.\\n    - **Limitations:** Forced ranking can be contentious if stakes are high.\\n\\n---\\n\\n### 19. **International Development Aid (Program Evaluation)**\\n- **Original Tension:** Scalable, transparent assessment vs. authenticity/adaptivity at the team level.\\n- **Base Solution Pattern:** *Logical Framework Approach (Logframe Matrix)*\\n    - **Mechanism:** Standardize metrics, allow local adaptation of indicators/targets.\\n- **Analogical Solution Framing:**\\n    - **Proposal:** Require teams to map their collaborative goals and self-defined indicators to a standard “logframe-style” assessment matrix, ensuring both transparency (top-level mapping) and flexibility (project-specific details).\\n    - **Adaptations:** Student/educator co-design of indicator mapping improves buy-in and fit.\\n    - **Limitations:** Some teams may need additional scaffolding.\\n\\n---\\n\\n### 20. **Jazz Ensemble Improvisation**\\n- **Original Tension:** Clear structure for collaborative assessment, yet supporting individual expression/adaptation.\\n- **Base Solution Pattern:** *Guided Head Arrangement with Solo Choruses*\\n    - **Mechanism:** Fixed structure (head), flexible, authentic solos within.\\n- **Analogical Solution Framing:**\\n    - **Proposal:** Structure collaborative assessments so each team must produce core deliverables (“the head”) using a shared template/rubric, but embed explicit space for individual or subgroup showcases (“solo”) to demonstrate unique value or process innovations.\\n    - **Adaptations:** “Solo” stages can be presentations, reports, or reflective essays.\\n    - **Limitations:** Balancing group and individual credit must be clear.\\n\\n---\\n\\n### 21. **Weather Forecasting**\\n- **Original Tension:** Scalable, accessible assessment data vs. flexible, specialized refinement for advanced learners/teams.\\n- **Base Solution Pattern:** *Multi-Layered Forecast Products*\\n    - **Mechanism:** Provide basic, clear outputs to all; advanced/customizable data to experts/specialists.\\n- **Analogical Solution Framing:**\\n    - **Proposal:** Offer all collaborative teams baseline assessment summaries and guidance, with optional “drill-down” analytic layers (raw collaboration logs, advanced rubrics) available for those wishing more tailored, nuanced feedback or for high-stakes contexts.\\n    - **Adaptations:** Must ensure deeper assessment options are accessible and interpretable.\\n    - **Limitations:** May require instructional support for advanced layers.\\n\\n---\\n\\n### 22. **Political Polling & Voter Analysis**\\n- **Original Tension:** Fine-grained insight and data integration for assessment, plus privacy and analytical simplicity.\\n- **Base Solution Pattern:** *Randomized Response Technique*\\n    - **Mechanism:** Use statistical inference on anonymized data to infer participant traits/attitudes.\\n- **Analogical Solution Framing:**\\n    - **Proposal:** Periodically survey collaborative groups or individuals using anonymized, privacy-protecting instruments on sensitive teamwork or participation indicators, then use statistical inference to gauge cultural/skill dynamics across the cohort without direct attribution.\\n    - **Adaptations:** Balancing insight and privacy; statistical technique selection.\\n    - **Limitations:** Lacks actionable individual-level intervention.\\n\\n---\\n\\n### 23. **Pharmaceutical Clinical Trials Management**\\n- **Original Tension:** Accurate, individualized data and learning analytics; full privacy; easy coordinator oversight.\\n- **Base Solution Pattern:** *Master Protocol Trials with Centralized Data De-identification*\\n    - **Mechanism:** Standardize data collection, anonymize centrally.\\n- **Analogical Solution Framing:**\\n    - **Proposal:** Use uniform templates/processes for team progress and performance data gathering, routing it through a centrally managed, privacy-respecting educational database—ensuring comparison, tracking, and research can occur with no identifying information attached.\\n    - **Adaptations:** Data reporting tools must be simple and secure.\\n    - **Limitations:** Some interventions may require de-anonymization in exceptional circumstances (e.g., at student request).\\n\\n---\\n\\n### 24. **Religious Confession & Accountability Systems**\\n- **Original Tension:** Honest, individualized self-assessment; privacy; minimal educator oversight burden.\\n- **Base Solution Pattern:** *Anonymous Confessional Booth with Self-Reporting*\\n    - **Mechanism:** Self-report honestly, anonymized; standard, light-touch review process.\\n- **Analogical Solution Framing:**\\n    - **Proposal:** Allow teams (or individuals within teams) to complete anonymous reflections/self-assessments on collaborative experience, reviewed by instructors for trends or concerns—but without linking to identities unless disclosure is explicitly granted.\\n    - **Adaptations:** Establish feedback channels for voluntary follow-up/intervention.\\n    - **Limitations:** Without follow-up, critical issues may remain unresolved.\\n\\n---\\n\\n## **Meta-Considerations for All Transfers**\\n- Many solution framings can/should be blended—e.g., using anonymization mechanisms (Patterns 13, 14, 15, 22, 23, 24) alongside multi-layered dashboards (Pattern 3, 21).\\n- Adaptability, transparency, and ethical considerations must be embedded at design stage, especially in educational settings.\\n- Key challenge is ensuring the innovation not only increases fairness, accuracy, and efficiency, but also improves learner ownership and outcomes.\\n\\n**In sum:** The above analogical mappings demonstrate diverse, actionable solution strategies for the complex tensions in collaborative learning assessment—each inspired by resolution mechanisms from a rigorously mapped source domain.', 'solution': 'Certainly! Here’s a detailed, structured response to the core problem—how to redesign group project systems to ensure **measurable individual mastery** while **retaining the collaborative benefits**—by evaluating the analogical solutions provided and selecting the most effective, balanced, and practical approaches.\\n\\n---\\n\\n## **Identifying the Core Needs and Challenges**\\n\\n**Key Problem Elements:**\\n- Current system: Structured group projects with rotating roles and peer evaluations.\\n- Issue revealed: Post-project **individual quiz scores** are *lower* than after individual work—i.e., **diluted accountability** or insufficient individual mastery.\\n- Goal: **Measurably ensure individual learning** (accountability/mastery) **without losing the motivational and skill-building power of collaboration**.\\n\\n**Guiding Criteria for Best Solutions:**\\n1. **Practicality**: Feasible to implement by teachers at scale.\\n2. **Innovation**: Introduces new or underutilized structures/processes.\\n3. **Balance**: Maintains group benefits while correcting for drift in individual mastery/accountability.\\n4. **Assessment Integrity**: Provides truly individualized, actionable learning data.\\n5. **Fairness/Transparency**: Clear rationale, buy-in from students, and ethical treatment.\\n\\n---\\n\\n## **Evaluating and Selecting Solution Patterns**\\n\\nBelow are the **most promising options**, with rationale and practical considerations. These are selected and synthesized from your analogies for maximum impact and feasibility.\\n\\n---\\n\\n### **1. Capstone Individual Assessment (Inspired by “Summative Anchor”)**\\n\\n**Description:**  \\nMake it a standard process that every group project concludes with a robust, individual summative assessment—a quiz, reflection, or individually-authored section—explicitly designed to measure each student’s mastery of the targeted concepts.\\n\\n**Why It Works:**  \\n- Ensures that despite team participation, **each student must demonstrate individual understanding** in a format similar to solo work.\\n- Modular: Can be added onto any project structure.\\n- High fidelity: Results can be directly compared to prior individual assessments.\\n\\n**Implementation Tips:**\\n- The individual assessment should be closely aligned with both the group project content and the learning objectives.\\n- Can alternate formats: written quizzes, oral defenses, reflective essays, etc.\\n- To keep it fresh and authentic, vary assessment types but keep standards consistent.\\n\\n**Potential Limits:**\\n- If too dominant, can make the group work seem instrumental rather than intrinsic.\\n- Requires time for marking and providing feedback.\\n\\n\\n---\\n\\n### **2. Transparent Roles and Peer Contribution Mapping (Inspired by “Contributor Roles Taxonomy” and “Commit Log/Peer Review”)**\\n\\n**Description:**  \\nAt the end (or at intervals), each student **records specific contributions** across a standardized set of roles (e.g., research, design, leadership, writing, editing), which is then **peer-verified**. Optionally, contributions can be logged throughout via collaborative platforms (e.g., Google Docs, project management tools).\\n\\n**Why It Works:**  \\n- Increases **accountability and visibility** of individual effort within the collaborative process.\\n- Peer review or verification **discourages exaggeration** or free-riding.\\n- Allows educators to identify and support underperforming or disconnected students early.\\n- Logging contributions digitally (in collaborative docs/Wikis) provides transparent evidence.\\n\\n**Implementation Tips:**\\n- Use a simple, intuitive rubric for self/peer-reporting.\\n- Regular, smaller check-ins may catch issues earlier than just post-project.\\n- Train students in fair and honest feedback; consider cross-validating with instructor observations.\\n\\n**Potential Limits:**\\n- If too granular or frequent, may feel burdensome or “policing”.\\n- Needs a healthy class culture to minimize “gaming” or bias.\\n\\n---\\n\\n### **3. Integrated “Solo Within the Group” Component (Inspired by “Jazz Ensemble: Solo Within Head”, “Portfolio/Process Audit Trail”)**\\n\\n**Description:**  \\nDesign projects so that, alongside collective outputs, **each student must produce (or present) an “individual showcase” closely connected to group work**—for example, a solo paper, reflection, or presentation focusing on their unique insights, learning, or contribution.\\n\\n**Why It Works:**\\n- Provides **clear opportunities for personal mastery and voice**, even within teamwork.\\n- “Solo” moments can be structured for depth and ownership (e.g., each student presents on a portion of the project or writes an individual “takeaway” essay).\\n- Connects assessment transparently to both group process and individual outcomes.\\n\\n**Implementation Tips:**\\n- Have these individual components link explicitly to both group goals and personal learning objectives.\\n- May rotate “solo” focus across different projects (e.g., one student leads analysis, another synthesis).\\n- Allow flexibility in how students express mastery (presentation, written work, creative product).\\n\\n**Potential Limits:**\\n- May need additional scaffolding/support for less confident students.\\n- Care needed to ensure “solo” components are rigorous and not just perfunctory.\\n\\n---\\n\\n### **4. Targeted Sampling and Two-Stage Assessments (Inspired by “Mass Screening/Confirmatory Assessment” and “Mark-Recapture”)**\\n\\n**Description:**  \\nUse **periodic, quick formative quizzes or spot-checks** during group work. If a student scores below a mastery threshold or concerning patterns arise (e.g., disengagement), the system triggers a **more in-depth individual assessment or intervention**.\\n\\n**Why It Works:**\\n- Focuses individual mastery verification *where it’s most needed*, minimizing extra work for all.\\n- Maintains efficiency for the majority, but offers **precision support for outliers**.\\n- Encourages sustained individual accountability throughout.\\n\\n**Implementation Tips:**\\n- Define clear thresholds/triggers for second-stage assessment.\\n- Communicate to students the purpose is support and assurance, not “catching out.”\\n- Adjust frequency/intensity based on evidence of learning loss or group drift.\\n\\n**Potential Limits:**\\n- Relies on having sensitive enough quick-assessments.\\n- Could miss “hidden” issues if students mask involvement in groups.\\n\\n---\\n\\n### **5. Rotating Deep Dives and External Reviews (Inspired by “Art Conservation Triage” and “Rotating Panel Feedback”)**\\n\\n**Description:**  \\nPeriodically, select a **subset of teams/students for in-depth feedback or external review** (by advanced students, teaching assistants, or external partners), with lessons disseminated broadly.\\n\\n**Why It Works:**\\n- Ensures **authentic, rigorous assessment for a manageable number** at a time.\\n- Promotes sharing of best practices and lessons learned back to the cohort.\\n- Keeps overall workload feasible for educators.\\n\\n**Implementation Tips:**\\n- Rotate selection transparently to include all over time.\\n- Use structured rubrics to guide external reviewers.\\n- Publicize anonymized findings/discussion points for learning benefit.\\n\\n**Potential Limits:**\\n- Those not sampled may receive less feedback—needs careful scheduling to ensure fairness.\\n- May generate stress if perceived as “inspection week.”\\n\\n---\\n\\n## **Best Blended System: Practical, Balanced, and Innovative**\\n\\nCombining these leading patterns suggests the following **multi-layered system**:\\n\\n> 1. **Every collaborative project includes:**\\n>     - **A final, individually-assessed mastery task** (quiz, reflection, interview, or solo product).\\n>     - **A transparent, peer-verified record of contributions** (with digital logging where feasible).\\n>     - **A built-in “solo showcase” component** for each participant.\\n>\\n> 2. **At intervals, use rapid in-group spot-checks/quizzes to pinpoint students who need additional support or assessment.**\\n> \\n> 3. **Periodically, select teams/individuals for deeper, externalized feedback or audit, rotating so all benefit over time.**\\n\\n**Supporting Principles:**\\n- Keep **assessment documentation light but regular**.\\n- Leverage technology (learning platforms, wikis, collaboration logs) judiciously.\\n- Communicate goals and reasoning to students to build buy-in and transparency.\\n- Continually **tune thresholds, roles, and structures based on data and feedback**.\\n\\n---\\n\\n## **Summary Table: Leading Patterns at a Glance**\\n\\n| Solution                            | Individual Mastery | Collaborative Spirit | Instructor Feasibility | Student Transparency |\\n|--------------------------------------|:------------------:|:-------------------:|:---------------------:|:-------------------:|\\n| Capstone Individual Assessment       | High               | Maintained          | Medium                | High                |\\n| Peer Contribution Mapping + Logging  | Medium-High        | High                | High                  | High                |\\n| “Solo Within Group” Components       | High               | High                | Medium                | High                |\\n| Spot Checks + Targeted Follow-ups    | High (for flagged) | High                | High                  | High                |\\n| Rotating Deep Dives/Reviews          | High (per group)   | Maintained          | High (over time)      | Medium-High         |\\n\\n---\\n\\n## **Conclusion & Recommendation**\\n\\n**No single method suffices alone**, but a **layered design that combines robust individual assessments with transparent contribution tracking and intermittent, targeted checks** offers a practical, innovative, and balanced solution. This approach will:\\n\\n- Ensure measurable, fair, and actionable evidence of individual achievement.\\n- Preserve and highlight the positive dynamics of collaboration.\\n- Remain feasible for teachers and fair to students.\\n\\n**Action Steps for Educators:**\\n- Revise project structures to *always* include an individual mastery assessment component.\\n- Implement lightweight peer contribution mapping.\\n- Design solo moments or deliverables within group projects.\\n- Use periodic spot checks to catch disengagement or confusion early.\\n- Schedule rotating, deeper feedback cycles for both formative and summative purposes.\\n\\nWith these measures, you can advance both collaboration and accountability—producing higher individual mastery as verified by your post-project quizzes, without sacrificing the crucial benefits of teamwork.'}\n"
     ]
    }
   ],
   "source": [
    "## Create Analogical Reasoning response\n",
    "input_state = {\"user_query\": test_q}\n",
    "\n",
    "final_state = graph.invoke(input_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2edba8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse_output\n",
    "import re\n",
    "\n",
    "def parse_solution(text):\n",
    "    # Remove Markdown formatting like \"**\" and \"\\n\"\n",
    "    clean_text = text.replace(\"**\", \"\").replace(\"\\\\n\", \"\\n\")\n",
    "\n",
    "    # Split into sections by horizontal rules (---)\n",
    "    sections = re.split(r'-{3,}', clean_text)\n",
    "\n",
    "    # Create a readable version of each section\n",
    "    readable_output = []\n",
    "    for section in sections:\n",
    "        section = section.strip()\n",
    "        if not section:\n",
    "            continue\n",
    "        # Optional: separate title and body if present\n",
    "        lines = section.split(\"\\n\")\n",
    "        if len(lines) > 1 and \":\" not in lines[0]:\n",
    "            # First line is likely a heading\n",
    "            heading = lines[0]\n",
    "            body = \"\\n\".join(lines[1:])\n",
    "            readable_output.append(f\"\\n=== {heading} ===\\n{body}\")\n",
    "        else:\n",
    "            readable_output.append(section)\n",
    "\n",
    "    return \"\\n\\n\".join(readable_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d3c50c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all parsed outputs to a text file\n",
    "with open(\"sample_output.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for key in final_state:\n",
    "        raw_output = str(final_state[key])  # Ensure it's a string\n",
    "        final_output = parse_solution(raw_output)\n",
    "        f.write(f\"\\n### {key} ###\\n\")\n",
    "        f.write(final_output)\n",
    "        f.write(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fc3c62",
   "metadata": {},
   "source": [
    "#### Without Analogical Reasoning - GPT 4.1 RAW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a6aa161c-0873-49c4-9bcc-85d644311d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GPT-4.1 RAW Response ===\n",
      "\n",
      "This is a thoughtful and common challenge in collaborative learning: preserving the benefits of teamwork (communication, collaboration, deeper thinking) and ensuring individual mastery and accountability.\n",
      "\n",
      "Here are targeted strategies educators can implement:\n",
      "\n",
      "\n",
      "=== ### 1. Hybrid Grading Approach ===\n",
      "- Dual Components: Make the final project grade a combination of group and individual assessments (e.g., 50% group outcome, 50% individual evidence).\n",
      "- Individual Quizzes or Reflections: Require each student to complete a quiz, written reflection, or oral explanation showing their understanding of the key science concepts behind the project.\n",
      "\n",
      "### 2. Role-Specific Deliverables\n",
      "- When rotating roles, assign each student role-specific, individual deliverables. For example, if one student is the \"researcher,\" they must individually summarize sources or findings.\n",
      "- Collect and grade these outputs separately to verify individual understanding.\n",
      "\n",
      "### 3. Regular Formative Check-ins\n",
      "- Schedule short, individual check-ins (exit tickets, clicker questions, journal entries) throughout the project timeline on fundamental concepts.\n",
      "- Use these low-stakes assessments to track progress and address gaps early.\n",
      "\n",
      "### 4. Randomized Oral or Written Defense\n",
      "- After project completion, randomly select students (or all students, if feasible) to give a brief, individual explanation of a portion of the project or answer follow-up questions.\n",
      "- This encourages personal responsibility for understanding the entirety of the project.\n",
      "\n",
      "### 5. Peer Teaching Moments\n",
      "- Include sessions where students must teach a segment of the project material to a peer or small group and are assessed on clarity and accuracy. Peer-teaching boosts accountability for individual learning.\n",
      "\n",
      "### 6. Transparent Criteria\n",
      "- Use rubrics that clearly separate teamwork skills (collaboration, contribution, communication) from content mastery, and make expectations explicit to students.\n",
      "\n",
      "### 7. Self and Peer Assessments Weighted for Accountability\n",
      "- Use calibrated self- and peer-evaluations (ideally, structured and not just subjective ratings) as just one part of the overall grade, not the entirety.\n",
      "- Combine with teacher verification or evidence-based checkpoints.\n",
      "\n",
      "Sample Redesigned System:\n",
      "| Assessment Type             | Weight | Description                                                                            |\n",
      "|\n",
      "\n",
      "|\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "=== | ===\n",
      "| Group Project Outcome       | 40%    | Graded as a team: product demonstrates effective collaboration and solid science        |\n",
      "| Individual Quiz/Reflection  | 30%    | Each student completes an assessment on key project science concepts                   |\n",
      "| Role-Specific Deliverable   | 20%    | Each student submits evidence of their unique contribution                             |\n",
      "| Peer/Self Assessment        | 10%    | Structured feedback on teamwork, professionalism, responsibility                       |\n",
      "\n",
      "Bottom Line:  \n",
      "Assess individual learning in parallel with group achievements.  \n",
      "This balance preserves collaboration’s power while guaranteeing everyone is learning. Consider explaining to students *why* both teamwork and individual mastery matter—explicit alignment can foster buy-in.\n",
      "\n",
      "If you’d like, share more about the exact subject/project, and I can provide a more tailored set of sample assignments or assessment formats!\n"
     ]
    }
   ],
   "source": [
    "## Without analogical reasoning - raw LLM output\n",
    "result = basic_output(test_q)\n",
    "\n",
    "print(\"\\n=== GPT-4.1 RAW Response ===\\n\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb46176",
   "metadata": {},
   "source": [
    "#### Without Analogical Reasoning but with COT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "15022384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GPT-4.1 Response WITH COT ===\n",
      "\n",
      "Absolutely—this is a great, nuanced question about balancing collaboration with individual mastery. Here’s a step-by-step approach educators can use:\n",
      "\n",
      "\n",
      "=== ### 1. Analyze the Problem ===\n",
      "\n",
      "- Observation: Teamwork improved via group projects, but individual accountability dropped—reflected by lower post-project quiz scores.\n",
      "- Challenge: Group work risks “social loafing” (some students do less), hiding who hasn’t mastered the content.\n",
      "\n",
      "\n",
      "=== ### 2. Clarify Goals ===\n",
      "\n",
      "- Maintain benefits of teamwork (communication, peer learning, problem-solving).\n",
      "- Ensure each student achieves and can demonstrate individual mastery.\n",
      "\n",
      "\n",
      "=== ### 3. Redesign Steps ===\n",
      "\n",
      "#### Step 1: Incorporate Regular Individual Accountability Within Groups\n",
      "\n",
      "- Frequent Individual Checks: Mix in *individual* exit tickets, checkpoints, or short quizzes during the project phase—not just at the end. This keeps everyone engaged and signals that individual mastery is always required.\n",
      "- Randomized Individual Interview or Defense: Occasionally (and unpredictably), have students individually defend or explain a project aspect to you or the class. This encourages participation and preparation.\n",
      "\n",
      "#### Step 2: Structure Projects for Individual Products Within a Collaborative Framework\n",
      "\n",
      "- Hybrid Deliverables: Design group projects with both a group product *and* an individual component (individual reflections, mini-reports, or analysis sections).  \n",
      "    - Example: Group develops and presents an experiment; each student writes their own analysis or conclusion.\n",
      "\n",
      "#### Step 3: Use Peer and Self-Assessment Strategically\n",
      "\n",
      "- Peer Assessment Tied to Learning Goals: Structure peer evaluations so they specifically address each member’s contribution and understanding of core concepts, not just effort/helpfulness.\n",
      "- Self-Assessment: Ask students to assess their learning and contributions, helping them self-identify gaps.\n",
      "\n",
      "#### Step 4: Scaffold Team Roles Towards Individual Accountability\n",
      "\n",
      "- Rotate Roles With Individual Ownership: Assign roles requiring each student to master certain topics, then test or quiz on that area individually.\n",
      "- Role-Based Quizzing: If a student was the “data analyst” in the group, quiz them specifically about data analysis methods used.\n",
      "\n",
      "#### Step 5: Separate Group and Individual Grading\n",
      "\n",
      "- Weighted Grades: Make a significant portion of the project grade individual (based on quizzes, reflections, interviews), and a portion group-based (final product, collaboration).\n",
      "    - Example: 60% individual, 40% group.\n",
      "\n",
      "#### Step 6: Build in Post-Project Individual Assessment and Remediation\n",
      "\n",
      "- Immediate Post-Project Quiz: Right after a project ends, give an individual quiz on the underlying concepts.\n",
      "- Targeted Remediation: For students who underperform, require additional (brief) individual assignments/reflection to close the gap.\n",
      "\n",
      "\n",
      "=== ### 4. Monitor and Adjust ===\n",
      "\n",
      "- Use Ongoing Data: Regularly compare individual assessments before and after implementing new systems to see if changes are effective.\n",
      "- Solicit Feedback: Keep open anonymous feedback channels for students about their sense of fairness and responsibility.\n",
      "\n",
      "\n",
      "=== ### Concrete Example Implementation ===\n",
      "\n",
      "Imagine a group builds a model rocket (project).  \n",
      "- During the build: Each student submits quick individual “explain what we’re doing and why” logs.\n",
      "- After the build: The group submits one launch report.\n",
      "- But: Each student writes *their own* analysis of why their launch did/didn’t work, linking to scientific principles—and takes a post-project quiz.\n",
      "- Grades: 40% group build/report, 60% own analysis + quiz.\n",
      "\n",
      "\n",
      "=== ### Summary Table ===\n",
      "\n",
      "| Step                        | Action                                                    |\n",
      "|\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "=== | ===\n",
      "| Individual Accountability   | Short in-class quizzes, explanations, reflections         |\n",
      "| Hybrid Deliverables         | Group AND individual assignment components                |\n",
      "| Role and Peer Assessment    | Tie roles and peer evals to content, not just participation |\n",
      "| Weighted Grading            | Clear split between individual/group grades               |\n",
      "| Post-Project Assessment     | Individual quiz + remediation as needed                   |\n",
      "\n",
      "Final Thought:  \n",
      "Collaboration and accountability aren’t mutually exclusive. Well-structured hybrid systems make teamwork rewarding *and* ensure every student achieves mastery.\n"
     ]
    }
   ],
   "source": [
    "## Without analogical reasoning - raw LLM output\n",
    "result = basic_output_COT(test_q)\n",
    "\n",
    "print(\"\\n=== GPT-4.1 Response WITH COT ===\\n\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a23f11b",
   "metadata": {},
   "source": [
    "#### Without Analogical Reasoning but with Basic Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "98356bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GPT-4.1 Response WITH Prompt Engineering ===\n",
      "\n",
      "You’ve surfaced a classic tension in group work: collaboration boosts engagement, but can mask—or even reduce—individual learning gains. To strike a balance, consider a redesign that weaves together individual accountability and collaborative skill-building. Here are practical-yet-innovative strategies:\n",
      "\n",
      "\n",
      "=== ### 1. Hybrid Grading Models ===\n",
      "\n",
      "- Individual & Group Scores Blend  \n",
      "  Assign a percentage of the project grade to individual contributions (e.g., quiz/performance, reflection, or portfolio) and another to the group product. For instance, 60% individual, 40% group. Make this transparent to students.  \n",
      "  *Why it works:* Students have personal “skin in the game,” incentivizing accountability.\n",
      "\n",
      "- Team Contract with Individual Goals  \n",
      "  At project kickoff, require each student to articulate learning goals and anticipated contributions. Check-in mid-project on these.  \n",
      "  *Why it works:* Promotes self-awareness and specific expectations, which you can reference during assessment.\n",
      "\n",
      "\n",
      "=== ### 2. Intertwined Solo and Group Tasks ===\n",
      "\n",
      "- “Jigsaw” Method Plus Solo Synthesis  \n",
      "  Each student becomes an expert on one aspect, teaches it to the group, then completes a related solo assessment (quiz, short essay) to demonstrate personal mastery.  \n",
      "  *Why it works:* Builds deep understanding and ensures each student internalizes content.\n",
      "\n",
      "- Rotating “Lead Scientist” Roles  \n",
      "  For each major activity, one student is responsible for leading, explaining concepts, and submitting a summary. Rotate this role so all have leadership moments evaluated — both by teacher and group.\n",
      "\n",
      "\n",
      "=== ### 3. Individual Reflection and Metacognition ===\n",
      "\n",
      "- Reflective Journals or Video Diaries  \n",
      "  Students regularly (e.g. after key milestones) reflect on both their and their group’s learning, hurdles, and the application of core concepts, citing specific evidence from the project.  \n",
      "  *Why it works:* Deepens content retention and gives you direct insight into what *each* student has learned.\n",
      "\n",
      "\n",
      "=== ### 4. Low-Stakes, Frequent Individual Checks ===\n",
      "\n",
      "- Mini-Quizzes & Concept Maps  \n",
      "  Short, scaffolded individual assessments at intervals during the project. Consider quick in-class polling, digital quizzes, or having students draw concept maps in response to a prompt.  \n",
      "  *Why it works:* Frequent feedback for both student and teacher, allowing you to catch gaps early.\n",
      "\n",
      "\n",
      "=== ### 5. Transparent Peer Feedback With Evidence ===\n",
      "\n",
      "- \"Show Your Work\" Peer Assessments  \n",
      "  Require students to cite concrete examples (e.g., “Sam researched X and explained Y during the discussion”) in peer evaluations. Pair this feedback with your observational notes.  \n",
      "  *Why it works:* Reduces bias and encourages students to value substantive contributions.\n",
      "\n",
      "\n",
      "=== ### Combining These Approaches ===\n",
      "\n",
      "For best results, layer these methods. For example:  \n",
      "1. Launch with a team contract and division of labor.\n",
      "2. Alternate between group work and individual “jigsaw” quizzes.\n",
      "3. Integrate reflection checkpoints.\n",
      "4. Conclude with both group product and individual synthesis tasks.\n",
      "\n",
      "Bottom line:  \n",
      "Delivering direct, measurable individual assessments at key points within a collaborative structure ensures accountability without sacrificing the social construction of knowledge. Students learn together but must also independently demonstrate mastery—both process and product are valued.\n",
      "\n",
      "Bonus: Involve students in co-designing parts of the system. This fosters buy-in and often surfaces creative accountability ideas you’d never consider alone!\n",
      "\n",
      "References for further reading:  \n",
      "- Slavin, R. E. (1996). Research on Cooperative Learning and Achievement: What We Know, What We Need to Know.  \n",
      "- Oakley, B., Felder, R. M., Brent, R., & Elhajj, I. (2004). Turning Student Groups into Effective Teams.\n",
      "\n",
      "Let me know if you want sample rubrics or quick templates!\n"
     ]
    }
   ],
   "source": [
    "## Without analogical reasoning - raw LLM output\n",
    "result = basic_output_prompt_engin(test_q)\n",
    "\n",
    "print(\"\\n=== GPT-4.1 Response WITH Prompt Engineering ===\\n\")\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analogical-lc-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
