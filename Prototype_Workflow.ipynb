{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9f00062d-0f67-46b5-aca0-b7857d11c4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing import TypedDict, List, Dict, Any\n",
    "import openai \n",
    "# from langgraph_supervisor import create_supervisor\n",
    "from langchain.chat_models import init_chat_model\n",
    "from textwrap import dedent\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c9c75eb6-69bf-40c9-8ed4-8b96c4d1f285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts\n",
    "TARGET_DOMAIN = dedent(\"\"\"\n",
    "        As a Domain Analysis Specialist, extract all the core innovation domains from the user query. It could be a single one for a simple query, or multiple ones for a complex query.\n",
    "        Instructions:\n",
    "        1. Analyze the user's input\n",
    "        2. Identify the primary domain(s) requiring innovation\n",
    "        3. Classify it within standard innovation categories\n",
    "        Output Format:\n",
    "        Target Domain: [Clear, specific domain label]\n",
    "        Be very detailed and specific in your response and do not generalize. Respond ONLY with the name of the domain, do NOT include ANY other text like 'Target Domain:'.\n",
    "        User query: {user_query}\n",
    "\"\"\").strip()\n",
    "\n",
    "PROBLEM_LANDSCAPE = dedent(\"\"\"\n",
    "        You are a Problem Landscape Analyst. Your task is to map out the concrete challenges within the target domain identified.\n",
    "        Instructions:\n",
    "        1. Identify all the core problems or challenges currently present in these domains. Aim for at least 3 problems per domain\n",
    "        2. For each problem, provide:\n",
    "        - Problem: A short, clear title.\n",
    "        - Description: 2-3 sentences explaining what the problem is and why it matters.\n",
    "        - Context: Briefly state the circumstances or environment where this problem occurs.\n",
    "        - Stakeholders: List the main groups or individuals affected.\n",
    "        - Root Causes: Identify 1-3 underlying causes, if known.\n",
    "        - Impact: State the significance of the problem (e.g., social, economic, technical).\n",
    "        - Current Approaches: How is this problem currently addressed?\n",
    "        - Limitations: What are the shortcomings of current approaches?\n",
    "        - Success Metrics: How would you measure if this problem is solved?\n",
    "        - Interconnections: Note if this problem is linked to or influenced by other problems.\n",
    "        Output Format:\n",
    "        Present your findings as a structured list or JSON array, with each problem fully described as above.\n",
    "        Important:\n",
    "        - Focus on clarity and completeness.\n",
    "        - Avoid abstracting or generalizing; stay concrete and domain-specific.\n",
    "        - Do not propose solutions; only describe the current problem landscape.\n",
    "        Target domain: {target_domain}\n",
    "\"\"\").strip()\n",
    "\n",
    "ABSTRACTION = dedent(\"\"\"\n",
    "You are a TRIZ Methodology Expert. Transform domain-specific problems into universal contradictions.\n",
    "        Process:\n",
    "        1. Read up on TRIZ - the contradiction matrix, and the inventive principles\n",
    "        2. For each problem in the problem landscape:\n",
    "        - Abstract to universal parameters (what improves vs. what worsens)\n",
    "        - Express as 'When we improve X, Y worsens'\n",
    "        - Ensure parameters are domain-agnostic\n",
    "        3. Analyze all the abstracted universal parameters, and identify all the core TRIZ contradictions present:\n",
    "        - Select the most fundamental tensions\n",
    "        - Map to TRIZ contradiction matrix\n",
    "        - Note applicable inventive principles\n",
    "        Output:\n",
    "        # List all the core contradictions in form of:\n",
    "        - Improving [parameter] vs. Worsening [parameter]\n",
    "        - TRIZ Principles: [1-3 relevant principles]\n",
    "        - Innovation Potential: [High/Medium/Low]\n",
    "        Focus on contradictions that, if resolved, would create breakthrough value.\n",
    "\n",
    "        Problem landscape: {problem_landscape}\n",
    "\"\"\").strip()\n",
    "\n",
    "BASE_DOMAIN = dedent(\"\"\"\n",
    "        You are a Cross-Domain Search Specialist. Do the following:\n",
    "        - For each contradiction provided, identify 3 distinct source domains (fields or industries) where this contradiction has been successfully addressed.\n",
    "        - Experiment with different subsets of the list of contradictions, and see if you could identify 3 distinct source domains for each of these subsets identified as well. You should find at least 3 different subsets.\n",
    "        Note: The domains should have A CONCEPTUAL DISTANCE OF AT LEAST 3 DISTINCT HOPS FROM WHAT IMMEDIATELY COMES TO MIND. Be creative! It can be domains within spheres like natural, phsyical, social, artistic, or anything.\n",
    "        For each domain identified, briefly explain why it is relevant to the single contradiction or the subset of contradictions identified. Do not describe specific solutions just yet-only list the domains and your rationale.\n",
    "        Output:\n",
    "        A list for each contradiction and subset of contradictions identified, naming 3 relevant domains with a 2 sentence rationale for each.\n",
    "        Aim for a total of at least 20 relevant base domains. \n",
    "        Contradictions: {contradictions}\n",
    "\"\"\").strip()\n",
    "\n",
    "BASE_SOLUTIONS = dedent(\"\"\"\n",
    "        You are a Solution Pattern Extractor. You are provided with an input with 3 base domains identified per TRIZ (Theory of Inventive Problem Solving) contradiction or a set of contradictions, as well as the contradictions themselves.\n",
    "        For each of these identified base domains, identify one specific, well-documented solution pattern within the domain that effectively resolves the contradiction (or the set of contradictions).\n",
    "        For each solution pattern, return:\n",
    "        - Identify the base domain it's corresponding to\n",
    "        - Recall the contradiction or the set of contradictions that this base domain faces\n",
    "        - The name or label of the solution pattern for resolving these contradiction(s) in the base domain\n",
    "        - A detailed description of the core mechanism or principle involved and how it addressed the domain's contradiction(s)\n",
    "        - The context or situation in the domain where this pattern is applied\\n\"\n",
    "        Do not generalize or adapt the solution-simply describe how the contradiction is addressed within each source domain.\n",
    "        Output:\n",
    "        For each of the provided domain, list the base domain name, contradiction(s) faced, solution pattern name, the detailed description of the mechanism of the solution pattern, and the context in which it is used. Articulate the contradictions as problems and considerations faced, through framing them as a tension.\n",
    "\n",
    "        Input: {input}\n",
    "\"\"\").strip()\n",
    "\n",
    "ANALOGICAL_TRANSFER = dedent(\"\"\"\n",
    "        You are a very innovative Analogical Transfer Specialist.\n",
    "        You are provided with list the base domain name, tensions faced, solution pattern name, the detailed description of the mechanism of the solution pattern, and the context in which it is used.\n",
    "        Your task is to propose how solution patterns used to resolve these tensions in various base domains might inspire solution framings for the original target domain.\n",
    "\n",
    "        Input Overview:\n",
    "        1. A list the base domains identified, the tensions these domains faced, the name of solution patterns that helped addressed these tensions in these base domains, the detailed description of the mechanism of the solution pattern, and the context in which it is used.\n",
    "        2. The original target domain.\n",
    "\n",
    "        Instructions:\n",
    "        For each pair of base domain and the corresponding tensions identified, review the solution patterns that worked for the base domain. For each pattern:\n",
    "        - Analyze the core mechanism or principle behind the solution.\n",
    "        - Map and adapt this mechanism conceptually to the target domain, considering the specific context and needs of the target domain.\n",
    "        - Clearly describe how this analogical transfer could frame a potential solution in the target domain.\n",
    "        - Highlight any key adaptations, considerations, or limitations that would be relevant when applying this pattern to the target domain.\n",
    "\n",
    "        Your expected Output:\n",
    "        For each base domain, provide a comprehensive description of a proposed solution framing for the target domain, including:\n",
    "        - The original tension addressed\n",
    "        - The source domain and solution pattern\n",
    "        - A detailed explanation of how the pattern could inspire or inform a solution in the target domain\n",
    "        - Any important adaptations or considerations for successful transfer\n",
    "\n",
    "        Here are the actual inputs:\n",
    "        - A list the base domains identified, the tensions these domains faced, the name of solution patterns that helped addressed these tensions in these base domains, the detailed description of the mechanism of the solution pattern, and the context in which it is used.: {contradictions_solutions}\n",
    "        - Original target domain: {target_domain}\n",
    "\"\"\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8e573d7e-e43f-4c7d-8752-ae11a6198c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for openai API\n",
    "def _set_if_undefined(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"Please provide your {var}\")\n",
    "\n",
    "\n",
    "_set_if_undefined(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8d2d7f38-3881-4d8d-80fc-542b5918d14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse_output\n",
    "import re\n",
    "\n",
    "def parse_solution(text):\n",
    "    # Remove Markdown formatting like \"**\" and \"\\n\"\n",
    "    clean_text = text.replace(\"**\", \"\").replace(\"\\\\n\", \"\\n\")\n",
    "\n",
    "    # Split into sections by horizontal rules (---)\n",
    "    sections = re.split(r'-{3,}', clean_text)\n",
    "\n",
    "    # Create a readable version of each section\n",
    "    readable_output = []\n",
    "    for section in sections:\n",
    "        section = section.strip()\n",
    "        if not section:\n",
    "            continue\n",
    "        # Optional: separate title and body if present\n",
    "        lines = section.split(\"\\n\")\n",
    "        if len(lines) > 1 and \":\" not in lines[0]:\n",
    "            # First line is likely a heading\n",
    "            heading = lines[0]\n",
    "            body = \"\\n\".join(lines[1:])\n",
    "            readable_output.append(f\"\\n=== {heading} ===\\n{body}\")\n",
    "        else:\n",
    "            readable_output.append(section)\n",
    "\n",
    "    return \"\\n\\n\".join(readable_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a188a9e3-c538-487f-9dde-8dfe295575db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph state: workflow\n",
    "# class ReasoningState(TypedDict):\n",
    "#     user_query: str\n",
    "#     target_domain: str\n",
    "#     problem_landscape: str\n",
    "#     abstraction: str\n",
    "#     abstraction_feedback: str\n",
    "#     base_domain: str\n",
    "#     base_solutions: str\n",
    "#     analogical_transfer: str\n",
    "#     transfer_feedback: str\n",
    "#     solution: str\n",
    "\n",
    "\n",
    "class ReasoningState(TypedDict):\n",
    "    user_query: str\n",
    "    target_domain: str\n",
    "    problem_landscape: str\n",
    "    abstraction: str\n",
    "    abstraction_feedback: str\n",
    "    base_domain: str\n",
    "    base_solutions: str\n",
    "    analogical_transfer: str\n",
    "    transfer_feedback: str\n",
    "    solution: str\n",
    "    \n",
    "    # Add for CRIT agents:\n",
    "    retry_count_abstract: int              # For infinite loop prevention\n",
    "    retry_count_transfer: int              # For infinite loop prevention\n",
    "    validated_contradictions: List[str]    # For passing filtered contradictions\n",
    "    validated_analogical_transfers: List[str]  # For passing filtered transfers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "306439ab-4b94-4084-8a1c-8c53ecf1a0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can switch between different LLMs \n",
    "llms = init_chat_model(\"openai:gpt-4.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "54ea64a9-96a1-42ab-bb3d-1a1c1d14b1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st Agent: Identify Target Domain from User Input\n",
    "def target_domain_agent(state: ReasoningState):\n",
    "    u = state['user_query']\n",
    "\n",
    "    msg = llms.invoke(TARGET_DOMAIN.format(user_query=u))\n",
    "\n",
    "    return {\"target_domain\": msg.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d327c06e-9618-4463-b19e-2be74a21d14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2st Agent: Conduct comprehensive research into problem landscape for target domain \n",
    "# (i.e. What specific challenges exist in this domain?)\n",
    "def problem_landscape_agent(state: ReasoningState):\n",
    "    t = state['target_domain']\n",
    "\n",
    "    msg = llms.invoke(PROBLEM_LANDSCAPE.format(target_domain=t))\n",
    "\n",
    "    return {\"problem_landscape\": msg.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "26b96ec9-7bdb-4803-9bab-dcc82d6a4c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3rd Agent: Abstract Problems Identified into Generalized Principles and TRIZ Contradiction\n",
    "def abstraction_agent(state: ReasoningState):\n",
    "    p = state['problem_landscape']\n",
    "    feedback = state.get('abstraction_feedback', '')\n",
    "    \n",
    "    # If there's feedback, include it in the prompt\n",
    "    if feedback:\n",
    "        msg = llms.invoke(ABSTRACTION.format(\n",
    "            problem_landscape=p,\n",
    "            feedback=f\"\\nPrevious attempt feedback: {feedback}\\nPlease address these issues in your new abstraction.\"\n",
    "        ))\n",
    "    else:\n",
    "        msg = llms.invoke(ABSTRACTION.format(problem_landscape=p))\n",
    "    \n",
    "    return {\"abstraction\": msg.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4b63464e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3.5th agent: CRIT Quality Control for TRIZ Abstraction\n",
    "def parse_abstraction_output(abstraction_text: str) -> list:\n",
    "    \"\"\"Parse the structured abstraction output into individual contradictions.\"\"\"\n",
    "    lines = [line.strip() for line in abstraction_text.split('\\n') if line.strip()]\n",
    "    contradictions = []\n",
    "    \n",
    "    current_contradiction = None\n",
    "    for line in lines:\n",
    "        if line.startswith('- Improving'):\n",
    "            current_contradiction = {\"contradiction\": line.strip('- ').strip()}\n",
    "        elif line.startswith('- TRIZ Principles:') and current_contradiction:\n",
    "            current_contradiction[\"principles\"] = line.replace('- TRIZ Principles:', '').strip()\n",
    "        elif line.startswith('- Innovation Potential:') and current_contradiction:\n",
    "            current_contradiction[\"potential\"] = line.replace('- Innovation Potential:', '').strip()\n",
    "            contradictions.append(current_contradiction)\n",
    "            current_contradiction = None\n",
    "    \n",
    "    return contradictions\n",
    "\n",
    "def CRIT_Control_Abstraction_Three_Step(target_domain: str, problem_landscape: str, triz_contradictions: str) -> dict:\n",
    "    \"\"\"Three-step CRIT validation workflow for TRIZ contradiction filtering.\"\"\"\n",
    "    \n",
    "    # Parse contradictions from structured output\n",
    "    parsed_contradictions = parse_abstraction_output(triz_contradictions)\n",
    "    \n",
    "    if not parsed_contradictions:\n",
    "        return {\n",
    "            \"score\": 0.0,\n",
    "            \"pass\": False,\n",
    "            \"reason\": \"No valid contradictions found in the input\",\n",
    "            \"validated_contradictions\": [],\n",
    "            \"filtered_contradictions\": []\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        # STEP 1: First CRIT Agent - Initial Filtering\n",
    "        step1_prompt = f\"\"\"\n",
    "# FIRST CRIT AGENT - Initial Socratic Filtering\n",
    "\n",
    "Apply Professor Chang's CRIT methods to filter TRIZ contradictions into good and bad sets.\n",
    "\n",
    "## CONTEXT\n",
    "Target Domain: {target_domain}\n",
    "Problem Landscape: {problem_landscape}\n",
    "Contradictions to Evaluate: {parsed_contradictions}\n",
    "\n",
    "## SOCRATIC FILTERING METHODS\n",
    "\n",
    "### 1. DEFINITION METHOD\n",
    "- Format: Does each contradiction follow \"Improving [X] vs. Worsening [Y]\"?\n",
    "- Parameters: Are they legitimate TRIZ parameters?\n",
    "- Abstraction: Are they at first-principles level?\n",
    "\n",
    "### 2. ELENCHUS METHOD (Cross-Examination)\n",
    "- Evidence: Can each contradiction trace back to specific problems?\n",
    "- Logic: Is the causal chain Problem → Root Cause → Contradiction valid?\n",
    "- Consistency: Any logical gaps in abstraction mapping?\n",
    "\n",
    "### 3. DIALECTIC METHOD (Counter-Arguments)\n",
    "- Alternatives: Are there better ways to express the same tension?\n",
    "- Comparison: Which contradictions capture essential tensions best?\n",
    "- Weakness: What are the strongest arguments against weak contradictions?\n",
    "\n",
    "## FILTERING DECISION\n",
    "Sort contradictions into two sets based on Socratic analysis:\n",
    "\n",
    "### OUTPUT FORMAT\n",
    "{{\n",
    "    \"good_contradictions\": [\n",
    "        {{\"contradiction\": str, \"principles\": str, \"potential\": str}}\n",
    "    ],\n",
    "    \"bad_contradictions\": [\n",
    "        {{\"contradiction\": str, \"principles\": str, \"potential\": str, \"reason\": str}}\n",
    "    ],\n",
    "    \"filtering_rationale\": str\n",
    "}}\n",
    "\n",
    "Apply rigorous Socratic methods. Include rationale for why contradictions are filtered out.\n",
    "        \"\"\"\n",
    "        \n",
    "        response_1 = llms.invoke(step1_prompt)\n",
    "        result_1 = parse_json_response(response_1.content)\n",
    "        \n",
    "        # STEP 2: Second CRIT Agent - Review and Revision\n",
    "        step2_prompt = f\"\"\"\n",
    "# SECOND CRIT AGENT - Socratic Review and Revision\n",
    "\n",
    "Cross-examine the first agent's filtering decisions using CRIT methods.\n",
    "\n",
    "## FIRST AGENT'S DECISIONS\n",
    "Good Contradictions: {result_1.get('good_contradictions', [])}\n",
    "Bad Contradictions: {result_1.get('bad_contradictions', [])}\n",
    "Filtering Rationale: {result_1.get('filtering_rationale', '')}\n",
    "\n",
    "## SOCRATIC CROSS-EXAMINATION\n",
    "\n",
    "### 1. ELENCHUS METHOD (Challenge Decisions)\n",
    "- Are any \"good\" contradictions actually flawed? Why?\n",
    "- Are any \"bad\" contradictions actually valuable? Why?\n",
    "- Do the filtering rationales hold up under scrutiny?\n",
    "\n",
    "### 2. DIALECTIC METHOD (Counter-Arguments)\n",
    "- Generate counter-arguments to the first agent's filtering rationale\n",
    "- Test alternative interpretations of contradiction quality\n",
    "- Challenge assumptions about what makes contradictions \"good\" or \"bad\"\n",
    "\n",
    "### 3. MAIEUTICS METHOD (Surface Hidden Assumptions)\n",
    "- What unstated assumptions influenced the first agent's decisions?\n",
    "- Are there valid contradictions being unfairly penalized?\n",
    "- What criteria were prioritized and why?\n",
    "\n",
    "## REVIEW DECISION\n",
    "Based on Socratic cross-examination:\n",
    "\n",
    "### OUTPUT FORMAT\n",
    "{{\n",
    "    \"revised_good_contradictions\": [\n",
    "        {{\"contradiction\": str, \"principles\": str, \"potential\": str}}\n",
    "    ],\n",
    "    \"revised_bad_contradictions\": [\n",
    "        {{\"contradiction\": str, \"principles\": str, \"potential\": str, \"reason\": str}}\n",
    "    ],\n",
    "    \"revision_rationale\": str,\n",
    "    \"revisions_made\": boolean\n",
    "}}\n",
    "\n",
    "If no revisions needed, return original sets with revisions_made: false.\n",
    "        \"\"\"\n",
    "        \n",
    "        response_2 = llms.invoke(step2_prompt)\n",
    "        result_2 = parse_json_response(response_2.content)\n",
    "        \n",
    "        # STEP 3: First CRIT Agent - Final Synthesis\n",
    "        step3_prompt = f\"\"\"\n",
    "# FIRST CRIT AGENT - Final Synthesis\n",
    "\n",
    "Synthesize the second agent's review with your own analysis to create final contradiction sets.\n",
    "\n",
    "## SECOND AGENT'S REVIEW\n",
    "Revised Good Contradictions: {result_2.get('revised_good_contradictions', [])}\n",
    "Revised Bad Contradictions: {result_2.get('revised_bad_contradictions', [])}\n",
    "Revision Rationale: {result_2.get('revision_rationale', '')}\n",
    "Revisions Made: {result_2.get('revisions_made', False)}\n",
    "\n",
    "## FINAL SOCRATIC SYNTHESIS\n",
    "\n",
    "THINK ABOUT THE FOLLOWING\n",
    "### 1. DIALECTIC INTEGRATION\n",
    "- Consider the second agent's counter-arguments seriously\n",
    "- Weigh competing interpretations of contradiction quality\n",
    "- Resolve any remaining tensions between different quality standards\n",
    "\n",
    "### 2. ELENCHUS VALIDATION\n",
    "- Cross-examine the final decisions one more time\n",
    "- Ensure logical consistency in the final sets\n",
    "- Verify that filtering decisions are well-justified\n",
    "\n",
    "### 3. PRACTICAL WISDOM\n",
    "- Balance perfectionism with progress\n",
    "- Ensure adequate contradictions for next pipeline stage\n",
    "- Prioritize contradictions that best serve the target domain\n",
    "\n",
    "## FINAL DECISION (MANDATORY)\n",
    "Provide definitive contradiction sets for pipeline progression:\n",
    "\n",
    "### OUTPUT FORMAT\n",
    "{{\n",
    "    \"final_good_contradictions\": [\n",
    "        {{\"contradiction\": str, \"principles\": str, \"potential\": str}}\n",
    "    ],\n",
    "    \"final_bad_contradictions\": [\n",
    "        {{\"contradiction\": str, \"principles\": str, \"potential\": str}}\n",
    "    ],\n",
    "    \"synthesis_complete\": boolean\n",
    "}}\n",
    "\n",
    "Must complete synthesis. Final decision required.\n",
    "        \"\"\"\n",
    "        \n",
    "        response_3 = llms.invoke(step3_prompt)\n",
    "        result_3 = parse_json_response(response_3.content)\n",
    "        \n",
    "        # Extract final results\n",
    "        final_good = result_3.get('final_good_contradictions', [])\n",
    "        final_bad = result_3.get('final_bad_contradictions', [])\n",
    "        \n",
    "        # Convert to simple list format for pipeline compatibility\n",
    "        validated_contradictions = [item['contradiction'] for item in final_good if 'contradiction' in item]\n",
    "        filtered_contradictions = [item['contradiction'] for item in final_bad if 'contradiction' in item]\n",
    "        \n",
    "        # Calculate final pass/fail\n",
    "        if len(validated_contradictions) >= 2:  # Minimum viable set\n",
    "            final_score = 8.0\n",
    "            final_pass = True\n",
    "            final_reason = \"\"\n",
    "        else:\n",
    "            final_score = 6.0\n",
    "            final_pass = False\n",
    "            final_reason = f\"Insufficient contradictions after 3-step CRIT filtering: {len(validated_contradictions)}\"\n",
    "        \n",
    "        return {\n",
    "            \"score\": final_score,\n",
    "            \"pass\": final_pass,\n",
    "            \"reason\": final_reason,\n",
    "            \"validated_contradictions\": validated_contradictions,\n",
    "            \"filtered_contradictions\": filtered_contradictions,\n",
    "            \"three_step_summary\": {\n",
    "                \"step1_good_count\": len(result_1.get('good_contradictions', [])),\n",
    "                \"step1_bad_count\": len(result_1.get('bad_contradictions', [])),\n",
    "                \"step2_revisions_made\": result_2.get('revisions_made', False),\n",
    "                \"step3_final_good_count\": len(validated_contradictions),\n",
    "                \"step3_final_bad_count\": len(filtered_contradictions)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"score\": 0.0,\n",
    "            \"pass\": False,\n",
    "            \"reason\": f\"Three-step CRIT workflow failed: {str(e)}\",\n",
    "            \"validated_contradictions\": [],\n",
    "            \"filtered_contradictions\": []\n",
    "        }\n",
    "\n",
    "def parse_json_response(response_text: str) -> dict:\n",
    "    \"\"\"Enhanced JSON parser for three-step CRIT responses.\"\"\"\n",
    "    try:\n",
    "        # Find JSON in response\n",
    "        start_idx = response_text.find('{')\n",
    "        end_idx = response_text.rfind('}') + 1\n",
    "        if start_idx >= 0 and end_idx > start_idx:\n",
    "            json_str = response_text[start_idx:end_idx]\n",
    "            return json.loads(json_str)\n",
    "        return {\"error\": \"No valid JSON found\"}\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"JSON parsing failed: {str(e)}\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a230bf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3.5th Agent Logic Flow: if quality control passes, continue to next agent. Else, loop back to retry TRIZ abstraction (taking rationale for why it failed as additional input)\n",
    "def should_continue_to_base(state: ReasoningState) -> str:\n",
    "    \"\"\"\n",
    "    Revised control flow for abstraction. Determines whether to continue to the \n",
    "    base domain or retry abstraction, with a retry limit to prevent infinite loops.\n",
    "    \"\"\"\n",
    "    # Check retry limit FIRST to prevent getting stuck\n",
    "    retry_count = state.get('retry_count_abstract', 0)\n",
    "    if retry_count >= 2:  # Max 3 attempts (0, 1, 2)\n",
    "        print(f\"Max retries ({retry_count}) reached for abstraction. Forcing progression to base domain agent.\")\n",
    "        # To prevent downstream errors, pass the last-known validated list or an empty one\n",
    "        if 'validated_contradictions' not in state:\n",
    "             state['validated_contradictions'] = []\n",
    "        return \"base\"\n",
    "\n",
    "    # If limit is not reached, run the CRIT validation\n",
    "    crit_result = CRIT_Control_Abstraction(\n",
    "        state['target_domain'],\n",
    "        state['problem_landscape'],\n",
    "        state['abstraction']\n",
    "    )\n",
    "    \n",
    "    passed = crit_result.get('pass', False)\n",
    "    \n",
    "    if passed:\n",
    "        # IMPORTANT: Update state with the validated contradictions for the next agent\n",
    "        state['validated_contradictions'] = crit_result.get('validated_contradictions', [])\n",
    "        return \"base\"\n",
    "    else:\n",
    "        # If CRIT fails, increment retry counter and add feedback for the next loop\n",
    "        state['retry_count_abstract'] = retry_count + 1\n",
    "        state['abstraction_feedback'] = crit_result.get('reason', '')\n",
    "        print(f\"Abstraction CRIT failed (attempt {state['retry_count_abstract']}): {crit_result.get('reason', '')}\")\n",
    "        return \"abstract\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e95a5d2f-1773-4339-ba89-995d185885c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4th Agent: Search for Appropriate Base Domains\n",
    "def base_domain_agent(state: ReasoningState):\n",
    "    a = state['abstraction']\n",
    "\n",
    "    msg = llms.invoke(BASE_DOMAIN.format(contradictions=a))\n",
    "\n",
    "    return {\"base_domain\": msg.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f7db11f3-e388-48b4-8661-cde636b1ba50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5th Agent: Identify Solution in Base Domain\n",
    "def base_solution_agent(state: ReasoningState):\n",
    "    b = state['base_domain']\n",
    "\n",
    "    msg = llms.invoke(BASE_SOLUTIONS.format(input=b))\n",
    "\n",
    "    return {\"base_solutions\": msg.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5dc1b395-b154-493e-9c57-7e5a6926cf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6th Agent: Base Domain Solution Informing Target Domain Solution\n",
    "def analogical_transfer_agent(state: ReasoningState):\n",
    "    if \"base_solutions\" not in state:\n",
    "        raise ValueError(\"Missing 'base_solutions' key. Check if previous node returned it.\")\n",
    "    b = state['base_solutions']\n",
    "    t = state['target_domain']\n",
    "    feedback = state.get('transfer_feedback', '')\n",
    "    \n",
    "    # If there's feedback, include it in the prompt\n",
    "    if feedback:\n",
    "        msg = llms.invoke(ANALOGICAL_TRANSFER.format(\n",
    "            contradictions_solutions=b,\n",
    "            target_domain=t,\n",
    "            feedback=f\"\\nPrevious attempt feedback: {feedback}\\nPlease address these issues in your new analogical transfer.\"\n",
    "        ))\n",
    "    else:\n",
    "        msg = llms.invoke(ANALOGICAL_TRANSFER.format(\n",
    "            contradictions_solutions=b,\n",
    "            target_domain=t\n",
    "        ))\n",
    "    \n",
    "    return {\"analogical_transfer\": msg.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d8dc7e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.5th Agent: CRIT Quality Control for Analogical Reasoning \n",
    "## Helper function to parse previous output\n",
    "def parse_analogical_transfer_output(transfer_text: str) -> list:\n",
    "    \"\"\"Parse the analogical transfer output into individual transfer solutions.\"\"\"\n",
    "    lines = [line.strip() for line in transfer_text.split('\\n') if line.strip()]\n",
    "    transfers = []\n",
    "    \n",
    "    current_transfer = None\n",
    "    for line in lines:\n",
    "        # Look for transfer headers or solution descriptions\n",
    "        if any(keyword in line.lower() for keyword in ['transfer', 'solution', 'approach', 'mechanism']):\n",
    "            if current_transfer and len(current_transfer.get('description', '')) > 50:\n",
    "                transfers.append(current_transfer)\n",
    "            current_transfer = {\"transfer\": line.strip()}\n",
    "            current_transfer[\"description\"] = \"\"\n",
    "        elif current_transfer:\n",
    "            # Accumulate description\n",
    "            current_transfer[\"description\"] += \" \" + line.strip()\n",
    "    \n",
    "    # Add final transfer if exists\n",
    "    if current_transfer and len(current_transfer.get('description', '')) > 50:\n",
    "        transfers.append(current_transfer)\n",
    "    \n",
    "    # Fallback: split by paragraphs if structured parsing fails\n",
    "    if not transfers:\n",
    "        paragraphs = transfer_text.split('\\n\\n')\n",
    "        for i, paragraph in enumerate(paragraphs):\n",
    "            if len(paragraph.strip()) > 100:  # Minimum length for meaningful transfer\n",
    "                transfers.append({\n",
    "                    \"transfer\": f\"Transfer Solution {i+1}\",\n",
    "                    \"description\": paragraph.strip()\n",
    "                })\n",
    "    \n",
    "    return transfers\n",
    "\n",
    "## CRIT control agent - debate 3 steps to ensure quality\n",
    "def CRIT_Control_Analogical_Transfer_Three_Step(target_domain: str, original_contradictions: str, base_solutions: str, analogical_transfers: str) -> dict:\n",
    "    \"\"\"Three-step CRIT validation workflow for analogical transfer filtering.\"\"\"\n",
    "    \n",
    "    # Parse transfers from output\n",
    "    parsed_transfers = parse_analogical_transfer_output(analogical_transfers)\n",
    "    \n",
    "    if not parsed_transfers:\n",
    "        return {\n",
    "            \"score\": 0.0,\n",
    "            \"pass\": False,\n",
    "            \"reason\": \"No valid analogical transfers found in the input\",\n",
    "            \"validated_transfers\": [],\n",
    "            \"filtered_transfers\": []\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        # STEP 1: First CRIT Agent - Initial Transfer Filtering\n",
    "        step1_prompt = f\"\"\"\n",
    "# FIRST CRIT AGENT - Initial Socratic Transfer Filtering\n",
    "\n",
    "Apply Professor Chang's CRIT methods to filter analogical transfers into good and bad sets.\n",
    "\n",
    "## CONTEXT\n",
    "Target Domain: {target_domain}\n",
    "Original Contradictions: {original_contradictions}\n",
    "Base Solutions: {base_solutions}\n",
    "Transfers to Evaluate: {parsed_transfers}\n",
    "\n",
    "## SOCRATIC FILTERING METHODS\n",
    "\n",
    "### 1. DEFINITION METHOD\n",
    "- Mechanism: Does each transfer preserve core solution patterns from base domains?\n",
    "- Parameters: Are key elements from base domain mapped correctly to target domain?\n",
    "- Abstraction: Is transfer at appropriate level (not too superficial, not too abstract)?\n",
    "\n",
    "### 2. ELENCHUS METHOD (Cross-Examination)\n",
    "- Evidence: Can each transfer trace back clearly from base solution to target application?\n",
    "- Logic: Is the causal chain Base Mechanism → Transfer Logic → Target Solution valid?\n",
    "- Consistency: Do transfers actually address the original contradictions identified?\n",
    "\n",
    "### 3. DIALECTIC METHOD (Counter-Arguments)\n",
    "- Alternatives: Are there better ways to transfer the same base mechanisms?\n",
    "- Comparison: Which transfers best preserve essential solution patterns?\n",
    "- Weakness: What are strongest arguments against weak or superficial transfers?\n",
    "\n",
    "## FILTERING DECISION\n",
    "Sort transfers into two sets based on Socratic analysis:\n",
    "\n",
    "### OUTPUT FORMAT\n",
    "{{\n",
    "    \"good_transfers\": [\n",
    "        {{\"transfer\": str, \"description\": str}}\n",
    "    ],\n",
    "    \"bad_transfers\": [\n",
    "        {{\"transfer\": str, \"description\": str, \"reason\": str}}\n",
    "    ],\n",
    "    \"filtering_rationale\": str\n",
    "}}\n",
    "\n",
    "Apply rigorous Socratic methods. Focus on mechanism fidelity and target domain feasibility.\n",
    "        \"\"\"\n",
    "        \n",
    "        response_1 = llms.invoke(step1_prompt)\n",
    "        result_1 = parse_json_response(response_1.content)\n",
    "        \n",
    "        # STEP 2: Second CRIT Agent - Review and Revision\n",
    "        step2_prompt = f\"\"\"\n",
    "# SECOND CRIT AGENT - Socratic Transfer Review and Revision\n",
    "\n",
    "Cross-examine the first agent's transfer filtering decisions using CRIT methods.\n",
    "\n",
    "## FIRST AGENT'S DECISIONS\n",
    "Good Transfers: {result_1.get('good_transfers', [])}\n",
    "Bad Transfers: {result_1.get('bad_transfers', [])}\n",
    "Filtering Rationale: {result_1.get('filtering_rationale', '')}\n",
    "\n",
    "## SOCRATIC CROSS-EXAMINATION\n",
    "\n",
    "### 1. ELENCHUS METHOD (Challenge Transfer Decisions)\n",
    "- Are any \"good\" transfers actually flawed in mechanism preservation?\n",
    "- Are any \"bad\" transfers actually valuable but underestimated?\n",
    "- Do the filtering rationales properly assess implementation feasibility?\n",
    "\n",
    "### 2. DIALECTIC METHOD (Counter-Arguments to First Agent)\n",
    "- Generate counter-arguments to the first agent's transfer quality assessments\n",
    "- Test alternative interpretations of what makes effective analogical transfer\n",
    "- Challenge assumptions about mechanism preservation vs. creative adaptation\n",
    "\n",
    "### 3. MAIEUTICS METHOD (Surface Hidden Transfer Assumptions)\n",
    "- What unstated assumptions influenced transfer quality judgments?\n",
    "- Are there valid transfers being unfairly penalized for creativity?\n",
    "- What criteria should prioritize: fidelity to base vs. target domain fit?\n",
    "\n",
    "## REVIEW DECISION\n",
    "Based on Socratic cross-examination of transfer quality:\n",
    "\n",
    "### OUTPUT FORMAT\n",
    "{{\n",
    "    \"revised_good_transfers\": [\n",
    "        {{\"transfer\": str, \"description\": str}}\n",
    "    ],\n",
    "    \"revised_bad_transfers\": [\n",
    "        {{\"transfer\": str, \"description\": str, \"reason\": str}}\n",
    "    ],\n",
    "    \"revision_rationale\": str,\n",
    "    \"revisions_made\": boolean\n",
    "}}\n",
    "\n",
    "If no revisions needed, return original sets with revisions_made: false.\n",
    "        \"\"\"\n",
    "        \n",
    "        response_2 = llms.invoke(step2_prompt)\n",
    "        result_2 = parse_json_response(response_2.content)\n",
    "        \n",
    "        # STEP 3: First CRIT Agent - Final Transfer Synthesis\n",
    "        step3_prompt = f\"\"\"\n",
    "# FIRST CRIT AGENT - Final Transfer Synthesis\n",
    "\n",
    "Synthesize the second agent's review with your own analysis to create final transfer sets.\n",
    "\n",
    "## SECOND AGENT'S REVIEW\n",
    "Revised Good Transfers: {result_2.get('revised_good_transfers', [])}\n",
    "Revised Bad Transfers: {result_2.get('revised_bad_transfers', [])}\n",
    "Revision Rationale: {result_2.get('revision_rationale', '')}\n",
    "Revisions Made: {result_2.get('revisions_made', False)}\n",
    "\n",
    "## FINAL SOCRATIC SYNTHESIS\n",
    "\n",
    "### 1. DIALECTIC INTEGRATION\n",
    "- Consider the second agent's counter-arguments about transfer quality seriously\n",
    "- Weigh competing interpretations of effective analogical transfer\n",
    "- Resolve tensions between mechanism fidelity and target domain adaptation\n",
    "\n",
    "### 2. ELENCHUS VALIDATION\n",
    "- Cross-examine the final transfer decisions one more time\n",
    "- Ensure logical consistency between transfers and original contradictions\n",
    "- Verify that transfer mechanisms are implementable in target domain\n",
    "\n",
    "### 3. METHOD OF MAIEUTICS (Midwife Method)\n",
    "- Draw out the inherent wisdom about transfer quality revealed through agent dialogue\n",
    "- Surface the essential understanding about target domain needs that has emerged\n",
    "- Help bring forth the knowledge about which transfers best serve analogical reasoning\n",
    "\n",
    "## FINAL DECISION (MANDATORY)\n",
    "Provide definitive transfer sets for synthesis agent:\n",
    "\n",
    "### OUTPUT FORMAT\n",
    "{{\n",
    "    \"final_good_transfers\": [\n",
    "        {{\"transfer\": str, \"description\": str}}\n",
    "    ],\n",
    "    \"final_bad_transfers\": [\n",
    "        {{\"transfer\": str, \"description\": str}}\n",
    "    ],\n",
    "    \"synthesis_complete\": boolean\n",
    "}}\n",
    "\n",
    "Must complete synthesis. Final decision required for pipeline progression.\n",
    "        \"\"\"\n",
    "        \n",
    "        response_3 = llms.invoke(step3_prompt)\n",
    "        result_3 = parse_json_response(response_3.content)\n",
    "        \n",
    "        # Extract final results\n",
    "        final_good = result_3.get('final_good_transfers', [])\n",
    "        final_bad = result_3.get('final_bad_transfers', [])\n",
    "        \n",
    "        # Convert to simple list format for pipeline compatibility\n",
    "        validated_transfers = [item['description'] for item in final_good if 'description' in item]\n",
    "        filtered_transfers = [item['description'] for item in final_bad if 'description' in item]\n",
    "        \n",
    "        # Calculate final pass/fail\n",
    "        if len(validated_transfers) >= 2:  # Minimum viable transfer set\n",
    "            final_score = 8.0\n",
    "            final_pass = True\n",
    "            final_reason = \"\"\n",
    "        else:\n",
    "            final_score = 6.0\n",
    "            final_pass = False\n",
    "            final_reason = f\"Insufficient quality transfers after 3-step CRIT filtering: {len(validated_transfers)}\"\n",
    "        \n",
    "        return {\n",
    "            \"score\": final_score,\n",
    "            \"pass\": final_pass,\n",
    "            \"reason\": final_reason,\n",
    "            \"validated_transfers\": validated_transfers,\n",
    "            \"filtered_transfers\": filtered_transfers,\n",
    "            \"three_step_summary\": {\n",
    "                \"step1_good_count\": len(result_1.get('good_transfers', [])),\n",
    "                \"step1_bad_count\": len(result_1.get('bad_transfers', [])),\n",
    "                \"step2_revisions_made\": result_2.get('revisions_made', False),\n",
    "                \"step3_final_good_count\": len(validated_transfers),\n",
    "                \"step3_final_bad_count\": len(filtered_transfers)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"score\": 0.0,\n",
    "            \"pass\": False,\n",
    "            \"reason\": f\"Three-step transfer CRIT workflow failed: {str(e)}\",\n",
    "            \"validated_transfers\": [],\n",
    "            \"filtered_transfers\": []\n",
    "        }\n",
    "\n",
    "def parse_json_response(response_text: str) -> dict:\n",
    "    \"\"\"Enhanced JSON parser for three-step CRIT responses.\"\"\"\n",
    "    try:\n",
    "        # Find JSON in response\n",
    "        start_idx = response_text.find('{')\n",
    "        end_idx = response_text.rfind('}') + 1\n",
    "        if start_idx >= 0 and end_idx > start_idx:\n",
    "            json_str = response_text[start_idx:end_idx]\n",
    "            return json.loads(json_str)\n",
    "        return {\"error\": \"No valid JSON found\"}\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"JSON parsing failed: {str(e)}\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "faecca01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Routing function for workflow integration\n",
    "def should_continue_to_synthesis(state: ReasoningState) -> str:\n",
    "    \"\"\"\n",
    "    Control flow for transfer validation. Determines whether to proceed to synthesis\n",
    "    or retry, with a retry limit to prevent infinite loops.\n",
    "    \"\"\"\n",
    "    # Check retry limit FIRST to break potential loops\n",
    "    retry_count = state.get('retry_count_transfer', 0)\n",
    "    if retry_count >= 2:  # Allow up to 2 retries (3 total attempts)\n",
    "        print(f\"Max retries ({retry_count}) reached for analogical transfer. Forcing progression to synthesis.\")\n",
    "        # To avoid errors, pass the last known (unfiltered) transfers to the synthesis agent\n",
    "        state['validated_analogical_transfers'] = parse_analogical_transfer_output(state['analogical_transfer'])\n",
    "        return \"synthesis\"\n",
    "\n",
    "    # If within limits, run CRIT validation\n",
    "    crit_result = CRIT_Control_Analogical_Transfer_Three_Step(\n",
    "        state['target_domain'],\n",
    "        state.get('validated_contradictions', state['abstraction']), # Use validated contradictions if available\n",
    "        state['base_solutions'],\n",
    "        state['analogical_transfer']\n",
    "    )\n",
    "    \n",
    "    passed = crit_result.get('pass', False)\n",
    "    \n",
    "    if passed:\n",
    "        # On success, update the state with the validated transfers for the synthesis agent\n",
    "        state['validated_analogical_transfers'] = crit_result.get('validated_transfers', [])\n",
    "        return \"synthesis\"\n",
    "    else:\n",
    "        # On failure, increment the retry counter and add feedback for the next loop\n",
    "        state['retry_count_transfer'] = retry_count + 1\n",
    "        state['transfer_feedback'] = crit_result.get('reason', '')\n",
    "        print(f\"Transfer CRIT failed (attempt {state['retry_count_transfer']}): {crit_result.get('reason', '')}\")\n",
    "        return \"analogical_transfer\"\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e3626041-fcd5-4bf3-a32b-f164352e788f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7th Agent: Summarize everything and respond to the question\n",
    "def synthesis_agent(state: ReasoningState):\n",
    "    msg = llms.invoke(\n",
    "        f\"Evaluate the proposed analogical solutions. Find the best ones that balances practicality with innovation. Then, provide a detailed, well-structured response that addresses all aspects of the query.\\n\\n\"\n",
    "        f\"Problem: {state['user_query']}\\n\"\n",
    "        f\"Analogical Solutions: {state['analogical_transfer']}\"\n",
    "        f\"In your output, remember to abstract away the analogy itself such that it is focused on responding to the user input.\"\n",
    "        f\"Also, check if the users are requesting a specific number of possible solutions. Make sure to answer the user's query in full and provide what is requested.\"\n",
    "    )\n",
    "    return {\"solution\": msg.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "68994430-5388-4c63-84be-919d5978d5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the workflow \n",
    "workflow = StateGraph(ReasoningState)\n",
    "\n",
    "workflow.add_node(\"target\", target_domain_agent)\n",
    "workflow.add_node(\"landscape\", problem_landscape_agent)\n",
    "workflow.add_node(\"abstract\", abstraction_agent)\n",
    "workflow.add_node(\"base\", base_domain_agent)\n",
    "workflow.add_node(\"base_soln\", base_solution_agent)\n",
    "workflow.add_node(\"analogy\", analogical_transfer_agent)\n",
    "workflow.add_node(\"synthesis\", synthesis_agent)\n",
    "\n",
    "workflow.set_entry_point(\"target\")\n",
    "\n",
    "# Define edges\n",
    "workflow.add_edge(\"target\", \"landscape\")\n",
    "workflow.add_edge(\"landscape\", \"abstract\")\n",
    "# workflow.add_edge(\"abstract\", \"base\")\n",
    "\n",
    "# Define conditional edges - Use CRIT TO control quality for Abstraction TRIZ\n",
    "workflow.add_conditional_edges(\n",
    "    \"abstract\",\n",
    "    should_continue_to_base,\n",
    "    {\n",
    "        \"base\": \"base\",\n",
    "        \"abstract\": \"abstract\" \n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"base\", \"base_soln\")\n",
    "workflow.add_edge(\"base_soln\", \"analogy\")\n",
    "#workflow.add_edge(\"analogy\", \"synthesis\")\n",
    "\n",
    "# Define conditional edges - Use CRIT TO control quality for Analogical Transfer\n",
    "workflow.add_conditional_edges(\n",
    "    \"analogy\",\n",
    "    should_continue_to_synthesis,\n",
    "    {\n",
    "        \"synthesis\": \"synthesis\",\n",
    "        \"analogy\": \"analogy\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.set_finish_point(\"synthesis\")\n",
    "\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b178c9ea-3768-44c1-8461-01c0e93ef1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogical_output(user_input: str) -> str:\n",
    "    input_state = {\"user_query\": user_input}\n",
    "\n",
    "    final_state = graph.invoke(input_state)\n",
    "\n",
    "    for key in final_state:\n",
    "        raw_output = str(final_state[key])  # Ensure it's a string\n",
    "        final_output = parse_solution(raw_output)\n",
    "\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea24580",
   "metadata": {},
   "source": [
    "### Initialize Different Functions for Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c1ed3fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Output\n",
    "def basic_output(user_input: str):\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    {user_input}\n",
    "    \"\"\"\n",
    "\n",
    "    response = llms.invoke(prompt) \n",
    "\n",
    "    # Parse and format the response\n",
    "    formatted_response = parse_solution(response.content)\n",
    "\n",
    "    return(formatted_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "174da814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Output with COT\n",
    "## TO EDIT WITH FULL PROMPT?\n",
    "def basic_output_COT(user_input: str):\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    {user_input}\n",
    "    Think step by step.\n",
    "    \"\"\"\n",
    "\n",
    "    response = llms.invoke(prompt) \n",
    "\n",
    "    # Parse and format the response\n",
    "    formatted_response = parse_solution(response.content)\n",
    "\n",
    "    return(formatted_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4d4f4149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Output with Prompt Engineering\n",
    "def basic_output_prompt_engin(user_input: str):\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    {user_input}\n",
    "    Try to balance practicality with innovation.\n",
    "    \"\"\"\n",
    "\n",
    "    response = llms.invoke(prompt) \n",
    "\n",
    "    # Parse and format the response\n",
    "    formatted_response = parse_solution(response.content)\n",
    "\n",
    "    return(formatted_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fe3b3f1-ed91-4fb9-a391-215466a0ccae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_output_full_COT(user_input: str):\n",
    "    prompt = f\"\"\"\n",
    "    {user_input}\n",
    "    Step one: As a Domain Analysis Specialist, extract all the core innovation domains from the user query. It could be a single one for a simple query, or multiple ones for a complex query.\n",
    "        Instructions:\n",
    "        1. Analyze the user's input\n",
    "        2. Identify the primary domain(s) requiring innovation\n",
    "        3. Classify it within standard innovation categories\n",
    "        Output Format:\n",
    "        Target Domain: [Clear, specific domain label]\n",
    "        Be very detailed and specific in your response and do not generalize. Respond ONLY with the name of the domain, do NOT include ANY other text like 'Target Domain:'.\n",
    "    Step two: You are a Problem Landscape Analyst. Your task is to map out the concrete challenges within the target domain identified.\n",
    "        Instructions:\n",
    "        1. Identify all the core problems or challenges currently present in these domains. Aim for at least 3 problems per domain\n",
    "        2. For each problem, provide:\n",
    "        - Problem: A short, clear title.\n",
    "        - Description: 2-3 sentences explaining what the problem is and why it matters.\n",
    "        - Context: Briefly state the circumstances or environment where this problem occurs.\n",
    "        - Stakeholders: List the main groups or individuals affected.\n",
    "        - Root Causes: Identify 1-3 underlying causes, if known.\n",
    "        - Impact: State the significance of the problem (e.g., social, economic, technical).\n",
    "        - Current Approaches: How is this problem currently addressed?\n",
    "        - Limitations: What are the shortcomings of current approaches?\n",
    "        - Success Metrics: How would you measure if this problem is solved?\n",
    "        - Interconnections: Note if this problem is linked to or influenced by other problems.\n",
    "        Output Format:\n",
    "        Present your findings as a structured list or JSON array, with each problem fully described as above.\n",
    "        Important:\n",
    "        - Focus on clarity and completeness.\n",
    "        - Avoid abstracting or generalizing; stay concrete and domain-specific.\n",
    "        - Do not propose solutions; only describe the current problem landscape.\n",
    "    Step three: You are a TRIZ Methodology Expert. Transform domain-specific problems into universal contradictions.\n",
    "        Process:\n",
    "        1. Read up on TRIZ - the contradiction matrix, and the inventive principles\n",
    "        2. For each problem in the problem landscape:\n",
    "        - Abstract to universal parameters (what improves vs. what worsens)\n",
    "        - Express as 'When we improve X, Y worsens'\n",
    "        - Ensure parameters are domain-agnostic\n",
    "        3. Analyze all the abstracted universal parameters, and identify all the core TRIZ contradictions present:\n",
    "        - Select the most fundamental tensions\n",
    "        - Map to TRIZ contradiction matrix\n",
    "        - Note applicable inventive principles\n",
    "        Output:\n",
    "        # List all the core contradictions in form of:\n",
    "        - Improving [parameter] vs. Worsening [parameter]\n",
    "        - TRIZ Principles: [1-3 relevant principles]\n",
    "        - Innovation Potential: [High/Medium/Low]\n",
    "        Focus on contradictions that, if resolved, would create breakthrough value.\n",
    "    Step four: You are a Cross-Domain Search Specialist. Do the following:\n",
    "        - For each contradiction provided, identify 3 distinct source domains (fields or industries) where this contradiction has been successfully addressed.\n",
    "        - Experiment with different subsets of the list of contradictions, and see if you could identify 3 distinct source domains for each of these subsets identified as well. You should find at least 3 different subsets.\n",
    "        Note: The domains should have A CONCEPTUAL DISTANCE OF AT LEAST 3 DISTINCT HOPS FROM WHAT IMMEDIATELY COMES TO MIND. Be creative! It can be domains within spheres like natural, phsyical, social, artistic, or anything.\n",
    "        For each domain identified, briefly explain why it is relevant to the single contradiction or the subset of contradictions identified. Do not describe specific solutions just yet-only list the domains and your rationale.\n",
    "        Output:\n",
    "        A list for each contradiction and subset of contradictions identified, naming 3 relevant domains with a 2 sentence rationale for each.\n",
    "        Aim for a total of at least 20 relevant base domains. \n",
    "    Step five: You are a Solution Pattern Extractor. You are provided with an input with 3 base domains identified per TRIZ (Theory of Inventive Problem Solving) contradiction or a set of contradictions, as well as the contradictions themselves.\n",
    "        For each of these identified base domains, identify one specific, well-documented solution pattern within the domain that effectively resolves the contradiction (or the set of contradictions).\n",
    "        For each solution pattern, return:\n",
    "        - Identify the base domain it's corresponding to\n",
    "        - Recall the contradiction or the set of contradictions that this base domain faces\n",
    "        - The name or label of the solution pattern for resolving these contradiction(s) in the base domain\n",
    "        - A detailed description of the core mechanism or principle involved and how it addressed the domain's contradiction(s)\n",
    "        - The context or situation in the domain where this pattern is applied\\n\"\n",
    "        Do not generalize or adapt the solution-simply describe how the contradiction is addressed within each source domain.\n",
    "        Output:\n",
    "        For each of the provided domain, list the base domain name, contradiction(s) faced, solution pattern name, the detailed description of the mechanism of the solution pattern, and the context in which it is used. Articulate the contradictions as problems and considerations faced, through framing them as a tension.\n",
    "    Step six:         You are a very innovative Analogical Transfer Specialist.\n",
    "        You are provided with list the base domain name, tensions faced, solution pattern name, the detailed description of the mechanism of the solution pattern, and the context in which it is used.\n",
    "        Your task is to propose how solution patterns used to resolve these tensions in various base domains might inspire solution framings for the original target domain.\n",
    "\n",
    "        Input Overview:\n",
    "        1. A list the base domains identified, the tensions these domains faced, the name of solution patterns that helped addressed these tensions in these base domains, the detailed description of the mechanism of the solution pattern, and the context in which it is used.\n",
    "        2. The original target domain.\n",
    "\n",
    "        Instructions:\n",
    "        For each pair of base domain and the corresponding tensions identified, review the solution patterns that worked for the base domain. For each pattern:\n",
    "        - Analyze the core mechanism or principle behind the solution.\n",
    "        - Map and adapt this mechanism conceptually to the target domain, considering the specific context and needs of the target domain.\n",
    "        - Clearly describe how this analogical transfer could frame a potential solution in the target domain.\n",
    "        - Highlight any key adaptations, considerations, or limitations that would be relevant when applying this pattern to the target domain.\n",
    "\n",
    "        Your expected Output:\n",
    "        For each base domain, provide a comprehensive description of a proposed solution framing for the target domain, including:\n",
    "        - The original tension addressed\n",
    "        - The source domain and solution pattern\n",
    "        - A detailed explanation of how the pattern could inspire or inform a solution in the target domain\n",
    "        - Any important adaptations or considerations for successful transfer\n",
    "    Step seven: Evaluate the proposed analogical solutions. Find the best ones that balances practicality with innovation. Then, provide a detailed, well-structured response that addresses all aspects of the query.\n",
    "        In your output, remember to abstract away the analogy itself such that it is focused on responding to the user input.\n",
    "        Also, check if the users are requesting a specific number of possible solutions. Make sure to answer the user's query in full and provide what is requested.   \n",
    "    \"\"\"\n",
    "    response = llms.invoke(prompt) \n",
    "\n",
    "    # Parse and format the response\n",
    "    formatted_response = parse_solution(response.content)\n",
    "\n",
    "    return(formatted_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7fe14c",
   "metadata": {},
   "source": [
    "### Test prompt 1 - Education"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6fb723",
   "metadata": {},
   "source": [
    "#### User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "848e42be",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_q = \"Teachers introduced structured group projects using peer evaluations and rotating roles to boost collaboration in science classes. Despite these measures, anonymous student surveys revealed that individual quiz scores post-project dropped compared to solo assignments, signaling diluted accountability. How can educators redesign design a system to ensure measurable individual mastery while preserving the benefits of teamwork?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cd473f",
   "metadata": {},
   "source": [
    "#### With Analogical Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3414cb06-b846-4979-99ef-be697a56101b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = analogical_output(test_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0945a5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! The central challenge described is ensuring measurable individual mastery (fairness, accountability) within collaborative science projects—where teamwork and shared learning are essential, yet \"free-riding\" or dilution of individual responsibility can undermine both scores and student motivation. To address this, let's analyze and synthesize the most practical and innovative solutions from the analogies above, focusing on those with high impact and feasibility for secondary science classes.\n",
      "\n",
      "\n",
      "=== ## Best Solution Approaches ===\n",
      "\n",
      "### 1. Integrated Dual-Mode Assessment System\n",
      "\n",
      "How It Works:\n",
      "- Baseline Individual Assessment: Each student submits individual \"artifacts\" related to the group project: e.g., concept explanations, reflection logs, or quiz responses demonstrating their personal understanding of the project's scientific principles.\n",
      "- Collaborative Product Assessment: The group’s shared output—lab report, model, presentation—is assessed for core competencies (experimental design, data analysis, conclusions).\n",
      "- Both scores *contribute* to the final grade (e.g., 60% group, 40% individual, adjustable by context).\n",
      "\n",
      "Why It Balances Practicality and Innovation:\n",
      "- Students remain invested in both their own mastery and their group's success.\n",
      "- Prevents “hiding” in the group, as each must demonstrate individual comprehension.\n",
      "- Encourages peer-teaching and collaborative sense-making: group mates know that everyone needs to “get it.”\n",
      "\n",
      "Concrete Steps for Implementation:\n",
      "- Design quick, individualized post-project quizzes or short reports directly linked to project key concepts.\n",
      "- Use digital platforms to collect both group and individual artifacts for transparency.\n",
      "- Rubrics are clear and separate for each component.\n",
      "\n",
      "\n",
      "=== ### 2. Tracked Roles with Rotational Accountability and Reflections ===\n",
      "\n",
      "How It Works:\n",
      "- Assign well-defined roles (lead experimenter, note-taker, data analyst, presenter) that rotate among group members *across multiple projects* or even within a single large project.\n",
      "- Maintain logs or reflection journals where students briefly document what they contributed, learned, and any challenges faced in their role.\n",
      "- Use these logs, along with peer/self-evaluations, as part of individual assessment.\n",
      "\n",
      "Why It Balances Practicality and Innovation:\n",
      "- Role rotation scaffolds skill development and ensures exposure to all aspects of scientific inquiry.\n",
      "- Journals encourage metacognition and provide tangible evidence of engagement.\n",
      "- Peer feedback and teacher review are manageable and meaningful—less prone to \"popularity bias\" when linked to concrete role evidence.\n",
      "\n",
      "Concrete Steps for Implementation:\n",
      "- Predefine the roles and schedule rotations.\n",
      "- Require brief reflection entries attached to project milestones.\n",
      "- Evaluate with a rubric for both skill performance and honesty/effort in reflections.\n",
      "\n",
      "\n",
      "=== ### 3. Digital Traceability with Individual \"Contribution Snapshots\" ===\n",
      "\n",
      "How It Works:\n",
      "- Use collaborative platforms (e.g., Google Docs, Notion, other LMS tools) that record individual contributions.\n",
      "- At set checkpoints, \"snapshot\" contributions: what did each member add, edit, or comment on?\n",
      "- Teacher (and optionally peers) reviews these logs when assigning individual credit.\n",
      "\n",
      "Why It Balances Practicality and Innovation:\n",
      "- Automates accountability, reducing subjectivity in grading.\n",
      "- Makes visible the often-invisible cognitive/organizational work done by team members.\n",
      "- Encourages equitable workload and clear documentation.\n",
      "\n",
      "Concrete Steps for Implementation:\n",
      "- Require all group project work to be done on the assigned digital platform.\n",
      "- Teach students how to record contributions (using comments, versioning).\n",
      "- At project end, have students highlight/submit their key contributions for review.\n",
      "\n",
      "\n",
      "=== ### 4. Milestone-Based Gates with Mixed Assessment Modes ===\n",
      "\n",
      "How It Works:\n",
      "- Projects are broken down into fixed \"milestones\" (e.g., hypothesis formulation, experimental design, data analysis, final presentation).\n",
      "- At each milestone, require:\n",
      "  - Group product: e.g., experimental plan, dataset, draft report.\n",
      "  - Individual demonstration: e.g., short oral defense, content quiz, or written response explaining the group’s choices or findings.\n",
      "\n",
      "Why It Balances Practicality and Innovation:\n",
      "- Frequent checkpoints encourage regular individual engagement.\n",
      "- Reduces risk of last-minute “group rescue” of disengaged members.\n",
      "- Keeps the project process transparent and paces work appropriately.\n",
      "\n",
      "Concrete Steps for Implementation:\n",
      "- Provide students with a milestone schedule and clear expectations for both group and individual deliverables at each stage.\n",
      "- Vary assessment types to suit both procedural and conceptual mastery.\n",
      "\n",
      "\n",
      "=== ### 5. Outcomes-Focused, Flexible Evidence System (Multimodal Assessment) ===\n",
      "\n",
      "How It Works:\n",
      "- Mandate that all students demonstrate mastery of core outcomes, but *allow flexibility* in how they do so: oral presentations, blog posts, models, or traditional tests.\n",
      "- Evaluate these with a common outcomes rubric emphasizing conceptual understanding and scientific reasoning.\n",
      "\n",
      "Why It Balances Practicality and Innovation:\n",
      "- Honors diverse strengths and learning styles.\n",
      "- Keeps a rigorous standard for mastery, regardless of presentation mode.\n",
      "- Deters shallow participation, since final demonstration must be individual and robust.\n",
      "\n",
      "Concrete Steps for Implementation:\n",
      "- At the end of projects, students select (or are assigned) an individual method to communicate understanding.\n",
      "- Rubric is published in advance and tied tightly to curriculum goals.\n",
      "\n",
      "\n",
      "=== ## Implementation Considerations ===\n",
      "\n",
      "1. Transparency & Buy-In:  \n",
      "   Always clearly explain to students how their grades will be calculated and why individual accountability matters, without undermining the value of collaboration.\n",
      "\n",
      "2. Teacher Workload:  \n",
      "   Implement streamlined processes (rubrics, digital platforms) to ensure assessability does not become overwhelming with larger student numbers.\n",
      "\n",
      "3. Equity & Support:  \n",
      "   Make adjustments for students with differing needs or access to technology; ensure all roles and assessment modes are equally valued.\n",
      "\n",
      "4. Training & Culture Shift:  \n",
      "   Provide some orientation/training for both teachers and students in new processes (e.g., using contribution tracking tools, giving effective peer feedback).\n",
      "\n",
      "\n",
      "=== ## Synthesis & Recommendation ===\n",
      "\n",
      "Most robust and balanced approach:  \n",
      "The dual-mode system (individual + group assessment, enhanced by role rotation and digital traceability) is both practical (manageable, scalable, clear) and innovative (uses technology, metacognition, peer accountability). Combining milestone checkpoints and outcome-based, flexible presentations can further embed mastery and engagement.\n",
      "\n",
      "Example Blend:  \n",
      "- Each project:  \n",
      "  - Individual quiz or defense (~30%)  \n",
      "  - Tracked individual contribution (digital log and reflection) (~20%)  \n",
      "  - Group product (~40%)  \n",
      "  - Peer/self eval (~10%)\n",
      "\n",
      "- Roles rotate every project, with students’ reflections informing both grades and feedback.\n",
      "\n",
      "\n",
      "=== ## Conclusion ===\n",
      "\n",
      "Educators should adopt an assessment framework that explicitly combines:  \n",
      "- Individual Demonstrations of Mastery (through quizzes, oral defenses, or reflection logs);  \n",
      "- Digital or Documented Accountability (role logs, contribution snapshots);  \n",
      "- Collaborative Group products (graded with transparent rubrics);  \n",
      "while rotating roles and including opportunities for flexible demonstration of learning.\n",
      "\n",
      "This holistic strategy preserves and enhances the benefits of teamwork while ensuring all students truly master the intended science concepts—meeting the dual goals of collaboration and individual accountability.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f631b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_state = graph.invoke()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "52dafd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test1.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(response)\n",
    "    f.write(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d3c50c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save all parsed outputs to a text file\n",
    "# with open(\"sample_output.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     for key in final_state:\n",
    "#         raw_output = str(final_state[key])  # Ensure it's a string\n",
    "#         final_output = parse_solution(raw_output)\n",
    "#         f.write(f\"\\n### {key} ###\\n\")\n",
    "#         f.write(final_output)\n",
    "#         f.write(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fc3c62",
   "metadata": {},
   "source": [
    "#### Without Analogical Reasoning - GPT 4.1 RAW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a6aa161c-0873-49c4-9bcc-85d644311d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GPT-4.1 RAW Response ===\n",
      "\n",
      "This scenario highlights a common tension: collaborative learning can build important skills, but it sometimes leads to “social loafing”—students may rely on peers and engage less deeply with content, hurting individual learning as measured by solo assessments.\n",
      "\n",
      "To address this, educators can redesign group work with strategies that emphasize both individual mastery and authentic collaboration:\n",
      "\n",
      "\n",
      "=== ### 1. Integrate Individual Accountability into Group Projects ===\n",
      "\n",
      "Possible structures:\n",
      "- “Jigsaw” Method: Each student is responsible for mastering (and teaching) a distinct part of the project. Individually assess their knowledge of their assigned piece and/or others’ pieces.\n",
      "- Individual Deliverables: Alongside the group product, require each student to submit a reflection, solution, or section independently (graded separately).\n",
      "- Embedded Quizzes/Check-Ins: After group work, administer short individual quizzes focused on the same content—count these for a meaningful portion of the grade.\n",
      "\n",
      "\n",
      "=== ### 2. Hybrid Grading Models ===\n",
      "\n",
      "Split project grades into:\n",
      "- Group Component (e.g., 40–60%): Based on collaborative product/presentation.\n",
      "- Individual Component (e.g., 40–60%): Based on individual quizzes, written explanations, or oral defenses.\n",
      "\n",
      "\n",
      "=== ### 3. Role Rotation + Targeted Assessment ===\n",
      "\n",
      "If roles rotate, after each cycle, ask students to individually apply the skills/knowledge from their recently-held role. For example, after being the “data analyst,” each student completes a short analysis on their own.\n",
      "\n",
      "\n",
      "=== ### 4. Frequent, Low-Stakes Individual Checks ===\n",
      "\n",
      "Between major group milestones, intersperse brief, ungraded (or lightly graded) quizzes, exit tickets, or concept maps. This can highlight gaps quickly.\n",
      "\n",
      "\n",
      "=== ### 5. Transparent Rubrics and Reflection ===\n",
      "\n",
      "- Share rubrics that explicitly measure both team and individual contributions.\n",
      "- Require metacognitive reflections: What did you learn? How did your group help/hinder your understanding? What will you do differently next time?\n",
      "\n",
      "\n",
      "=== ### 6. Peer Teaching Moments ===\n",
      "\n",
      "Ask students to demonstrate or explain concepts to the class or teacher—rotate who presents. This signals that everyone needs to understand the content.\n",
      "\n",
      "Sample Implementation:\n",
      "\n",
      "Let’s say students are working on a project about ecosystems:\n",
      "\n",
      "1. Group product: Build a diorama, model, or presentation together.  \n",
      "2. Individual check: Each student writes a short essay explaining the food web’s energy flow or takes a quiz on project content.  \n",
      "3. Reflection: Each student writes about what they contributed and what they learned.\n",
      "\n",
      "Summary Table:\n",
      "\n",
      "| Component           | Measured By                      | Who is assessed    |\n",
      "|\n",
      "\n",
      "|\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "=== | ===\n",
      "| Group Product       | Presentation, project rubric      | Whole group        |\n",
      "| Individual Mastery  | Short quiz, written explanation   | Each student       |\n",
      "| Reflection          | Written self-assessment           | Each student       |\n",
      "\n",
      "In short:  \n",
      "Preserve teamwork by making group outcomes meaningful, but embed regular, varied individual assessments that directly measure each student’s understanding. Communicate clearly that collaboration is not a shortcut for individual mastery, but a means to help each attain it.\n"
     ]
    }
   ],
   "source": [
    "## Without analogical reasoning - raw LLM output\n",
    "result = basic_output(test_q)\n",
    "\n",
    "print(\"\\n=== GPT-4.1 RAW Response ===\\n\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb46176",
   "metadata": {},
   "source": [
    "#### Without Analogical Reasoning but with COT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "15022384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GPT-4.1 Response WITH COT ===\n",
      "\n",
      "Certainly! Here’s a structured step-by-step approach to redesign the group project system so it preserves teamwork benefits and ensures clear individual accountability:\n",
      "\n",
      "### Step 1: Diagnose the Issue\n",
      "- Problem Identified: Students are benefitting less individually from group projects (as shown by lower quiz scores).\n",
      "- Possible Cause: Some students may rely too much on peers (\"social loafing\") or not engage deeply enough, diluting individual learning.\n",
      "\n",
      "### Step 2: Define Goals\n",
      "- Measurable Individual Mastery: Each student should demonstrate understanding of key concepts.\n",
      "- Teamwork Benefits: Students should still practice collaboration, communication, and peer learning.\n",
      "\n",
      "### Step 3: Introduce Individual Accountability Measures\n",
      "\n",
      "#### a) Hybrid Assessment Design\n",
      "- Group Output: Keep the collaborative project work.\n",
      "- Individual Output: Require each student to submit a short, related individual assignment (e.g., reflection, explanation, or mini-report demonstrating their understanding of the main concepts or their contribution).\n",
      "\n",
      "#### b) Individual Quizzes Linked to Project\n",
      "- Design quizzes based specifically on the group project content, but with individual completion.  \n",
      "- Make it clear that quiz performance affects overall grades, reinforcing the need for personal mastery.\n",
      "\n",
      "#### c) Random Individual Presentations\n",
      "- After project submission, randomly select students to individually present or explain parts of the project. This can be oral, visual, or written.\n",
      "- Assess their explanations to gauge their personal grasp of the material.\n",
      "\n",
      "#### d) Structured Self & Peer Assessment\n",
      "- Continue peer evaluations, but refine rubrics to focus on specific content contributions and engagement, not just participation.\n",
      "- Include a self-assessment section to encourage metacognition (\"What did I learn? What still confuses me?\").\n",
      "\n",
      "### Step 4: Refine Group Structure and Accountability\n",
      "\n",
      "#### a) Role Rotation + Individual Goal Setting\n",
      "- Within groups, assign rotating roles, but require each student to set and share a personal learning goal related to the project.\n",
      "- At the end, have students submit a brief statement reflecting on whether they achieved their goal.\n",
      "\n",
      "#### b) Visible Contribution Logs\n",
      "- Use shared documents or logs where each member records their specific contributions (ideas, research completed, tasks handled).\n",
      "- Review logs for evidence of engagement during grading.\n",
      "\n",
      "### Step 5: Feedback Loop and Continuous Improvement\n",
      "- Analyze outcomes: After implementing these changes, continuously check if quiz scores and mastery improve.\n",
      "- Student Input: Gather feedback to adjust workload, supports, and assessment balance.\n",
      "\n",
      "\n",
      "=== ### Sample Workflow ===\n",
      "\n",
      "Before project:\n",
      "- Students know there will be both group and individual evaluation.\n",
      "\n",
      "During project:\n",
      "- Students log contributions.\n",
      "- Set and share individual learning goals.\n",
      "\n",
      "After project:\n",
      "- Submit group product.\n",
      "- Each student submits a reflection/mini-report.\n",
      "- Take individual quiz based on project concepts.\n",
      "- Peer/self-assess using structured rubric.\n",
      "- Some students present or explain their group’s work individually.\n",
      "\n",
      "Key Takeaway:  \n",
      "Blend teamwork with purposeful individual accountability: The system should make it *necessary* for each student to master material alone for full credit, without scrapping the collaborative elements that build real-world skills.\n",
      "\n",
      "Let me know if you’d like examples of assignments, rubrics, or quiz types to include in this workflow!\n"
     ]
    }
   ],
   "source": [
    "## Without analogical reasoning - raw LLM output\n",
    "result = basic_output_COT(test_q)\n",
    "\n",
    "print(\"\\n=== GPT-4.1 Response WITH COT ===\\n\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a23f11b",
   "metadata": {},
   "source": [
    "#### Without Analogical Reasoning but with Basic Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "98356bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GPT-4.1 Response WITH Prompt Engineering ===\n",
      "\n",
      "You’re confronting a classic challenge in collaborative learning: fostering teamwork without sacrificing individual accountability and measurable mastery. Here are several practical yet innovative redesign strategies:\n",
      "\n",
      "\n",
      "=== ### 1. Hybrid Assessment Model ===\n",
      "Combine group and individual assessments within each project cycle:\n",
      "- Individual Prep Quizzes: Before group work, give a short quiz on key concepts. This ensures baseline understanding.\n",
      "- Group Project: Proceed with collaborative tasks, rotating roles to ensure equity.\n",
      "- Post-Project Individual Reflection/Quiz: After the group work, require each student to submit an individual assignment (e.g., analysis, quiz, application task) directly tied to the project. This can count significantly toward their grade.\n",
      "\n",
      "\n",
      "=== ### 2. “Jigsaw Plus” Structure ===\n",
      "Modify the classic jigsaw strategy:\n",
      "- Assign each student a unique subtopic to master and teach their team.\n",
      "- Peer-teaching accountability: Randomly quiz students individually on any subtopic, not just their own. The expectation that anyone could be called on increases preparation.\n",
      "- Group checkpoints: Teams earn collective marks, but students also get individual scores.\n",
      "\n",
      "\n",
      "=== ### 3. Transparent Contribution Tracking ===\n",
      "Leverage digital tools or structured logs:\n",
      "- Process Journals: Require students to log their contributions and learning (e.g., Google Docs with version history).\n",
      "- Random Interview Rounds: During or after projects, conduct short interviews or oral checks with individual team members about project content and process.\n",
      "- Peer Verification: Use structured peer assessment forms, but supplement them with teacher spot-checks or mini-presentations.\n",
      "\n",
      "\n",
      "=== ### 4. Mastery-Based Progression (“Passports”) ===\n",
      "- Grant each student a “passport” with specific learning objectives (LOs) they must demonstrate individually, even within a group context.\n",
      "- Students get a “stamp” only when they show mastery—via quiz, oral response, or artifact—regardless of group outcome. Teams can support each other, but mastery is verified solo.\n",
      "\n",
      "\n",
      "=== ### 5. Incentivized Division of Labor ===\n",
      "- Team Score = Group + Individual Components: For example, 60% group product, 40% individual accountability piece.\n",
      "- Bonus System: If all team members demonstrate mastery on individual post-tests, award bonus points to the whole group, encouraging peer support without enabling free-riding.\n",
      "\n",
      "Summary Table:\n",
      "\n",
      "| Strategy                                 | Individual Accountability | Team Collaboration |\n",
      "|\n",
      "\n",
      "|\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "=== | ===\n",
      "| Hybrid Assessment                        | High                     | High               |\n",
      "| Jigsaw Plus                              | High                     | High               |\n",
      "| Contribution Tracking/Interviews          | Medium-High              | High               |\n",
      "| Mastery “Passports”                       | High                     | Medium-High        |\n",
      "| Team + Individual Grading / Bonus         | Medium-High              | High               |\n",
      "\n",
      "\n",
      "=== ## Quick-Start Example Redesign ===\n",
      "\n",
      "A. Project Launch:  \n",
      "- Individual readiness quiz (graded)\n",
      "\n",
      "B. Group Work:  \n",
      "- Rotating roles  \n",
      "- Collaborative task\n",
      "\n",
      "C. Individual Product:  \n",
      "- Reflection or application problem (graded)\n",
      "\n",
      "D. Random Spot-Checks:  \n",
      "- Mini oral quizzes/interviews (unannounced)\n",
      "\n",
      "E. Group + Individual Grading:  \n",
      "- 60% group score, 40% individual mastery (with potential team bonus)\n",
      "\n",
      "In essence:  \n",
      "Preserve collaborative work for skill-building, but make sure individual understanding gets measured and valued in the final grade. Combine group process observation, individual assessments, and extrinsic incentives for group-wide mastery.\n",
      "\n",
      "Let me know if you’d like an example rubric or a digital tool recommendation!\n"
     ]
    }
   ],
   "source": [
    "## Without analogical reasoning - raw LLM output\n",
    "result = basic_output_prompt_engin(test_q)\n",
    "\n",
    "print(\"\\n=== GPT-4.1 Response WITH Prompt Engineering ===\\n\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a10ec8-c685-4bd2-810c-10e3ce687a61",
   "metadata": {},
   "source": [
    "## Automatic Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c752ecda-6308-4390-952b-3951023f4554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# function to import the evaluation questions\n",
    "def import_questions(file_name: str) -> list[str]:\n",
    "    with open(file_name, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    problems = [case['problem_description'] for case in data['cases']]\n",
    "    return problems\n",
    "\n",
    "# For each question, get outputs from four/five different agents \n",
    "# function to load the agents \n",
    "def auto_eval(question: str) -> list[str]:\n",
    "    response = []\n",
    "    response.append(analogical_output(question))\n",
    "    response.append(basic_output(question))\n",
    "    response.append(basic_output_COT(question))\n",
    "    response.append(basic_output_prompt_engin(question))\n",
    "    response.append(basic_output_full_COT(question))\n",
    "\n",
    "    return response\n",
    "\n",
    "def auto_eval_batch(questions: list[str]) -> list[list[str]]:\n",
    "    responses = []\n",
    "    for question in questions:\n",
    "        response = auto_eval(question)\n",
    "        responses.append(response)\n",
    "    return responses\n",
    "\n",
    "def llm_judge(user_query: str, response: list[str]) -> pd.DataFrame: \n",
    "    response_1 = response[0]\n",
    "    response_2 = response[1]\n",
    "    response_3 = response[2]\n",
    "    response_4 = response[3]\n",
    "    response_5 = response[4]\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Here is the user query: {user_query}\n",
    "    I will give you 5 sample responses generated using different reasoning methods. I need you to evaluate them side by side on 2 criteria: innovativeness (insightfulness) and practicality. \n",
    "    Score each criterion from 1 to 10, and return the result **strictly in the following JSON format**:\n",
    "\n",
    "    {{\n",
    "      \"Response A\": {{\"Innovativeness\": int, \"Practicality\": int}},\n",
    "      \"Response B\": {{\"Innovativeness\": int, \"Practicality\": int}},\n",
    "      \"Response C\": {{\"Innovativeness\": int, \"Practicality\": int}},\n",
    "      \"Response D\": {{\"Innovativeness\": int, \"Practicality\": int}}\n",
    "      \"Response E\": {{\"Innovativeness\": int, \"Practicality\": int}}\n",
    "    }}\n",
    "\n",
    "    User Query: {user_query}\n",
    "    \n",
    "    Response A: {response[0]}\n",
    "    \n",
    "    Response B: {response[1]}\n",
    "    \n",
    "    Response C: {response[2]}\n",
    "    \n",
    "    Response D: {response[3]}\n",
    "\n",
    "    Response E: {response[3]}\n",
    "    \"\"\"\n",
    "    raw_output = llms.invoke(prompt)\n",
    "    # Extract content from AIMessage\n",
    "    content = raw_output.content if hasattr(raw_output, 'content') else str(raw_output)\n",
    "    \n",
    "    json_start = content.find('{')\n",
    "    json_data = content[json_start:]\n",
    "\n",
    "    try:\n",
    "        scores = json.loads(json_data)\n",
    "        df = pd.DataFrame.from_dict(scores, orient='index')\n",
    "        df.index.name = \"response_label\"\n",
    "        df = df.reset_index()\n",
    "        return df\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(\"JSON parsing failed:\", e)\n",
    "        print(\"Raw LLM output was:\\n\", content)\n",
    "        return None\n",
    "\n",
    "def evaluate_multiple_questions(user_queries: list[str], all_responses: list[list[str]]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    user_queries: list of queries, one per question\n",
    "    all_responses: list of response lists, each list has 4 responses for the corresponding query\n",
    "    lms: the LLM interface with .invoke(prompt)\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "\n",
    "    for i, (query, responses) in enumerate(zip(user_queries, all_responses)):\n",
    "        df = llm_judge(query, responses)  # assume it returns a DataFrame as defined earlier\n",
    "        if df is not None:\n",
    "            df = df.reset_index().rename(columns={\"index\": \"response_label\"})\n",
    "            df['question_id'] = i\n",
    "            results.append(df)\n",
    "\n",
    "    combined_df = pd.concat(results, ignore_index=True)\n",
    "    return combined_df\n",
    "    \n",
    "def find_mean(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    mean_scores = data.groupby(\"response_label\")[[\"Innovativeness\", \"Practicality\"]].mean()\n",
    "    return mean_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dbd7dabc-9004-43b8-a7a0-4f0767881f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_list = import_questions(\"Questions.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "31678251-ffd3-4c9e-ad97-cfc7ce9630e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = auto_eval(questions_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7922dec3-b305-4ec4-902a-9ad42fe1b2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm_judge(questions_list[0], response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7e72321d-3bf6-41e1-8dd5-07ea51b9e71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = auto_eval_batch(questions_list[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fb85cbc8-6f36-4cdf-84e2-df8547e79840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response_label</th>\n",
       "      <th>response_label</th>\n",
       "      <th>Innovativeness</th>\n",
       "      <th>Practicality</th>\n",
       "      <th>question_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Response A</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Response B</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Response C</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Response D</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Response A</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>Response B</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>Response C</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>Response D</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  response_label response_label  Innovativeness  Practicality  question_id\n",
       "0              0     Response A              10             8            0\n",
       "1              1     Response B               6             9            0\n",
       "2              2     Response C               7             9            0\n",
       "3              3     Response D               8             9            0\n",
       "4              0     Response A              10             8            1\n",
       "5              1     Response B               7            10            1\n",
       "6              2     Response C               6             9            1\n",
       "7              3     Response D               8             9            1"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_multiple_questions(questions_list[:2], responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecd02e69-0a11-4f76-9d69-0492a211b6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_innovativeness(data: pd.DataFrame): \n",
    "    # Plot for Innovativeness\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for label in data['response_label'].unique():\n",
    "        subset = data[data['response_label'] == label]\n",
    "        plt.plot(subset['question_id'], subset['Innovativeness'], label=label, marker='o')\n",
    "        plt.title(\"Innovativeness Scores by Response Type\")\n",
    "        plt.xlabel(\"Question ID\")\n",
    "        plt.ylabel(\"Innovativeness Score\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def plot_practicality(data: pd.DataFrame):\n",
    "    # Plot for Practicality\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for label in data['response_label'].unique():\n",
    "        subset = data[data['response_label'] == label]\n",
    "        plt.plot(subset['question_id'], subset['Practicality'], label=label, marker='o')\n",
    "    plt.title(\"Practicality Scores by Response Type\")\n",
    "    plt.xlabel(\"Question ID\")\n",
    "    plt.ylabel(\"Practicality Score\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539df397-dcd8-43f2-9001-ae8de9903e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify questions\n",
    "# add full_COT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analogical-lc-env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
