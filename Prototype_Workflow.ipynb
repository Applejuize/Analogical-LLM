{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f00062d-0f67-46b5-aca0-b7857d11c4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing import TypedDict\n",
    "import openai \n",
    "# from langgraph_supervisor import create_supervisor\n",
    "from langchain.chat_models import init_chat_model\n",
    "from textwrap import dedent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9c75eb6-69bf-40c9-8ed4-8b96c4d1f285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts\n",
    "TARGET_DOMAIN = dedent(\"\"\"\n",
    "        As a Domain Analysis Specialist, extract the core innovation domain from the user query.\n",
    "        Instructions:\n",
    "        1. Analyze the user's input\n",
    "        2. Identify the primary domain requiring innovation\n",
    "        3. Classify it within standard innovation categories\n",
    "        Output Format:\n",
    "        Target Domain: [Clear, specific domain label]\n",
    "        Be very detailed and specific in your response and do not generalize. Respond ONLY with the name of the domain, do NOT include ANY other text like 'Target Domain:'.\n",
    "        User query: {user_query}\n",
    "\"\"\").strip()\n",
    "\n",
    "PROBLEM_LANDSCAPE = dedent(\"\"\"\n",
    "        You are a Problem Landscape Analyst. Your task is to map out the concrete challenges within the target domain identified.\n",
    "        Instructions:\n",
    "        1. Identify 3-5 core problems or challenges currently present in this domain.\n",
    "        2. For each problem, provide:\n",
    "        - Problem: A short, clear title.\n",
    "        - Description: 2-3 sentences explaining what the problem is and why it matters.\n",
    "        - Context: Briefly state the circumstances or environment where this problem occurs.\n",
    "        - Stakeholders: List the main groups or individuals affected.\n",
    "        - Root Causes: Identify 1-3 underlying causes, if known.\n",
    "        - Impact: State the significance of the problem (e.g., social, economic, technical).\n",
    "        - Current Approaches: How is this problem currently addressed?\n",
    "        - Limitations: What are the shortcomings of current approaches?\n",
    "        - Success Metrics: How would you measure if this problem is solved?\n",
    "        - Interconnections: Note if this problem is linked to or influenced by other problems.\n",
    "        Output Format:\n",
    "        Present your findings as a structured list or JSON array, with each problem fully described as above.\n",
    "        Important:\n",
    "        - Focus on clarity and completeness.\n",
    "        - Avoid abstracting or generalizing; stay concrete and domain-specific.\n",
    "        - Do not propose solutions; only describe the current problem landscape.\n",
    "        Target domain: {target_domain}\n",
    "\"\"\").strip()\n",
    "\n",
    "ABSTRACTION = dedent(\"\"\"\n",
    "You are a TRIZ Methodology Expert. Transform domain-specific problems into universal contradictions.\n",
    "        Process:\n",
    "        1. For each problem in the problem landscape\n",
    "        - Abstract to universal parameters (what improves vs. what worsens)\n",
    "        - Express as 'When we improve X, Y worsens'\n",
    "        - Ensure parameters are domain-agnostic\n",
    "        2. Identify 3-4 core contradictions:\n",
    "        - Select the most fundamental tensions\n",
    "        - Map to TRIZ contradiction matrix\n",
    "        - Note applicable inventive principles\n",
    "        Output:\n",
    "        # Core Contradictions:\n",
    "        1. Improving [parameter] vs. Worsening [parameter]\n",
    "        - TRIZ Principles: [1-3 relevant principles]\n",
    "        - Innovation Potential: [High/Medium/Low]\n",
    "        - Universal Application: [Brief example from another domain]\n",
    "        Focus on contradictions that, if resolved, would create breakthrough value.\n",
    "\n",
    "        Problem landscape: {problem_landscape}\n",
    "\"\"\").strip()\n",
    "\n",
    "BASE_DOMAIN = dedent(\"\"\"\n",
    "        You are a Cross-Domain Search Specialist.\n",
    "        For each contradiction provided, identify two distinct source domains (fields or industries) where this contradiction has been successfully addressed.\n",
    "        The domains should have A CONCEPTUAL DISTANCE OF AT LEAST 2 DISTINCT HOPS FROM WHAT IMMEDIATELY COMES TO MIND. Be creative! It can be domains within spheres like natural, phsyical, social, artistic, anything.\n",
    "        For each domain, briefly explain why it is relevant to the contradiction. Do not describe specific solutions-only list the domains and your rationale.\n",
    "        Output:\n",
    "        A list for each contradiction, naming two relevant domains with a one-sentence rationale for each.\n",
    "\n",
    "        Contradictions: {contradictions}\n",
    "\"\"\").strip()\n",
    "\n",
    "BASE_SOLUTIONS = dedent(\"\"\"\n",
    "        You are a Solution Pattern Extractor. You are provided with an input with 2 base domains identified per contradiction.\n",
    "        For each of these identified base domains , identify one specific, well-documented solution pattern within the domain that effectively resolves the contradiction.\n",
    "        For each solution pattern:\n",
    "        - The original base domain identified\n",
    "        - The name or label of the solution pattern\n",
    "        - A detailed description of the core mechanism or principle involved and how it addressed the domain's contradiction\n",
    "        - The context or situation in the domain where this pattern is applied\\n\"\n",
    "        Do not generalize or adapt the solution-simply describe how the contradiction is addressed within each source domain.\n",
    "        Output:\n",
    "        For each of the provided domain, list the base domain name, solution pattern name, a detailed description of its mechanism, and the context in which it is used.\n",
    "\n",
    "        Input: {input}\n",
    "\"\"\").strip()\n",
    "\n",
    "ANALOGICAL_TRANSFER = dedent(\"\"\"\n",
    "You are an Analogical Transfer Specialist.\n",
    "\n",
    "Your task is to propose how solution patterns used to resolve abstracted contradictions in various base domains might inspire solution framings for the original target domain.\n",
    "\n",
    "Input:\n",
    "1. A list of abstracted contradictions, each with two base domains and their associated solution patterns (including how each contradiction was resolved in those domains).\n",
    "2. The original target domain.\n",
    "\n",
    "Instructions:\n",
    "For each contradiction, review the solution patterns from both base domains. For each pattern:\n",
    "- Analyze the core mechanism or principle behind the solution.\n",
    "- Map and adapt this mechanism conceptually to the target domain, considering the specific context and needs of the target domain.\n",
    "- Clearly describe how this analogical transfer could frame a potential solution in the target domain.\n",
    "- Highlight any key adaptations, considerations, or limitations that would be relevant when applying this pattern to the target domain.\n",
    "\n",
    "Output:\n",
    "For each contradiction and base domain solution pattern, provide a comprehensive description of a proposed solution framing for the target domain, including:\n",
    "- The original contradiction addressed\n",
    "- The source domain and solution pattern\n",
    "- A detailed explanation of how the pattern could inspire or inform a solution in the target domain\n",
    "- Any important adaptations or considerations for successful transfer\n",
    "\n",
    "Input:\n",
    "- List of abstracted contradictions: {contradictions_solutions}\n",
    "- Original target domain: {target_domain}\n",
    "\"\"\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e573d7e-e43f-4c7d-8752-ae11a6198c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for openai API\n",
    "def _set_if_undefined(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"Please provide your {var}\")\n",
    "\n",
    "\n",
    "_set_if_undefined(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a188a9e3-c538-487f-9dde-8dfe295575db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph state: workflow\n",
    "class ReasoningState(TypedDict):\n",
    "    user_query: str\n",
    "    target_domain: str\n",
    "    problem_landscape: str\n",
    "    abstraction: str\n",
    "    base_domain: str\n",
    "    base_solutions: str\n",
    "    analogical_transfer: str\n",
    "    solution: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "306439ab-4b94-4084-8a1c-8c53ecf1a0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can switch between different LLMs \n",
    "llms = init_chat_model(\"openai:gpt-4.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54ea64a9-96a1-42ab-bb3d-1a1c1d14b1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st Agent: Identify Target Domain from User Input\n",
    "def target_domain_agent(state: ReasoningState):\n",
    "    u = state['user_query']\n",
    "\n",
    "    msg = llms.invoke(TARGET_DOMAIN.format(user_query=u))\n",
    "\n",
    "    return {\"target_domain\": msg.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d327c06e-9618-4463-b19e-2be74a21d14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2st Agent: Conduct comprehensive research into problem landscape for target domain \n",
    "# (i.e. What specific challenges exist in this domain?)\n",
    "def problem_landscape_agent(state: ReasoningState):\n",
    "    t = state['target_domain']\n",
    "\n",
    "    msg = llms.invoke(PROBLEM_LANDSCAPE.format(target_domain=t))\n",
    "\n",
    "    return {\"problem_landscape\": msg.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26b96ec9-7bdb-4803-9bab-dcc82d6a4c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3rd Agent: Abstract Problems Identified into Generalized Principles and TRIZ Contradiction\n",
    "def abstraction_agent(state: ReasoningState):\n",
    "    p = state['problem_landscape']\n",
    "\n",
    "    msg = llms.invoke(ABSTRACTION.format(problem_landscape=p))\n",
    "\n",
    "    return {\"abstraction\": msg.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e95a5d2f-1773-4339-ba89-995d185885c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4th Agent: Search for Appropriate Base Domains\n",
    "def base_domain_agent(state: ReasoningState):\n",
    "    a = state['abstraction']\n",
    "\n",
    "    msg = llms.invoke(BASE_DOMAIN.format(contradictions=a))\n",
    "\n",
    "    return {\"base_domain\": msg.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7db11f3-e388-48b4-8661-cde636b1ba50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5th Agent: Identify Solution in Base Domain\n",
    "def base_solution_agent(state: ReasoningState):\n",
    "    b = state['base_domain']\n",
    "\n",
    "    msg = llms.invoke(BASE_SOLUTIONS.format(input=b))\n",
    "\n",
    "    return {\"base_solutions\": msg.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5dc1b395-b154-493e-9c57-7e5a6926cf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6th Agent: Base Domain Solution Informing Target Domain Solution\n",
    "def analogical_transfer_agent(state: ReasoningState):\n",
    "    if \"base_solutions\" not in state:\n",
    "        raise ValueError(\"Missing 'base_solutions' key. Check if previous node returned it.\")\n",
    "    b = state['base_solutions']\n",
    "    t = state['target_domain']\n",
    "    \n",
    "    msg = llms.invoke(ANALOGICAL_TRANSFER.format(contradictions_solutions=b, target_domain = t))\n",
    "\n",
    "    return {\"analogical_transfer\": msg.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3626041-fcd5-4bf3-a32b-f164352e788f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7th Agent: Summarize everything and respond to the question\n",
    "def synthesis_agent(state: ReasoningState):\n",
    "    msg = llms.invoke(\n",
    "        f\"Evaluate the proposed analogical solutions. Find the best one that balances practicality with innovation. Then, provide a detailed, well-structured response that addresses all aspects of the query.\\n\\n\"\n",
    "        f\"Problem: {state['user_query']}\\n\"\n",
    "        f\"Analogical Solutions: {state['analogical_transfer']}\"\n",
    "        f\"In your output, remember to abstract away the analogy itself such that it is focused on responding to the user input.\"\n",
    "    )\n",
    "    return {\"solution\": msg.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68994430-5388-4c63-84be-919d5978d5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the workflow \n",
    "workflow = StateGraph(ReasoningState)\n",
    "\n",
    "workflow.add_node(\"target\", target_domain_agent)\n",
    "workflow.add_node(\"landscape\", problem_landscape_agent)\n",
    "workflow.add_node(\"abstract\", abstraction_agent)\n",
    "workflow.add_node(\"base\", base_domain_agent)\n",
    "workflow.add_node(\"base_soln\", base_solution_agent)\n",
    "workflow.add_node(\"analogy\", analogical_transfer_agent)\n",
    "workflow.add_node(\"synthesis\", synthesis_agent)\n",
    "\n",
    "workflow.set_entry_point(\"target\")\n",
    "\n",
    "# Define edges\n",
    "workflow.add_edge(\"target\", \"landscape\")\n",
    "workflow.add_edge(\"landscape\", \"abstract\")\n",
    "workflow.add_edge(\"abstract\", \"base\")\n",
    "workflow.add_edge(\"base\", \"base_soln\")\n",
    "workflow.add_edge(\"base_soln\", \"analogy\")\n",
    "workflow.add_edge(\"analogy\", \"synthesis\")\n",
    "\n",
    "workflow.set_finish_point(\"synthesis\")\n",
    "\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5235345-734c-4aae-a32b-82acc97ddcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_q = \"Teachers introduced structured group projects using peer evaluations and rotating roles to boost collaboration in science classes. Despite these measures, anonymous student surveys revealed that individual quiz scores post-project dropped compared to solo assignments, signaling diluted accountability. How can educators redesign design a system to ensure measurable individual mastery while preserving the benefits of teamwork?\"\n",
    "input_state = {\"user_query\": test_q}\n",
    "\n",
    "final_state = graph.invoke(input_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "750d99f1-e935-4250-82dd-d110d2e098e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse_output\n",
    "import re\n",
    "\n",
    "def parse_solution(text):\n",
    "    # Remove Markdown formatting like \"**\" and \"\\n\"\n",
    "    clean_text = text.replace(\"**\", \"\").replace(\"\\\\n\", \"\\n\")\n",
    "\n",
    "    # Split into sections by horizontal rules (---)\n",
    "    sections = re.split(r'-{3,}', clean_text)\n",
    "\n",
    "    # Create a readable version of each section\n",
    "    readable_output = []\n",
    "    for section in sections:\n",
    "        section = section.strip()\n",
    "        if not section:\n",
    "            continue\n",
    "        # Optional: separate title and body if present\n",
    "        lines = section.split(\"\\n\")\n",
    "        if len(lines) > 1 and \":\" not in lines[0]:\n",
    "            # First line is likely a heading\n",
    "            heading = lines[0]\n",
    "            body = \"\\n\".join(lines[1:])\n",
    "            readable_output.append(f\"\\n=== {heading} ===\\n{body}\")\n",
    "        else:\n",
    "            readable_output.append(section)\n",
    "\n",
    "    return \"\\n\\n\".join(readable_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c40e70ac-30df-4ae1-bbfd-3b3cf4db43d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all parsed outputs to a text file\n",
    "with open(\"sample_output.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for key in final_state:\n",
    "        raw_output = str(final_state[key])  # Ensure it's a string\n",
    "        final_output = parse_solution(raw_output)\n",
    "        f.write(f\"\\n### {key} ###\\n\")\n",
    "        f.write(final_output)\n",
    "        f.write(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4b68a71d-9193-4005-ac65-5a668d06b1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate output without analogical reasoning Pipeline for comparison\n",
    "def basic_output(user_input: str):\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    {user_input}\n",
    "    \"\"\"\n",
    "\n",
    "    response = llms.invoke(prompt) \n",
    "\n",
    "    # Parse and format the response\n",
    "    formatted_response = parse_solution(response.content)\n",
    "\n",
    "    return(formatted_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2d5a4675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GPT-4.1 Response Without Analogical Reasoning Pipeline ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_q = \"Teachers introduced structured group projects using peer evaluations and rotating roles to boost collaboration in science classes. Despite these measures, anonymous student surveys revealed that individual quiz scores post-project dropped compared to solo assignments, signaling diluted accountability. How can educators redesign design a system to ensure measurable individual mastery while preserving the benefits of teamwork?\"\n",
    "result = basic_output(test_q)\n",
    "\n",
    "print(\"\\n=== GPT-4.1 Response Without Analogical Reasoning Pipeline ===\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "400da2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You’re facing a classic challenge: balancing authentic collaboration with the need for individual accountability and measurable learning outcomes. Below are several evidence-based strategies to ensure students maintain and demonstrate individual mastery, even within group-based science projects:\n",
      "\n",
      "\n",
      "=== ### 1. Hybrid Assessment Model ===\n",
      "\n",
      "- Combine group and individual grades:  \n",
      "  Assign a portion of the project grade to the *group* (based on final product, teamwork, etc.), but *reserve a significant percentage (e.g., 40–60%)* for individual performance, demonstrated through:\n",
      "    - Quizzes/tests on project content\n",
      "    - Individual write-ups or reflections\n",
      "    - “Exit tickets” or quick checks addressing key concepts\n",
      "\n",
      "\n",
      "=== ### 2. Embedded Individual Tasks ===\n",
      "\n",
      "- Interleaved Solo Work:  \n",
      "  Require each student to submit or present a distinct section of the project—such as a data analysis, explanation, or conclusion—in addition to the group submission.\n",
      "- Rotating “Expert” Check-ins:  \n",
      "  Each student is responsible for “teaching back” their assigned role or finding, either to a peer, the teacher, or a small group.\n",
      "\n",
      "\n",
      "=== ### 3. Accountability Structures Within Teams ===\n",
      "\n",
      "- Random Spot Checking:  \n",
      "  Prior to or after group presentations, randomly select a student to answer questions about *any* part of the project. This raises the stakes for every member to understand all components.\n",
      "- “Jigsaw” Variation:  \n",
      "  Begin with students mastering a subtopic individually. They teach it to their group, and ultimately all members must demonstrate mastery of each subtopic on an individual assessment.\n",
      "\n",
      "\n",
      "=== ### 4. Individual Formative Assessments Tied to Group Work ===\n",
      "\n",
      "- Frequent, Low-Stakes Quizzes:  \n",
      "  Make short, individual quizzes on project-related skills/content a regular checkpoint.\n",
      "- Reflection Prompts:  \n",
      "  Ask students to explain what they personally contributed and learned, requiring specific references to both team interaction and subject matter.\n",
      "\n",
      "\n",
      "=== ### 5. Transparent Rubrics and Feedback ===\n",
      "\n",
      "- Clear Criteria for Individual Work:  \n",
      "  Share with students in advance *how* their individual understanding will be measured, both during and after the group project.\n",
      "- Peer and Self-Evaluation:  \n",
      "  While peer evaluations are helpful in gauging group dynamics, supplement them with teacher assessment of individual learning.\n",
      "\n",
      "\n",
      "=== ### 6. Gradual Release of Responsibility ===\n",
      "\n",
      "- Scaffolded Experiences:  \n",
      "  Early in the year or in lower-stakes projects, emphasize teamwork. Gradually increase the weight of individual mastery as students become skilled collaborators.\n",
      "\n",
      "\n",
      "=== ### Concrete Example ===\n",
      "\n",
      "Suppose students create a group presentation on photosynthesis. The revised system could look like this:\n",
      "\n",
      "- Group: Collaborate on research, visuals, and oral delivery (40%).\n",
      "- Individual:  \n",
      "   - Each student writes a short essay explaining a key process (e.g., light-dependent reactions) in their own words (20%).\n",
      "   - Each student takes a brief quiz on the topic (20%).\n",
      "   - During the presentation, any student may be asked an impromptu question by the teacher (20%).\n",
      "\n",
      "In summary:  \n",
      "*Preserving teamwork while ensuring individual accountability means regularly and transparently measuring each student’s learning, both within and beyond the group setting. By weaving in solo deliverables, spot-checks, and individual assessments throughout collaborative projects, you’ll foster both critical 21st-century skills and content mastery.*\n",
      "\n",
      "References:  \n",
      "- Slavin, R. E. (1995). *Cooperative learning: Theory, research, and practice.*\n",
      "- Johnson, D. W., Johnson, R. T., & Smith, K. A. (2014). *Cooperative learning: Improving university instruction by basing practice on validated theory.*  \n",
      "\n",
      "If you have specific project types or age groups in mind, I can tailor suggestions even more closely!\n"
     ]
    }
   ],
   "source": [
    "## GPT 4.1 without Analogical Reasoning Without CoT\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4cde0abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate output without analogical reasoning Pipeline BUT WITH COT for comparison\n",
    "def basic_output_COT(user_input: str):\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    {user_input}\n",
    "    Think step by step.\n",
    "    \"\"\"\n",
    "\n",
    "    response = llms.invoke(prompt) \n",
    "\n",
    "    # Parse and format the response\n",
    "    formatted_response = parse_solution(response.content)\n",
    "\n",
    "    return(formatted_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "35361e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GPT-4.1 Response Without Analogical Reasoning Pipeline WITH COT ===\n",
      "\n",
      "Certainly! To address the problem—group work fostering teamwork but possibly reducing individual mastery/accountability—educators should reconsider both the design and assessment of group projects. Here’s a step-by-step approach:\n",
      "\n",
      "### Step 1: Diagnose the Core Issues\n",
      "- Observation: Individual post-project quizzes show lower scores than after solo work.\n",
      "- Analysis: Students may be \"hitchhiking\" (letting groupmates do more), or division of labor is masking some students’ conceptual gaps.\n",
      "\n",
      "### Step 2: Set Clear Dual Objectives\n",
      "- Maintain collaboration (teamwork, communication, problem-solving).\n",
      "- Ensure individual understanding and accountability (content mastery).\n",
      "\n",
      "### Step 3: Redesign Project Structure\n",
      "\n",
      "A. Roles and Responsibilities\n",
      "- Continue rotating roles but with defined individual deliverables for each phase.\n",
      "- Example: For a lab report, each student writes a different section (method, results, analysis) tied to understanding key concepts.\n",
      "\n",
      "B. Scaffolding Collaborative Work\n",
      "- Begin each project with an individual “readiness assurance” quiz to ensure base knowledge.\n",
      "- End with a group synthesis where students must teach back or present individually on certain components.\n",
      "\n",
      "### Step 4: Enhance Assessment Methods\n",
      "\n",
      "A. Mix of Individual and Group Grades\n",
      "- Assign a substantial portion (50–70%) of the project grade to individual performance (individual write-ups, oral defenses, or quizzes covering all project content).\n",
      "- The rest can be group-based (final artifact, group process, creativity).\n",
      "\n",
      "B. Individual Accountability During Projects\n",
      "- Use random “spot-checks”: At intervals, ask individuals to explain group decisions or data to the teacher or peers.\n",
      "- Incorporate short, unannounced individual quizzes on group project content.\n",
      "\n",
      "C. Peer Teaching\n",
      "- Each student is responsible for mastering and teaching one aspect of the project to their peers—accountability built-in.\n",
      "\n",
      "D. Reflection Components\n",
      "- Require individual reflections: “What did you learn? Which parts did you struggle with? How did you contribute?” Grade for depth and honesty.\n",
      "\n",
      "### Step 5: Provide Transparency and Feedback\n",
      "- Clearly communicate grading rubrics differentiating individual vs. group components.\n",
      "- Give quick feedback on readiness quizzes so students know where to focus.\n",
      "\n",
      "### Step 6: Monitor and Iterate\n",
      "- After implementing, monitor quiz scores, student feedback, and engagement.\n",
      "- Adjust the ratio of individual:group assessments or frequency of checks as needed.\n",
      "\n",
      "\n",
      "=== ### Sample Project Structure ===\n",
      "\n",
      "| Phase         | Activity           | Accountability Method            | Weight |\n",
      "|\n",
      "\n",
      "|\n",
      "\n",
      "|\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "=== | ===\n",
      "| Project Start | Readiness Quiz     | Individual quiz                  | 20%    |\n",
      "| Project Work  | Group tasks        | Individual section + group work  | 30%    |\n",
      "| Midpoint      | Spot-check/explain | Give oral/written explanation    | 10%    |\n",
      "| Project End   | Group presentation | Individual Q&A (oral exam)       | 20%    |\n",
      "| Post-project  | Individual reflection/quiz | Reflection/quiz        | 20%    |\n",
      "\n",
      "In summary:  \n",
      "Blend group and individual tasks and assessments throughout the project. Ensure individual mastery is measured separately, but still tied to collaborative experiences. Adjust and iterate until you find a balance for your students.\n",
      "\n",
      "Further Reading:  \n",
      "- “Cooperative Learning: Improving University Instruction by Basing Practice on Validated Theory” (Millis, 2010)\n",
      "- “Designing Group Work: Strategies for the Heterogeneous Classroom” (Cohen & Lotan, 2014)\n"
     ]
    }
   ],
   "source": [
    "test_q = \"Teachers introduced structured group projects using peer evaluations and rotating roles to boost collaboration in science classes. Despite these measures, anonymous student surveys revealed that individual quiz scores post-project dropped compared to solo assignments, signaling diluted accountability. How can educators redesign design a system to ensure measurable individual mastery while preserving the benefits of teamwork?\"\n",
    "result = basic_output_COT(test_q)\n",
    "\n",
    "print(\"\\n=== GPT-4.1 Response Without Analogical Reasoning Pipeline WITH COT ===\\n\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d7f3ea08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate output without analogical reasoning Pipeline BUT WITH PROMPT ENGINEERING for comparison (ask it to balance innovativeness with practicality)\n",
    "def basic_output_prompt_engin(user_input: str):\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    {user_input}\n",
    "    Try to balance practicality with innovation.\n",
    "    \"\"\"\n",
    "\n",
    "    response = llms.invoke(prompt) \n",
    "\n",
    "    # Parse and format the response\n",
    "    formatted_response = parse_solution(response.content)\n",
    "\n",
    "    return(formatted_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e85f2167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GPT-4.1 Response Without Analogical Reasoning Pipeline WITH Prompt Engineering ===\n",
      "\n",
      "Certainly! Balancing individual accountability with authentic collaboration is a common challenge in group work. Here’s a multi-pronged, practical-yet-innovative redesign educators can consider:\n",
      "\n",
      "\n",
      "=== 1. Hybrid Assessment Model ===\n",
      "\n",
      "- Personal Assignments Within Group Projects: Structure group projects so that each student is responsible for a distinct sub-task, deliverable, or research component. For example, in a biology project on ecosystems, one might investigate plant species, another animal relationships, etc. Require each member to submit a short individual report, reflection, or data analysis alongside the collective assignment.\n",
      "- Individual “Defense”: After project completion, each student briefly presents or answers oral/written questions about *their* contribution and the overall project. This spot-checks true comprehension and holds each accountable for both their AND the group's work.\n",
      "\n",
      "\n",
      "=== 2. Embedded “Microquizzes” ===\n",
      "\n",
      "- Frequent, Low-Stakes Quizzing: At milestones during group work, give quick, individual quizzes or online responses on the knowledge or skills developed in that phase. These are formative, inform instruction, and provide incentives for staying engaged.\n",
      "- Tie to Group Content: Make quiz content directly related to current group tasks, reinforcing that collaborative learning supports personal mastery.\n",
      "\n",
      "\n",
      "=== 3. Two-Stage Assessments ===\n",
      "\n",
      "- Collaborative + Individual Phases: Adopt a “two-stage test” approach: students first attempt a quiz/test individually, then immediately reattempt it in their groups, discussing and submitting a group answer. Weight grades, e.g., 70% individual, 30% group. This preserves teamwork but foregrounds individual effort. See: [Two-Stage Exams in STEM](https://www.cwsei.ubc.ca/resources/files/Two-stage_exams.pdf).\n",
      "\n",
      "\n",
      "=== 4. Peer & Self Reflection Focused on Learning, Not Just Process ===\n",
      "\n",
      "- Make part of the project grade reflective: students must honestly evaluate their own and their peers’ content mastery and teamwork, citing specifics (\"I learned X from Y's explanation of the process...\").\n",
      "\n",
      "\n",
      "=== 5. Autograded Tech Tools for Accountability ===\n",
      "\n",
      "- If feasible, use edtech platforms (Google Classroom, Edpuzzle, Kahoot, etc.) for short, targeted check-ins—ensuring fast feedback and easy tracking of individual understanding throughout the project cycle.\n",
      "\n",
      "\n",
      "=== 6. Rotating “Lead Teacher” Roles With Brief Tutorials ===\n",
      "\n",
      "- For especially tricky concepts, have “rotation leaders” prepare and teach ~5 minute tutorials to the group, then answer reflection questions individually. This builds buy-in and tests mastery.\n",
      "\n",
      "Summary Table:\n",
      "\n",
      "| Strategy                         | Teamwork Supported       | Individual Mastery Measured      | Practicality                          |\n",
      "|\n",
      "\n",
      "|\n",
      "\n",
      "|\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "=== | ===\n",
      "| Hybrid assessment                 | Yes                      | Yes                              | Project tweaking                      |\n",
      "| Microquizzes                      | Yes (if aligned)         | Yes (quizzes)                    | Easy with tech/short paper quizzes    |\n",
      "| Two-stage tests                   | Yes                      | Yes                              | Needs planning, but efficient         |\n",
      "| Reflective peer/self-assessment   | Yes                      | Some (with strong rubrics)       | Quick, needs good prompts             |\n",
      "\n",
      "Final Advice:  \n",
      "Blend at least two of these in each project cycle. For instance: break group projects into sub-tasks, use solo checkpoints, and wrap up with a 2-stage quiz. This maximizes learning, maintains motivation, and keeps grading manageable!\n",
      "\n",
      "Let me know if you'd like templates or rubrics for any approach.\n"
     ]
    }
   ],
   "source": [
    "test_q = \"Teachers introduced structured group projects using peer evaluations and rotating roles to boost collaboration in science classes. Despite these measures, anonymous student surveys revealed that individual quiz scores post-project dropped compared to solo assignments, signaling diluted accountability. How can educators redesign design a system to ensure measurable individual mastery while preserving the benefits of teamwork?\"\n",
    "result = basic_output_prompt_engin(test_q)\n",
    "\n",
    "print(\"\\n=== GPT-4.1 Response Without Analogical Reasoning Pipeline WITH Prompt Engineering ===\\n\")\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analogical-lc-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
