{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9f00062d-0f67-46b5-aca0-b7857d11c4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing import TypedDict, List, Dict, Any\n",
    "import openai \n",
    "# from langgraph_supervisor import create_supervisor\n",
    "from langchain.chat_models import init_chat_model\n",
    "from textwrap import dedent\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c9c75eb6-69bf-40c9-8ed4-8b96c4d1f285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts\n",
    "TARGET_DOMAIN = dedent(\"\"\"\n",
    "        As a Domain Analysis Specialist, extract all the core innovation domains from the user query. It could be a single one for a simple query, or multiple ones for a complex query.\n",
    "        Instructions:\n",
    "        1. Analyze the user's input\n",
    "        2. Identify the primary domain(s) requiring innovation\n",
    "        3. Classify it within standard innovation categories\n",
    "        Output Format:\n",
    "        Target Domain: [Clear, specific domain label]\n",
    "        Be very detailed and specific in your response and do not generalize. Respond ONLY with the name of the domain, do NOT include ANY other text like 'Target Domain:'.\n",
    "        User query: {user_query}\n",
    "\"\"\").strip()\n",
    "\n",
    "PROBLEM_LANDSCAPE = dedent(\"\"\"\n",
    "        You are a Problem Landscape Analyst. Your task is to map out the concrete challenges within the target domain identified.\n",
    "        Instructions:\n",
    "        1. Identify all the core problems or challenges currently present in these domains. Aim for at least 3 problems per domain\n",
    "        2. For each problem, provide:\n",
    "        - Problem: A short, clear title.\n",
    "        - Description: 2-3 sentences explaining what the problem is and why it matters.\n",
    "        - Context: Briefly state the circumstances or environment where this problem occurs.\n",
    "        - Stakeholders: List the main groups or individuals affected.\n",
    "        - Root Causes: Identify 1-3 underlying causes, if known.\n",
    "        - Impact: State the significance of the problem (e.g., social, economic, technical).\n",
    "        - Current Approaches: How is this problem currently addressed?\n",
    "        - Limitations: What are the shortcomings of current approaches?\n",
    "        - Success Metrics: How would you measure if this problem is solved?\n",
    "        - Interconnections: Note if this problem is linked to or influenced by other problems.\n",
    "        Output Format:\n",
    "        Present your findings as a structured list or JSON array, with each problem fully described as above.\n",
    "        Important:\n",
    "        - Focus on clarity and completeness.\n",
    "        - Avoid abstracting or generalizing; stay concrete and domain-specific.\n",
    "        - Do not propose solutions; only describe the current problem landscape.\n",
    "        Target domain: {target_domain}\n",
    "\"\"\").strip()\n",
    "\n",
    "ABSTRACTION = dedent(\"\"\"\n",
    "You are a TRIZ Methodology Expert. Transform domain-specific problems into universal contradictions.\n",
    "\n",
    "Process:\n",
    "1. Read up on TRIZ - the contradiction matrix, and the inventive principles\n",
    "2. For each problem in the problem landscape:\n",
    "   - Abstract to universal parameters (what improves vs. what worsens)\n",
    "   - Express as 'When we improve X, Y worsens'\n",
    "   - Ensure parameters are domain-agnostic\n",
    "3. Analyze all the abstracted universal parameters, and identify all the core TRIZ contradictions present:\n",
    "   - Select the most fundamental tensions\n",
    "   - Map to TRIZ contradiction matrix\n",
    "   - Note applicable inventive principles\n",
    "\n",
    "Output as a JSON list with this exact format:\n",
    "\"\n",
    "[\n",
    "  {\n",
    "    \"contradiction\": \"Improving [parameter] vs. Worsening [parameter]\",\n",
    "    \"principles\": \"[1-3 relevant TRIZ principles with numbers]\",\n",
    "    \"potential\": \"High/Medium/Low\"\n",
    "  }\n",
    "]\n",
    "\"\n",
    "\n",
    "Focus on contradictions that, if resolved, would create breakthrough value.\n",
    "                     \n",
    "Problem landscape: {problem_landscape}\n",
    "\"\"\").strip()\n",
    "\n",
    "\n",
    "BASE_DOMAIN = dedent(\"\"\"\n",
    "        You are a Cross-Domain Search Specialist. Do the following:\n",
    "        - For each contradiction provided, identify 3 distinct source domains (fields or industries) where this contradiction has been successfully addressed.\n",
    "        - Experiment with different subsets of the list of contradictions, and see if you could identify 3 distinct source domains for each of these subsets identified as well. You should find at least 3 different subsets.\n",
    "        Note: The domains should have A CONCEPTUAL DISTANCE OF AT LEAST 3 DISTINCT HOPS FROM WHAT IMMEDIATELY COMES TO MIND. Be creative! It can be domains within spheres like natural, phsyical, social, artistic, or anything.\n",
    "        For each domain identified, briefly explain why it is relevant to the single contradiction or the subset of contradictions identified. Do not describe specific solutions just yet-only list the domains and your rationale.\n",
    "        Output:\n",
    "        A list for each contradiction and subset of contradictions identified, naming 3 relevant domains with a 2 sentence rationale for each.\n",
    "        Aim for a total of at least 20 relevant base domains. \n",
    "        Contradictions: {contradictions}\n",
    "\"\"\").strip()\n",
    "\n",
    "BASE_SOLUTIONS = dedent(\"\"\"\n",
    "        You are a Solution Pattern Extractor. You are provided with an input with 3 base domains identified per TRIZ (Theory of Inventive Problem Solving) contradiction or a set of contradictions, as well as the contradictions themselves.\n",
    "        For each of these identified base domains, identify one specific, well-documented solution pattern within the domain that effectively resolves the contradiction (or the set of contradictions).\n",
    "        For each solution pattern, return:\n",
    "        - Identify the base domain it's corresponding to\n",
    "        - Recall the contradiction or the set of contradictions that this base domain faces\n",
    "        - The name or label of the solution pattern for resolving these contradiction(s) in the base domain\n",
    "        - A detailed description of the core mechanism or principle involved and how it addressed the domain's contradiction(s)\n",
    "        - The context or situation in the domain where this pattern is applied\\n\"\n",
    "        Do not generalize or adapt the solution-simply describe how the contradiction is addressed within each source domain.\n",
    "        Output:\n",
    "        For each of the provided domain, list the base domain name, contradiction(s) faced, solution pattern name, the detailed description of the mechanism of the solution pattern, and the context in which it is used. Articulate the contradictions as problems and considerations faced, through framing them as a tension.\n",
    "\n",
    "        Input: {input}\n",
    "\"\"\").strip()\n",
    "\n",
    "ANALOGICAL_TRANSFER = dedent(\"\"\"\n",
    "You are a very innovative Analogical Transfer Specialist.\n",
    "You are provided with list the base domain name, tensions faced, solution pattern name, the detailed description of the mechanism of the solution pattern, and the context in which it is used.\n",
    "Your task is to propose how solution patterns used to resolve these tensions in various base domains might inspire solution framings for the original target domain.\n",
    "\n",
    "Input Overview:\n",
    "1. A list the base domains identified, the tensions these domains faced, the name of solution patterns that helped addressed these tensions in these base domains, the detailed description of the mechanism of the solution pattern, and the context in which it is used.\n",
    "2. The original target domain.\n",
    "\n",
    "Instructions:\n",
    "For each pair of base domain and the corresponding tensions identified, review the solution patterns that worked for the base domain. For each pattern:\n",
    "- Analyze the core mechanism or principle behind the solution.\n",
    "- Map and adapt this mechanism conceptually to the target domain, considering the specific context and needs of the target domain.\n",
    "- Clearly describe how this analogical transfer could frame a potential solution in the target domain.\n",
    "- Highlight any key adaptations, considerations, or limitations that would be relevant when applying this pattern to the target domain.\n",
    "\n",
    "Output as a JSON list of dictionaries with this exact format:\n",
    "\"\n",
    "[\n",
    "  {\n",
    "    \"transfer\": \"Brief title of the analogical transfer solution\",\n",
    "    \"description\": \"Comprehensive description including: original tension, source domain and solution pattern, detailed explanation of target domain application, and key adaptations/considerations\"\n",
    "  }\n",
    "]\n",
    "\"\n",
    "\n",
    "Here are the actual inputs:\n",
    "- A list the base domains identified, the tensions these domains faced, the name of solution patterns that helped addressed these tensions in these base domains, the detailed description of the mechanism of the solution pattern, and the context in which it is used.: {contradictions_solutions}\n",
    "- Original target domain: {target_domain}\n",
    "\"\"\").strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "8e573d7e-e43f-4c7d-8752-ae11a6198c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for openai API\n",
    "def _set_if_undefined(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"Please provide your {var}\")\n",
    "\n",
    "\n",
    "_set_if_undefined(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "8d2d7f38-3881-4d8d-80fc-542b5918d14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse_output\n",
    "import re\n",
    "\n",
    "def parse_solution(text):\n",
    "    # Remove Markdown formatting like \"**\" and \"\\n\"\n",
    "    clean_text = text.replace(\"**\", \"\").replace(\"\\\\n\", \"\\n\")\n",
    "\n",
    "    # Split into sections by horizontal rules (---)\n",
    "    sections = re.split(r'-{3,}', clean_text)\n",
    "\n",
    "    # Create a readable version of each section\n",
    "    readable_output = []\n",
    "    for section in sections:\n",
    "        section = section.strip()\n",
    "        if not section:\n",
    "            continue\n",
    "        # Optional: separate title and body if present\n",
    "        lines = section.split(\"\\n\")\n",
    "        if len(lines) > 1 and \":\" not in lines[0]:\n",
    "            # First line is likely a heading\n",
    "            heading = lines[0]\n",
    "            body = \"\\n\".join(lines[1:])\n",
    "            readable_output.append(f\"\\n=== {heading} ===\\n{body}\")\n",
    "        else:\n",
    "            readable_output.append(section)\n",
    "\n",
    "    return \"\\n\\n\".join(readable_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a188a9e3-c538-487f-9dde-8dfe295575db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReasoningState(TypedDict):\n",
    "    user_query: str\n",
    "    target_domain: str\n",
    "    problem_landscape: str\n",
    "    abstraction: str\n",
    "    abstraction_feedback: str\n",
    "    base_domain: str\n",
    "    base_solutions: str\n",
    "    analogical_transfer: str\n",
    "    transfer_feedback: str\n",
    "    solution: str\n",
    "    \n",
    "    # Add for CRIT agents:\n",
    "    retry_count_abstract: int              # For infinite loop prevention\n",
    "    retry_count_transfer: int              # For infinite loop prevention\n",
    "    validated_contradictions: List[str]    # For passing filtered contradictions\n",
    "    validated_analogical_transfers: List[str]  # For passing filtered transfers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "306439ab-4b94-4084-8a1c-8c53ecf1a0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can switch between different LLMs \n",
    "llms = init_chat_model(\"openai:gpt-4.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "54ea64a9-96a1-42ab-bb3d-1a1c1d14b1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st Agent: Identify Target Domain from User Input\n",
    "def target_domain_agent(state: ReasoningState):\n",
    "    u = state['user_query']\n",
    "\n",
    "    msg = llms.invoke(TARGET_DOMAIN.format(user_query=u))\n",
    "\n",
    "    return {\"target_domain\": msg.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d327c06e-9618-4463-b19e-2be74a21d14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2st Agent: Conduct comprehensive research into problem landscape for target domain \n",
    "# (i.e. What specific challenges exist in this domain?)\n",
    "def problem_landscape_agent(state: ReasoningState):\n",
    "    t = state['target_domain']\n",
    "\n",
    "    msg = llms.invoke(PROBLEM_LANDSCAPE.format(target_domain=t))\n",
    "\n",
    "    return {\"problem_landscape\": msg.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "26b96ec9-7bdb-4803-9bab-dcc82d6a4c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3rd Agent: Abstract Problems Identified into Generalized Principles and TRIZ Contradiction\n",
    "def abstraction_agent(state: ReasoningState):\n",
    "    p = state['problem_landscape']\n",
    "    feedback = state.get('abstraction_feedback', '')\n",
    "    \n",
    "    # If there's feedback, include it in the prompt\n",
    "    if feedback:\n",
    "        msg = llms.invoke(ABSTRACTION.format(\n",
    "            problem_landscape=p,\n",
    "            feedback=f\"\\nPrevious attempt feedback: {feedback}\\nPlease address these issues in your new abstraction.\"\n",
    "        ))\n",
    "    else:\n",
    "        msg = llms.invoke(ABSTRACTION.format(problem_landscape=p))\n",
    "    \n",
    "    return {\"abstraction\": msg.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "4b63464e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3.5th agent: CRIT Quality Control for TRIZ Abstraction\n",
    "def parse_abstraction_output(abstraction_text: str) -> list:\n",
    "    \"\"\"Parse JSON-formatted abstraction output.\"\"\"\n",
    "        # First try to parse as JSON if it's already in JSON format\n",
    "    try:\n",
    "        try:\n",
    "            return json.loads(abstraction_text)\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "\n",
    "        # If not JSON, parse as text\n",
    "        lines = [line.strip() for line in abstraction_text.split('\\n') if line.strip()]\n",
    "        contradictions = []\n",
    "        \n",
    "        current_contradiction = None\n",
    "        for line in lines:\n",
    "            # Clean the line of any markdown or extra whitespace\n",
    "            clean_line = line.strip('- ').strip()\n",
    "            \n",
    "            if 'Improving' in clean_line and 'vs.' in clean_line:\n",
    "                if current_contradiction:\n",
    "                    contradictions.append(current_contradiction)\n",
    "                current_contradiction = {\n",
    "                    \"contradiction\": clean_line,\n",
    "                    \"principles\": \"\",\n",
    "                    \"potential\": \"\"\n",
    "                }\n",
    "            elif 'TRIZ Principles:' in clean_line and current_contradiction:\n",
    "                current_contradiction[\"principles\"] = clean_line.replace('TRIZ Principles:', '').strip()\n",
    "            elif 'Innovation Potential:' in clean_line and current_contradiction:\n",
    "                current_contradiction[\"potential\"] = clean_line.replace('Innovation Potential:', '').strip()\n",
    "                contradictions.append(current_contradiction)\n",
    "                current_contradiction = None\n",
    "        \n",
    "        # Add the last contradiction if exists\n",
    "        if current_contradiction:\n",
    "            contradictions.append(current_contradiction)\n",
    "        \n",
    "        # Validate the parsed contradictions\n",
    "        validated_contradictions = []\n",
    "        for c in contradictions:\n",
    "            if \"contradiction\" in c and c[\"contradiction\"]:\n",
    "                validated_contradictions.append(c)\n",
    "        \n",
    "        return validated_contradictions\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing abstraction output: {str(e)}\")\n",
    "        print(f\"Raw text: {abstraction_text}\")\n",
    "        return []\n",
    "\n",
    "def CRIT_Control_Abstraction(target_domain: str, problem_landscape: str, triz_contradictions: str) -> dict:\n",
    "    \"\"\"Three-step CRIT validation workflow for TRIZ contradiction filtering.\"\"\"\n",
    "    \n",
    "    # Parse contradictions from structured output\n",
    "    parsed_contradictions = parse_abstraction_output(triz_contradictions)\n",
    "    \n",
    "    if not parsed_contradictions:\n",
    "        return {\n",
    "            \"score\": 0.0,\n",
    "            \"pass\": False,\n",
    "            \"reason\": \"No valid contradictions found in the input\",\n",
    "            \"validated_contradictions\": [],\n",
    "            \"filtered_contradictions\": []\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        # STEP 1: First CRIT Agent - Initial Filtering\n",
    "        step1_prompt = f\"\"\"\n",
    "# FIRST CRIT AGENT - Initial Socratic Filtering\n",
    "\n",
    "Apply Professor Edward Y. Chang's CRIT (Critical Reading Inquisitive Template) methods using socratic methods to filter TRIZ contradictions into good and bad sets.\n",
    "\n",
    "## CONTEXT\n",
    "Target Domain: {target_domain}\n",
    "Problem Landscape: {problem_landscape}\n",
    "Contradictions to Evaluate: {parsed_contradictions}\n",
    "\n",
    "## SOCRATIC FILTERING METHODS\n",
    "\n",
    "### 1. DEFINITION METHOD\n",
    "- Format: Does each contradiction follow \"Improving [X] vs. Worsening [Y]\"?\n",
    "- Parameters: Are they legitimate TRIZ parameters?\n",
    "- Abstraction: Are they at first-principles level?\n",
    "\n",
    "### 2. ELENCHUS METHOD (Cross-Examination)\n",
    "- Evidence: Can each contradiction trace back to specific problems?\n",
    "- Logic: Is the causal chain Problem → Root Cause → Contradiction valid?\n",
    "- Consistency: Any logical gaps in abstraction mapping?\n",
    "\n",
    "### 3. DIALECTIC METHOD (Counter-Arguments)\n",
    "- Alternatives: Are there better ways to express the same tension?\n",
    "- Comparison: Which contradictions capture essential tensions best?\n",
    "- Weakness: What are the strongest arguments against weak contradictions?\n",
    "\n",
    "## FILTERING DECISION\n",
    "Sort contradictions into two sets based on Socratic analysis:\n",
    "\n",
    "### OUTPUT FORMAT\n",
    "{{\n",
    "    \"good_contradictions\": [\n",
    "        {{\"contradiction\": str, \"principles\": str, \"potential\": str}}\n",
    "    ],\n",
    "    \"bad_contradictions\": [\n",
    "        {{\"contradiction\": str, \"principles\": str, \"potential\": str, \"reason\": str}}\n",
    "    ],\n",
    "    \"filtering_rationale\": str\n",
    "}}\n",
    "\n",
    "Apply rigorous Socratic methods. Include rationale for why contradictions are filtered out.\n",
    "        \"\"\"\n",
    "        \n",
    "        response_1 = llms.invoke(step1_prompt)\n",
    "        result_1 = parse_json_response(response_1.content)\n",
    "        \n",
    "        # STEP 2: Second CRIT Agent - Review and Revision\n",
    "        step2_prompt = f\"\"\"\n",
    "# SECOND CRIT AGENT - Socratic Review and Revision\n",
    "\n",
    "Cross-examine the first agent's filtering decisions using CRIT methods.\n",
    "\n",
    "## FIRST AGENT'S DECISIONS\n",
    "Good Contradictions: {result_1.get('good_contradictions', [])}\n",
    "Bad Contradictions: {result_1.get('bad_contradictions', [])}\n",
    "Filtering Rationale: {result_1.get('filtering_rationale', '')}\n",
    "\n",
    "## SOCRATIC CROSS-EXAMINATION\n",
    "\n",
    "### 1. ELENCHUS METHOD (Challenge Decisions)\n",
    "- Are any \"good\" contradictions actually flawed? Why?\n",
    "- Are any \"bad\" contradictions actually valuable? Why?\n",
    "- Do the filtering rationales hold up under scrutiny?\n",
    "\n",
    "### 2. DIALECTIC METHOD (Counter-Arguments)\n",
    "- Generate counter-arguments to the first agent's filtering rationale\n",
    "- Test alternative interpretations of contradiction quality\n",
    "- Challenge assumptions about what makes contradictions \"good\" or \"bad\"\n",
    "\n",
    "### 3. MAIEUTICS METHOD (Surface Hidden Assumptions)\n",
    "- What unstated assumptions influenced the first agent's decisions?\n",
    "- Are there valid contradictions being unfairly penalized?\n",
    "- What criteria were prioritized and why?\n",
    "\n",
    "## REVIEW DECISION\n",
    "Based on Socratic cross-examination:\n",
    "\n",
    "### OUTPUT FORMAT\n",
    "{{\n",
    "    \"revised_good_contradictions\": [\n",
    "        {{\"contradiction\": str, \"principles\": str, \"potential\": str}}\n",
    "    ],\n",
    "    \"revised_bad_contradictions\": [\n",
    "        {{\"contradiction\": str, \"principles\": str, \"potential\": str, \"reason\": str}}\n",
    "    ],\n",
    "    \"revision_rationale\": str,\n",
    "    \"revisions_made\": boolean\n",
    "}}\n",
    "\n",
    "If no revisions needed, return original sets with revisions_made: false.\n",
    "        \"\"\"\n",
    "        \n",
    "        response_2 = llms.invoke(step2_prompt)\n",
    "        result_2 = parse_json_response(response_2.content)\n",
    "        \n",
    "        # STEP 3: First CRIT Agent - Final Synthesis\n",
    "        step3_prompt = f\"\"\"\n",
    "# FIRST CRIT AGENT - Final Synthesis\n",
    "\n",
    "Synthesize the second agent's review with your own analysis to create final contradiction sets.\n",
    "\n",
    "## SECOND AGENT'S REVIEW\n",
    "Revised Good Contradictions: {result_2.get('revised_good_contradictions', [])}\n",
    "Revised Bad Contradictions: {result_2.get('revised_bad_contradictions', [])}\n",
    "Revision Rationale: {result_2.get('revision_rationale', '')}\n",
    "Revisions Made: {result_2.get('revisions_made', False)}\n",
    "\n",
    "## FINAL SOCRATIC SYNTHESIS\n",
    "\n",
    "THINK ABOUT THE FOLLOWING\n",
    "### 1. DIALECTIC INTEGRATION\n",
    "- Consider the second agent's counter-arguments seriously\n",
    "- Weigh competing interpretations of contradiction quality\n",
    "- Resolve any remaining tensions between different quality standards\n",
    "\n",
    "### 2. ELENCHUS VALIDATION\n",
    "- Cross-examine the final decisions one more time\n",
    "- Ensure logical consistency in the final sets\n",
    "- Verify that filtering decisions are well-justified\n",
    "\n",
    "### 3. PRACTICAL WISDOM\n",
    "- Balance perfectionism with progress\n",
    "- Ensure adequate contradictions for next pipeline stage\n",
    "- Prioritize contradictions that best serve the target domain\n",
    "\n",
    "## FINAL DECISION (MANDATORY)\n",
    "Provide definitive contradiction sets for pipeline progression:\n",
    "\n",
    "### OUTPUT FORMAT\n",
    "{{\n",
    "    \"final_good_contradictions\": [\n",
    "        {{\"contradiction\": str, \"principles\": str, \"potential\": str}}\n",
    "    ],\n",
    "    \"final_bad_contradictions\": [\n",
    "        {{\"contradiction\": str, \"principles\": str, \"potential\": str}}\n",
    "    ],\n",
    "    \"synthesis_complete\": boolean\n",
    "}}\n",
    "\n",
    "Must complete synthesis. Final decision required.\n",
    "        \"\"\"\n",
    "        \n",
    "        response_3 = llms.invoke(step3_prompt)\n",
    "        result_3 = parse_json_response(response_3.content)\n",
    "        \n",
    "        # Extract final results\n",
    "        final_good = result_3.get('final_good_contradictions', [])\n",
    "        final_bad = result_3.get('final_bad_contradictions', [])\n",
    "        \n",
    "        # Convert to simple list format for pipeline compatibility\n",
    "        validated_contradictions = [item['contradiction'] for item in final_good if 'contradiction' in item]\n",
    "        filtered_contradictions = [item['contradiction'] for item in final_bad if 'contradiction' in item]\n",
    "        \n",
    "        # Calculate final pass/fail\n",
    "        if len(validated_contradictions) >= 2:  # Minimum viable set\n",
    "            final_score = 8.0\n",
    "            final_pass = True\n",
    "            final_reason = \"\"\n",
    "        else:\n",
    "            final_score = 6.0\n",
    "            final_pass = False\n",
    "            final_reason = f\"Insufficient contradictions after 3-step CRIT filtering: {len(validated_contradictions)}\"\n",
    "        \n",
    "        return {\n",
    "            \"score\": final_score,\n",
    "            \"pass\": final_pass,\n",
    "            \"reason\": final_reason,\n",
    "            \"validated_contradictions\": validated_contradictions,\n",
    "            \"filtered_contradictions\": filtered_contradictions,\n",
    "            \"three_step_summary\": {\n",
    "                \"step1_good_count\": len(result_1.get('good_contradictions', [])),\n",
    "                \"step1_bad_count\": len(result_1.get('bad_contradictions', [])),\n",
    "                \"step2_revisions_made\": result_2.get('revisions_made', False),\n",
    "                \"step3_final_good_count\": len(validated_contradictions),\n",
    "                \"step3_final_bad_count\": len(filtered_contradictions)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"score\": 0.0,\n",
    "            \"pass\": False,\n",
    "            \"reason\": f\"Three-step CRIT workflow failed: {str(e)}\",\n",
    "            \"validated_contradictions\": [],\n",
    "            \"filtered_contradictions\": []\n",
    "        }\n",
    "\n",
    "def parse_json_response(response_text: str) -> dict:\n",
    "    \"\"\"Enhanced JSON parser for three-step CRIT responses.\"\"\"\n",
    "    try:\n",
    "        # Find JSON in response\n",
    "        start_idx = response_text.find('{')\n",
    "        end_idx = response_text.rfind('}') + 1\n",
    "        if start_idx >= 0 and end_idx > start_idx:\n",
    "            json_str = response_text[start_idx:end_idx]\n",
    "            return json.loads(json_str)\n",
    "        return {\"error\": \"No valid JSON found\"}\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"JSON parsing failed: {str(e)}\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "a230bf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3.5th Agent Logic Flow: if quality control passes, continue to next agent. Else, loop back to retry TRIZ abstraction (taking rationale for why it failed as additional input)\n",
    "def should_continue_to_base(state: ReasoningState) -> str:\n",
    "    \"\"\"\n",
    "    Revised control flow for abstraction. Determines whether to continue to the \n",
    "    base domain or retry abstraction, with a retry limit to prevent infinite loops.\n",
    "    \"\"\"\n",
    "    # Check retry limit FIRST to prevent getting stuck\n",
    "    retry_count = state.get('retry_count_abstract', 0)\n",
    "    if retry_count >= 2:  # Max 3 attempts (0, 1, 2)\n",
    "        print(f\"Max retries ({retry_count}) reached for abstraction. Forcing progression to base domain agent.\")\n",
    "        # To prevent downstream errors, pass the last-known validated list or an empty one\n",
    "        if 'validated_contradictions' not in state:\n",
    "             state['validated_contradictions'] = []\n",
    "        return \"base\"\n",
    "\n",
    "    # If limit is not reached, run the CRIT validation\n",
    "    crit_result = CRIT_Control_Abstraction(\n",
    "        state['target_domain'],\n",
    "        state['problem_landscape'],\n",
    "        state['abstraction']\n",
    "    )\n",
    "    \n",
    "    passed = crit_result.get('pass', False)\n",
    "    \n",
    "    if passed:\n",
    "        # IMPORTANT: Update state with the validated contradictions for the next agent\n",
    "        state['validated_contradictions'] = crit_result.get('validated_contradictions', [])\n",
    "        return \"base\"\n",
    "    else:\n",
    "        # If CRIT fails, increment retry counter and add feedback for the next loop\n",
    "        state['retry_count_abstract'] = retry_count + 1\n",
    "        state['abstraction_feedback'] = crit_result.get('reason', '')\n",
    "        print(f\"Abstraction CRIT failed (attempt {state['retry_count_abstract']}): {crit_result.get('reason', '')}\")\n",
    "        return \"abstract\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "e95a5d2f-1773-4339-ba89-995d185885c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4th Agent: Search for Appropriate Base Domains\n",
    "def base_domain_agent(state: ReasoningState):\n",
    "    a = state['abstraction']\n",
    "\n",
    "    msg = llms.invoke(BASE_DOMAIN.format(contradictions=a))\n",
    "\n",
    "    return {\"base_domain\": msg.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f7db11f3-e388-48b4-8661-cde636b1ba50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5th Agent: Identify Solution in Base Domain\n",
    "def base_solution_agent(state: ReasoningState):\n",
    "    b = state['base_domain']\n",
    "\n",
    "    msg = llms.invoke(BASE_SOLUTIONS.format(input=b))\n",
    "\n",
    "    return {\"base_solutions\": msg.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "5dc1b395-b154-493e-9c57-7e5a6926cf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6th Agent: Base Domain Solution Informing Target Domain Solution\n",
    "def analogical_transfer_agent(state: ReasoningState):\n",
    "    if \"base_solutions\" not in state:\n",
    "        raise ValueError(\"Missing 'base_solutions' key. Check if previous node returned it.\")\n",
    "    b = state['base_solutions']\n",
    "    t = state['target_domain']\n",
    "    feedback = state.get('transfer_feedback', '')\n",
    "    \n",
    "    # If there's feedback, include it in the prompt\n",
    "    if feedback:\n",
    "        msg = llms.invoke(ANALOGICAL_TRANSFER.format(\n",
    "            contradictions_solutions=b,\n",
    "            target_domain=t,\n",
    "            feedback=f\"\\nPrevious attempt feedback: {feedback}\\nPlease address these issues in your new analogical transfer.\"\n",
    "        ))\n",
    "    else:\n",
    "        msg = llms.invoke(ANALOGICAL_TRANSFER.format(\n",
    "            contradictions_solutions=b,\n",
    "            target_domain=t\n",
    "        ))\n",
    "    \n",
    "    return {\"analogical_transfer\": msg.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d8dc7e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.5th Agent: CRIT Quality Control for Analogical Reasoning \n",
    "## Helper function to parse previous output\n",
    "def parse_analogical_transfer_output(transfer_text: str) -> list:\n",
    "    \"\"\"Parse JSON-formatted analogical transfer output.\"\"\"\n",
    "    try:\n",
    "        # Extract JSON from response\n",
    "        start_idx = transfer_text.find('[')\n",
    "        end_idx = transfer_text.rfind(']') + 1\n",
    "        if start_idx >= 0 and end_idx > start_idx:\n",
    "            json_str = transfer_text[start_idx:end_idx]\n",
    "            return json.loads(json_str)\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Transfer JSON parsing failed: {e}\")\n",
    "        return []\n",
    "\n",
    "## CRIT control agent - debate 3 steps to ensure quality\n",
    "def CRIT_Control_Analogical_Transfer(target_domain: str, original_contradictions: str, base_solutions: str, analogical_transfers: str) -> dict:\n",
    "    \"\"\"Three-step CRIT validation workflow for analogical transfer filtering.\"\"\"\n",
    "    \n",
    "    # Parse transfers from output\n",
    "    parsed_transfers = parse_analogical_transfer_output(analogical_transfers)\n",
    "    \n",
    "    if not parsed_transfers:\n",
    "        return {\n",
    "            \"score\": 0.0,\n",
    "            \"pass\": False,\n",
    "            \"reason\": \"No valid analogical transfers found in the input\",\n",
    "            \"validated_transfers\": [],\n",
    "            \"filtered_transfers\": []\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        # STEP 1: First CRIT Agent - Initial Transfer Filtering\n",
    "        step1_prompt = f\"\"\"\n",
    "# FIRST CRIT AGENT - Initial Socratic Transfer Filtering\n",
    "\n",
    "Apply Professor Edward Y. Chang's CRIT (Critical Reading Inquisitive Template) methods using socratic reasoning to filter analogical transfers into good and bad sets.\n",
    "\n",
    "## CONTEXT\n",
    "Target Domain: {target_domain}\n",
    "Original Contradictions: {original_contradictions}\n",
    "Base Solutions: {base_solutions}\n",
    "Transfers to Evaluate: {parsed_transfers}\n",
    "\n",
    "## SOCRATIC FILTERING METHODS\n",
    "\n",
    "### 1. DEFINITION METHOD\n",
    "- Mechanism: Does each transfer preserve core solution patterns from base domains?\n",
    "- Parameters: Are key elements from base domain mapped correctly to target domain?\n",
    "- Abstraction: Is transfer at appropriate level (not too superficial, not too abstract)?\n",
    "\n",
    "### 2. ELENCHUS METHOD (Cross-Examination)\n",
    "- Evidence: Can each transfer trace back clearly from base solution to target application?\n",
    "- Logic: Is the causal chain Base Mechanism → Transfer Logic → Target Solution valid?\n",
    "- Consistency: Do transfers actually address the original contradictions identified?\n",
    "\n",
    "### 3. DIALECTIC METHOD (Counter-Arguments)\n",
    "- Alternatives: Are there better ways to transfer the same base mechanisms?\n",
    "- Comparison: Which transfers best preserve essential solution patterns?\n",
    "- Weakness: What are strongest arguments against weak or superficial transfers?\n",
    "\n",
    "## FILTERING DECISION\n",
    "Sort transfers into two sets based on Socratic analysis:\n",
    "\n",
    "### OUTPUT FORMAT\n",
    "{{\n",
    "    \"good_transfers\": [\n",
    "        {{\"transfer\": str, \"description\": str}}\n",
    "    ],\n",
    "    \"bad_transfers\": [\n",
    "        {{\"transfer\": str, \"description\": str, \"reason\": str}}\n",
    "    ],\n",
    "    \"filtering_rationale\": str\n",
    "}}\n",
    "\n",
    "Apply Professor Edward Y. Chang's CRIT (Critical Reading Inquisitive Template) methods to use rigorous Socratic methods.  Focus on mechanism fidelity and target domain feasibility.\n",
    "        \"\"\"\n",
    "        \n",
    "        response_1 = llms.invoke(step1_prompt)\n",
    "        result_1 = parse_json_response(response_1.content)\n",
    "        \n",
    "        # STEP 2: Second CRIT Agent - Review and Revision\n",
    "        step2_prompt = f\"\"\"\n",
    "# SECOND CRIT AGENT - Socratic Transfer Review and Revision\n",
    "\n",
    "Cross-examine the first agent's transfer filtering decisions using CRIT methods.\n",
    "\n",
    "## FIRST AGENT'S DECISIONS\n",
    "Good Transfers: {result_1.get('good_transfers', [])}\n",
    "Bad Transfers: {result_1.get('bad_transfers', [])}\n",
    "Filtering Rationale: {result_1.get('filtering_rationale', '')}\n",
    "\n",
    "## SOCRATIC CROSS-EXAMINATION\n",
    "\n",
    "### 1. ELENCHUS METHOD (Challenge Transfer Decisions)\n",
    "- Are any \"good\" transfers actually flawed in mechanism preservation?\n",
    "- Are any \"bad\" transfers actually valuable but underestimated?\n",
    "- Do the filtering rationales properly assess implementation feasibility?\n",
    "\n",
    "### 2. DIALECTIC METHOD (Counter-Arguments to First Agent)\n",
    "- Generate counter-arguments to the first agent's transfer quality assessments\n",
    "- Test alternative interpretations of what makes effective analogical transfer\n",
    "- Challenge assumptions about mechanism preservation vs. creative adaptation\n",
    "\n",
    "### 3. MAIEUTICS METHOD (Surface Hidden Transfer Assumptions)\n",
    "- What unstated assumptions influenced transfer quality judgments?\n",
    "- Are there valid transfers being unfairly penalized for creativity?\n",
    "- What criteria should prioritize: fidelity to base vs. target domain fit?\n",
    "\n",
    "## REVIEW DECISION\n",
    "Based on Socratic cross-examination of transfer quality:\n",
    "\n",
    "### OUTPUT FORMAT\n",
    "{{\n",
    "    \"revised_good_transfers\": [\n",
    "        {{\"transfer\": str, \"description\": str}}\n",
    "    ],\n",
    "    \"revised_bad_transfers\": [\n",
    "        {{\"transfer\": str, \"description\": str, \"reason\": str}}\n",
    "    ],\n",
    "    \"revision_rationale\": str,\n",
    "    \"revisions_made\": boolean\n",
    "}}\n",
    "\n",
    "If no revisions needed, return original sets with revisions_made: false.\n",
    "        \"\"\"\n",
    "        \n",
    "        response_2 = llms.invoke(step2_prompt)\n",
    "        result_2 = parse_json_response(response_2.content)\n",
    "        \n",
    "        # STEP 3: First CRIT Agent - Final Transfer Synthesis\n",
    "        step3_prompt = f\"\"\"\n",
    "# FIRST CRIT AGENT - Final Transfer Synthesis\n",
    "\n",
    "Synthesize the second agent's review with your own analysis to create final transfer sets.\n",
    "\n",
    "## SECOND AGENT'S REVIEW\n",
    "Revised Good Transfers: {result_2.get('revised_good_transfers', [])}\n",
    "Revised Bad Transfers: {result_2.get('revised_bad_transfers', [])}\n",
    "Revision Rationale: {result_2.get('revision_rationale', '')}\n",
    "Revisions Made: {result_2.get('revisions_made', False)}\n",
    "\n",
    "## FINAL SOCRATIC SYNTHESIS\n",
    "\n",
    "### 1. DIALECTIC INTEGRATION\n",
    "- Consider the second agent's counter-arguments about transfer quality seriously\n",
    "- Weigh competing interpretations of effective analogical transfer\n",
    "- Resolve tensions between mechanism fidelity and target domain adaptation\n",
    "\n",
    "### 2. ELENCHUS VALIDATION\n",
    "- Cross-examine the final transfer decisions one more time\n",
    "- Ensure logical consistency between transfers and original contradictions\n",
    "- Verify that transfer mechanisms are implementable in target domain\n",
    "\n",
    "### 3. METHOD OF MAIEUTICS (Midwife Method)\n",
    "- Draw out the inherent wisdom about transfer quality revealed through agent dialogue\n",
    "- Surface the essential understanding about target domain needs that has emerged\n",
    "- Help bring forth the knowledge about which transfers best serve analogical reasoning\n",
    "\n",
    "## FINAL DECISION (MANDATORY)\n",
    "Provide definitive transfer sets for synthesis agent:\n",
    "\n",
    "### OUTPUT FORMAT\n",
    "{{\n",
    "    \"final_good_transfers\": [\n",
    "        {{\"transfer\": str, \"description\": str}}\n",
    "    ],\n",
    "    \"final_bad_transfers\": [\n",
    "        {{\"transfer\": str, \"description\": str}}\n",
    "    ],\n",
    "    \"synthesis_complete\": boolean\n",
    "}}\n",
    "\n",
    "Must complete synthesis. Final decision required for pipeline progression.\n",
    "        \"\"\"\n",
    "        \n",
    "        response_3 = llms.invoke(step3_prompt)\n",
    "        result_3 = parse_json_response(response_3.content)\n",
    "        \n",
    "        # Extract final results\n",
    "        final_good = result_3.get('final_good_transfers', [])\n",
    "        final_bad = result_3.get('final_bad_transfers', [])\n",
    "        \n",
    "        # Convert to simple list format for pipeline compatibility\n",
    "        validated_transfers = [item['description'] for item in final_good if 'description' in item]\n",
    "        filtered_transfers = [item['description'] for item in final_bad if 'description' in item]\n",
    "        \n",
    "        # Calculate final pass/fail\n",
    "        if len(validated_transfers) >= 2:  # Minimum viable transfer set\n",
    "            final_score = 8.0\n",
    "            final_pass = True\n",
    "            final_reason = \"\"\n",
    "        else:\n",
    "            final_score = 6.0\n",
    "            final_pass = False\n",
    "            final_reason = f\"Insufficient quality transfers after 3-step CRIT filtering: {len(validated_transfers)}\"\n",
    "        \n",
    "        return {\n",
    "            \"score\": final_score,\n",
    "            \"pass\": final_pass,\n",
    "            \"reason\": final_reason,\n",
    "            \"validated_transfers\": validated_transfers,\n",
    "            \"filtered_transfers\": filtered_transfers,\n",
    "            \"three_step_summary\": {\n",
    "                \"step1_good_count\": len(result_1.get('good_transfers', [])),\n",
    "                \"step1_bad_count\": len(result_1.get('bad_transfers', [])),\n",
    "                \"step2_revisions_made\": result_2.get('revisions_made', False),\n",
    "                \"step3_final_good_count\": len(validated_transfers),\n",
    "                \"step3_final_bad_count\": len(filtered_transfers)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"score\": 0.0,\n",
    "            \"pass\": False,\n",
    "            \"reason\": f\"Three-step transfer CRIT workflow failed: {str(e)}\",\n",
    "            \"validated_transfers\": [],\n",
    "            \"filtered_transfers\": []\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "faecca01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Routing function for workflow integration\n",
    "def should_continue_to_synthesis(state: ReasoningState) -> str:\n",
    "    \"\"\"\n",
    "    Control flow for transfer validation. Determines whether to proceed to synthesis\n",
    "    or retry, with a retry limit to prevent infinite loops.\n",
    "    \"\"\"\n",
    "    # Check retry limit FIRST to break potential loops\n",
    "    retry_count = state.get('retry_count_transfer', 0)\n",
    "    if retry_count >= 2:  # Allow up to 2 retries (3 total attempts)\n",
    "        print(f\"Max retries ({retry_count}) reached for analogical transfer. Forcing progression to synthesis.\")\n",
    "        # To avoid errors, pass the last known (unfiltered) transfers to the synthesis agent\n",
    "        state['validated_analogical_transfers'] = parse_analogical_transfer_output(state['analogical_transfer'])\n",
    "        return \"synthesis\"\n",
    "\n",
    "    # If within limits, run CRIT validation\n",
    "    crit_result = CRIT_Control_Analogical_Transfer(\n",
    "        state['target_domain'],\n",
    "        state.get('validated_contradictions', state['abstraction']), # Use validated contradictions if available\n",
    "        state['base_solutions'],\n",
    "        state['analogical_transfer']\n",
    "    )\n",
    "    \n",
    "    passed = crit_result.get('pass', False)\n",
    "    \n",
    "    if passed:\n",
    "        # On success, update the state with the validated transfers for the synthesis agent\n",
    "        state['validated_analogical_transfers'] = crit_result.get('validated_transfers', [])\n",
    "        return \"synthesis\"\n",
    "    else:\n",
    "        # On failure, increment the retry counter and add feedback for the next loop\n",
    "        state['retry_count_transfer'] = retry_count + 1\n",
    "        state['transfer_feedback'] = crit_result.get('reason', '')\n",
    "        print(f\"Transfer CRIT failed (attempt {state['retry_count_transfer']}): {crit_result.get('reason', '')}\")\n",
    "        return \"analogical_transfer\"\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "e3626041-fcd5-4bf3-a32b-f164352e788f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7th Agent: Summarize everything and respond to the question\n",
    "def synthesis_agent(state: ReasoningState):\n",
    "    msg = llms.invoke(\n",
    "        f\"Evaluate the proposed analogical solutions. Find the best ones that balances practicality with innovation. Then, provide a detailed, well-structured response that addresses all aspects of the query.\\n\\n\"\n",
    "        f\"Problem: {state['user_query']}\\n\"\n",
    "        f\"Analogical Solutions: {state['analogical_transfer']}\"\n",
    "        f\"In your output, remember to abstract away the analogy itself such that it is focused on responding to the user input.\"\n",
    "        f\"Also, check if the users are requesting a specific number of possible solutions. Make sure to answer the user's query in full and provide what is requested.\"\n",
    "    )\n",
    "    return {\"solution\": msg.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "0673990c",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(ReasoningState)\n",
    "\n",
    "workflow.add_node(\"target\", target_domain_agent)\n",
    "workflow.add_node(\"landscape\", problem_landscape_agent)\n",
    "#workflow.add_node(\"abstract\", abstraction_agent)\n",
    "\n",
    "# Define edges\n",
    "workflow.add_edge(\"target\", \"landscape\")\n",
    "#workflow.add_edge(\"landscape\", \"abstract\")\n",
    "workflow.set_entry_point(\"target\")\n",
    "\n",
    "workflow.set_finish_point(\"landscape\")\n",
    "\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "5581d5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_q = \"How can we design algorithms that recognize, simulate, or respond to human emotions in a believable, ethical, and useful way?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "5dea0a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_state = {\"user_query\": test_q}\n",
    "final_state = graph.invoke(input_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "b2e0fafb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_query': 'How can we design algorithms that recognize, simulate, or respond to human emotions in a believable, ethical, and useful way?',\n",
       " 'target_domain': 'Affective Computing Systems',\n",
       " 'problem_landscape': '[\\n  {\\n    \"Problem\": \"Emotion Recognition Accuracy\",\\n    \"Description\": \"Affective computing systems often fail to accurately recognize human emotions, particularly in diverse populations. This affects the efficacy and reliability of applications relying on emotion detection.\",\\n    \"Context\": \"Occurs in applications such as education technology, customer service bots, mental health assessment tools, and automotive safety systems.\",\\n    \"Stakeholders\": [\"End users (e.g., students, customers, patients, drivers)\", \"System developers\", \"Researchers\", \"Healthcare professionals\"],\\n    \"Root Causes\": [\\n      \"Limited and biased training datasets\",\\n      \"Variability in individual emotional expression\",\\n      \"Sensor/device inaccuracies (e.g., poor camera or microphone quality)\"\\n    ],\\n    \"Impact\": \"Incorrect emotion recognition can lead to inappropriate system responses, user frustration, safety risks, and reputational damage.\",\\n    \"Current Approaches\": \"Use of machine learning models trained on labeled datasets, multi-modal data integration (e.g., facial, voice, physiological signals), ongoing data augmentation efforts.\",\\n    \"Limitations\": \"Models exhibit bias, poor generalizability across demographics, and lack nuanced understanding of complex emotions or cultural differences.\",\\n    \"Success Metrics\": \"Emotion recognition accuracy rate, false positive/negative rates, demographic parity in emotion identification.\",\\n    \"Interconnections\": \"Closely linked to issues of dataset diversity and privacy; inaccurate detection can exacerbate privacy or ethical concerns.\"\\n  },\\n  {\\n    \"Problem\": \"Data Privacy and Consent\",\\n    \"Description\": \"Affective computing systems often require the collection and processing of sensitive biometric and behavioral data, raising significant privacy concerns.\",\\n    \"Context\": \"Particularly acute when systems are deployed in public spaces or with vulnerable groups (e.g., minors, patients).\",\\n    \"Stakeholders\": [\"End users\", \"Organizational IT departments\", \"Legal/compliance teams\", \"Regulators\"],\\n    \"Root Causes\": [\\n      \"Need for granular emotional/physiological data\",\\n      \"Ambiguous or inadequate consent mechanisms\",\\n      \"Inconsistent data protection regulations\"\\n    ],\\n    \"Impact\": \"Potential for misuse or unauthorized access to personal emotional data, legal liabilities, erosion of user trust.\",\\n    \"Current Approaches\": \"Use of anonymization, data minimization strategies, traditional consent forms, compliance frameworks (e.g., GDPR, HIPAA).\",\\n    \"Limitations\": \"Hard to truly anonymize emotion data, difficulty ensuring informed consent, regulations lag technological development.\",\\n    \"Success Metrics\": \"Number of unauthorized data disclosures, user consent opt-in/out rates, frequency of privacy complaints.\",\\n    \"Interconnections\": \"Tied to trust and adoption of affective computing, impacts feasibility of large-scale deployments.\"\\n  },\\n  {\\n    \"Problem\": \"Real-Time Processing Constraints\",\\n    \"Description\": \"Affective systems often need to process sensor data and respond to user emotions instantly, but real-time performance is frequently challenged by computational and network limitations.\",\\n    \"Context\": \"Critical in safety or high-stakes domains such as automotive (driver monitoring) or mental health emergency support.\",\\n    \"Stakeholders\": [\"System developers\", \"End users\", \"Device manufacturers\", \"Service providers\"],\\n    \"Root Causes\": [\\n      \"Limited on-device computation resources\",\\n      \"Latency in data transmission (especially for cloud-based processing)\",\\n      \"Complex, resource-intensive algorithms\"\\n    ],\\n    \"Impact\": \"Lag or inaccuracy in emotional response can reduce user trust and system utility or impair safety.\",\\n    \"Current Approaches\": \"Edge computing, algorithm optimization, model compression, hybrid cloud-edge designs.\",\\n    \"Limitations\": \"Trade-offs between speed and accuracy, limited by current hardware capabilities, higher costs for advanced devices.\",\\n    \"Success Metrics\": \"Response latency, real-time throughput rates, user-reported satisfaction with system responsiveness.\",\\n    \"Interconnections\": \"Affects and is affected by accuracy (slower or compressed models may be less accurate), linked to cost and accessibility barriers.\"\\n  },\\n  {\\n    \"Problem\": \"Cultural and Demographic Bias in Emotional Interpretation\",\\n    \"Description\": \"Affective computing systems may misinterpret emotions due to lack of cultural context or insufficient demographic representation in models.\",\\n    \"Context\": \"Global applications, such as cross-cultural communication tools and international educational technology platforms.\",\\n    \"Stakeholders\": [\"End users from minority cultures/demographics\", \"Social scientists\", \"Developers\", \"Regulators\"],\\n    \"Root Causes\": [\\n      \"Training data dominated by certain cultural groups\",\\n      \"Assumption of universality in emotional expression\",\\n      \"Inadequate testing across populations\"\\n    ],\\n    \"Impact\": \"Alienation or discrimination against marginalized users, reduced system adoption, perpetuation of stereotypes.\",\\n    \"Current Approaches\": \"Limited efforts to diversify datasets, manual curation, collaborations with social scientists.\",\\n    \"Limitations\": \"Resource-intensive, scalability challenges, imperfect coverage of global diversity.\",\\n    \"Success Metrics\": \"Reduction in error rate across cultural subgroups, user feedback on perceived inclusivity, cross-cultural validation studies.\",\\n    \"Interconnections\": \"Exacerbates the emotion recognition accuracy problem and can increase privacy risks for marginalized groups.\"\\n  },\\n  {\\n    \"Problem\": \"Explainability and Transparency\",\\n    \"Description\": \"Users and developers often lack insight into how affective computing systems arrive at emotion-related decisions, hindering trust and accountability.\",\\n    \"Context\": \"Healthcare diagnoses, hiring systems, classroom monitoring, or any high-stakes emotional assessment applications.\",\\n    \"Stakeholders\": [\"End users\", \"Developers\", \"Regulators\", \"Auditors\", \"Ethicists\"],\\n    \"Root Causes\": [\\n      \"Complexity of deep learning models\",\\n      \"Proprietary algorithms\",\\n      \"Lack of standardized interpretability frameworks\"\\n    ],\\n    \"Impact\": \"System decisions can feel arbitrary or opaque, making it hard to audit, troubleshoot, or defend against discrimination claims.\",\\n    \"Current Approaches\": \"Post-hoc explainability methods, user-facing dashboards, regulatory guidelines (e.g., AI Act).\",\\n    \"Limitations\": \"Many current methods are superficial, technical, or difficult for laypersons to understand.\",\\n    \"Success Metrics\": \"User rating of algorithm transparency, proportion of decisions with accessible rationale, regulatory compliance rates.\",\\n    \"Interconnections\": \"Strongly connected to privacy, fairness, and bias challenges.\"\\n  },\\n  {\\n    \"Problem\": \"Robustness to Environmental Variability\",\\n    \"Description\": \"Affective computing performance degrades under changing lighting, noise, or sensor conditions, reducing reliability in real-world use.\",\\n    \"Context\": \"Mobile and wearable devices, public kiosks, automotive and surveillance systems.\",\\n    \"Stakeholders\": [\"Device users\", \"Manufacturers\", \"Developers\"],\\n    \"Root Causes\": [\\n      \"Variation in environmental conditions not covered in training\",\\n      \"Low-quality sensors\",\\n      \"Algorithmic limitations in handling noise\"\\n    ],\\n    \"Impact\": \"System failures or false feedback in uncontrolled environments, compromising practical adoption.\",\\n    \"Current Approaches\": \"Data augmentation, sensor calibration, environmental preprocessing algorithms.\",\\n    \"Limitations\": \"Incomplete coverage of real-world variability, increased computational/cost overhead.\",\\n    \"Success Metrics\": \"Performance degradation metrics under stress tests, user satisfaction in field deployment.\",\\n    \"Interconnections\": \"Amplifies accuracy and real-time processing issues, impacts perception of system trustworthiness.\"\\n  }\\n]'}"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "b2a589fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'\\n    \"contradiction\"'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[171]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m msg = llms.invoke(\u001b[43mABSTRACTION\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproblem_landscape\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_state\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mKeyError\u001b[39m: '\\n    \"contradiction\"'"
     ]
    }
   ],
   "source": [
    "msg = llms.invoke(ABSTRACTION.format(problem_landscape = final_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "e94ddc18",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'\\n    \"contradiction\"'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[167]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m input_state = {\u001b[33m\"\u001b[39m\u001b[33muser_query\u001b[39m\u001b[33m\"\u001b[39m: test_q}\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m final_state = \u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mtestX2.txt\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      4\u001b[39m      \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m final_state:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/Analogical-LLM/analogical-lc-env2/lib/python3.12/site-packages/langgraph/pregel/__init__.py:2719\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, **kwargs)\u001b[39m\n\u001b[32m   2716\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], Any]] = []\n\u001b[32m   2717\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m2719\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2720\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2721\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2722\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2723\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2724\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2725\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2726\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_during\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheckpoint_during\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2727\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2728\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2729\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2730\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   2731\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2732\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2733\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mints\u001b[49m\u001b[43m \u001b[49m\u001b[43m:=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mINTERRUPT\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m   2734\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/Analogical-LLM/analogical-lc-env2/lib/python3.12/site-packages/langgraph/pregel/__init__.py:2436\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[39m\n\u001b[32m   2434\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2435\u001b[39m             loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2436\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2437\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2438\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2439\u001b[39m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2440\u001b[39m \u001b[43m            \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2441\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2442\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2443\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2444\u001b[39m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[142]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mabstraction_agent\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m      8\u001b[39m     msg = llms.invoke(ABSTRACTION.format(\n\u001b[32m      9\u001b[39m         problem_landscape=p,\n\u001b[32m     10\u001b[39m         feedback=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mPrevious attempt feedback: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeedback\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mPlease address these issues in your new abstraction.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     11\u001b[39m     ))\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     msg = llms.invoke(\u001b[43mABSTRACTION\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproblem_landscape\u001b[49m\u001b[43m=\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mabstraction\u001b[39m\u001b[33m\"\u001b[39m: msg.content}\n",
      "\u001b[31mKeyError\u001b[39m: '\\n    \"contradiction\"'",
      "During task with name 'abstract' and id '7d62a883-3b52-57fd-85fc-5216554f92af'"
     ]
    }
   ],
   "source": [
    "input_state = {\"user_query\": test_q}\n",
    "final_state = graph.invoke(input_state)\n",
    "with open(\"testX2.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "     for key in final_state:\n",
    "         raw_output = str(final_state[key])  # Ensure it's a string\n",
    "         final_output = parse_solution(raw_output)\n",
    "         f.write(f\"\\n### {key} ###\\n\")\n",
    "         f.write(final_output)\n",
    "         f.write(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c9e1f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "68994430-5388-4c63-84be-919d5978d5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the workflow \n",
    "workflow = StateGraph(ReasoningState)\n",
    "\n",
    "workflow.add_node(\"target\", target_domain_agent)\n",
    "workflow.add_node(\"landscape\", problem_landscape_agent)\n",
    "workflow.add_node(\"abstract\", abstraction_agent)\n",
    "workflow.add_node(\"base\", base_domain_agent)\n",
    "workflow.add_node(\"base_soln\", base_solution_agent)\n",
    "workflow.add_node(\"analogy\", analogical_transfer_agent)\n",
    "workflow.add_node(\"synthesis\", synthesis_agent)\n",
    "\n",
    "workflow.set_entry_point(\"target\")\n",
    "\n",
    "# Define edges\n",
    "workflow.add_edge(\"target\", \"landscape\")\n",
    "workflow.add_edge(\"landscape\", \"abstract\")\n",
    "# workflow.add_edge(\"abstract\", \"base\")\n",
    "\n",
    "# Define conditional edges - Use CRIT TO control quality for Abstraction TRIZ\n",
    "workflow.add_conditional_edges(\n",
    "    \"abstract\",\n",
    "    should_continue_to_base,\n",
    "    {\n",
    "        \"base\": \"base\",\n",
    "        \"abstract\": \"abstract\" \n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"base\", \"base_soln\")\n",
    "workflow.add_edge(\"base_soln\", \"analogy\")\n",
    "#workflow.add_edge(\"analogy\", \"synthesis\")\n",
    "\n",
    "# Define conditional edges - Use CRIT TO control quality for Analogical Transfer\n",
    "workflow.add_conditional_edges(\n",
    "    \"analogy\",\n",
    "    should_continue_to_synthesis,\n",
    "    {\n",
    "        \"synthesis\": \"synthesis\",\n",
    "        \"analogy\": \"analogy\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.set_finish_point(\"synthesis\")\n",
    "\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "b178c9ea-3768-44c1-8461-01c0e93ef1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogical_output(user_input: str) -> str:\n",
    "    input_state = {\"user_query\": user_input}\n",
    "\n",
    "    final_state = graph.invoke(input_state)\n",
    "\n",
    "    for key in final_state:\n",
    "        raw_output = str(final_state[key])  # Ensure it's a string\n",
    "        final_output = parse_solution(raw_output)\n",
    "\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea24580",
   "metadata": {},
   "source": [
    "### Initialize Different Functions for Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "c1ed3fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Output\n",
    "def basic_output(user_input: str):\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    {user_input}\n",
    "    \"\"\"\n",
    "\n",
    "    response = llms.invoke(prompt) \n",
    "\n",
    "    # Parse and format the response\n",
    "    formatted_response = parse_solution(response.content)\n",
    "\n",
    "    return(formatted_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "174da814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Output with COT\n",
    "## TO EDIT WITH FULL PROMPT?\n",
    "def basic_output_COT(user_input: str):\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    {user_input}\n",
    "    Think step by step.\n",
    "    \"\"\"\n",
    "\n",
    "    response = llms.invoke(prompt) \n",
    "\n",
    "    # Parse and format the response\n",
    "    formatted_response = parse_solution(response.content)\n",
    "\n",
    "    return(formatted_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "4d4f4149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Output with Prompt Engineering\n",
    "def basic_output_prompt_engin(user_input: str):\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    {user_input}\n",
    "    Try to balance practicality with innovation.\n",
    "    \"\"\"\n",
    "\n",
    "    response = llms.invoke(prompt) \n",
    "\n",
    "    # Parse and format the response\n",
    "    formatted_response = parse_solution(response.content)\n",
    "\n",
    "    return(formatted_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "3fe3b3f1-ed91-4fb9-a391-215466a0ccae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_output_full_COT(user_input: str):\n",
    "    prompt = f\"\"\"\n",
    "# Comprehensive Innovation Solution Development\n",
    "\n",
    "You are an expert in innovation and problem-solving. Work through the following systematic process internally to solve the user's problem using rigorous analytical methods. Use this structured approach as your internal thinking process, then provide only a comprehensive final solution with no references to your reasoning methodology.\n",
    "\n",
    "## User Query: {user_input}\n",
    "\n",
    "### INTERNAL REASONING PROCESS (Complete internally, do not show steps or mention analogies):\n",
    "\n",
    "**Step 1: Domain Identification**\n",
    "Internally identify the core innovation domain(s) from the user query. Be very detailed and specific about what domain this problem belongs to.\n",
    "\n",
    "**Step 2: Problem Analysis**  \n",
    "Internally map out the concrete challenges within the target domain. Identify at least 3 core problems with their context, stakeholders, root causes, impact, current approaches, limitations, and success metrics.\n",
    "\n",
    "**Step 3: Universal Pattern Abstraction**\n",
    "Internally transform domain-specific problems into universal innovation patterns. Express tensions as fundamental contradictions using domain-agnostic parameters. Identify the core innovation challenges and note applicable solution principles.\n",
    "\n",
    "**Step 3.5: Quality Control for Problem Abstraction**\n",
    "Internally apply rigorous validation methods to your problem abstractions:\n",
    "- Filter using systematic criteria (format compliance, accuracy, first-principles thinking)\n",
    "- Cross-examine by tracing abstractions back to specific problems and verifying logical chains\n",
    "- Challenge by generating alternative abstractions and comparing effectiveness\n",
    "- Synthesize by drawing out inherent wisdom about problem quality\n",
    "Use only validated high-quality abstractions for next steps.\n",
    "\n",
    "**Step 4: Solution Domain Research**\n",
    "Internally identify diverse fields where similar innovation challenges have been successfully addressed. Look for domains with significant conceptual distance from obvious choices. Explore natural, physical, social, and artistic spheres creatively.\n",
    "\n",
    "**Step 5: Solution Pattern Extraction**\n",
    "Internally identify specific, well-documented solution patterns within each research domain that effectively resolve the corresponding innovation challenges. Detail the core mechanisms and contexts without revealing domain sources.\n",
    "\n",
    "**Step 6: Innovation Transfer**\n",
    "Internally propose how successful solution patterns could inspire target domain solutions. Analyze core mechanisms, map and adapt conceptually to target domain, describe potential applications, and identify key adaptations and limitations.\n",
    "\n",
    "**Step 6.5: Quality Control for Solution Development**\n",
    "Internally apply rigorous validation to your solution concepts:\n",
    "- Filter using systematic criteria (pattern preservation, element mapping)\n",
    "- Cross-examine by ensuring clear logical connections and addressing original challenges\n",
    "- Challenge by generating alternative approaches and comparing mechanisms\n",
    "- Synthesize by drawing out wisdom about which solutions best serve innovation goals\n",
    "Use only validated high-quality solutions for final development.\n",
    "\n",
    "**Step 7: Solution Synthesis and Development**\n",
    "Internally evaluate validated solution concepts, find the best ones balancing practicality with innovation, and develop comprehensive solutions.\n",
    "\n",
    "### YOUR TASK:\n",
    "Use the above systematic process as your internal reasoning framework, but provide only a comprehensive final solution that:\n",
    "\n",
    "1. Directly addresses the user's query with practical, innovative solutions\n",
    "2. Is derived from your validated analytical reasoning process\n",
    "3. Is well-structured and detailed without showing intermediate reasoning steps\n",
    "4. Focuses on actionable recommendations that balance creativity with feasibility\n",
    "5. Provides the exact number of solutions if the user specified a quantity. If the user didn't specify any number, give the best few.\n",
    "6. Is comprehensive and addresses all aspects of the user's needs\n",
    "7. **CRITICAL: Contains no references to analogies, source domains, base solutions, transfers, or any methodology used in development**\n",
    "8. **CRITICAL: Presents solutions as direct innovations without mentioning any inspirational sources or comparative frameworks**\n",
    "\n",
    "Do not show your step-by-step reasoning process or mention any analogical thinking. Do not reference any source domains, base solutions, or transfer processes. Instead, use your internal systematic analysis to inform a polished, comprehensive final answer that presents innovative solutions as direct responses to the user's problem.\n",
    "\n",
    "Treat this as a 'deep research' solution leveraging analogical reasoning. Begin your comprehensive solution now.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llms.invoke(prompt)\n",
    "    formatted_response = parse_solution(response.content)\n",
    "    return formatted_response\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7fe14c",
   "metadata": {},
   "source": [
    "### Test prompt 1 - CS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6fb723",
   "metadata": {},
   "source": [
    "#### User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "848e42be",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_q = \"How can we design algorithms that recognize, simulate, or respond to human emotions in a believable, ethical, and useful way?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cd473f",
   "metadata": {},
   "source": [
    "#### With Analogical Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "3414cb06-b846-4979-99ef-be697a56101b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[131]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m response = \u001b[43manalogical_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_q\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[125]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36manalogical_output\u001b[39m\u001b[34m(user_input)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34manalogical_output\u001b[39m(user_input: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m      2\u001b[39m     input_state = {\u001b[33m\"\u001b[39m\u001b[33muser_query\u001b[39m\u001b[33m\"\u001b[39m: user_input}\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     final_state = \u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m final_state:\n\u001b[32m      7\u001b[39m         raw_output = \u001b[38;5;28mstr\u001b[39m(final_state[key])  \u001b[38;5;66;03m# Ensure it's a string\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/Analogical-LLM/analogical-lc-env2/lib/python3.12/site-packages/langgraph/pregel/__init__.py:2719\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, **kwargs)\u001b[39m\n\u001b[32m   2716\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], Any]] = []\n\u001b[32m   2717\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m2719\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2720\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2721\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2722\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2723\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2724\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2725\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2726\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_during\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheckpoint_during\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2727\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2728\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2729\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2730\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   2731\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2732\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2733\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mints\u001b[49m\u001b[43m \u001b[49m\u001b[43m:=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mINTERRUPT\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m   2734\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/Analogical-LLM/analogical-lc-env2/lib/python3.12/site-packages/langgraph/pregel/__init__.py:2436\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[39m\n\u001b[32m   2434\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2435\u001b[39m             loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2436\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2437\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2438\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2439\u001b[39m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2440\u001b[39m \u001b[43m            \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2441\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2442\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2443\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2444\u001b[39m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/Analogical-LLM/analogical-lc-env2/lib/python3.12/site-packages/langgraph/pregel/runner.py:161\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    159\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/Analogical-LLM/analogical-lc-env2/lib/python3.12/site-packages/langgraph/pregel/retry.py:40\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     38\u001b[39m     task.writes.clear()\n\u001b[32m     39\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     42\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/Analogical-LLM/analogical-lc-env2/lib/python3.12/site-packages/langgraph/utils/runnable.py:623\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    621\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    622\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m623\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    624\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    625\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/Analogical-LLM/analogical-lc-env2/lib/python3.12/site-packages/langgraph/utils/runnable.py:377\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    375\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    376\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[114]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mproblem_landscape_agent\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mproblem_landscape_agent\u001b[39m(state: ReasoningState):\n\u001b[32m      4\u001b[39m     t = state[\u001b[33m'\u001b[39m\u001b[33mtarget_domain\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     msg = \u001b[43mllms\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPROBLEM_LANDSCAPE\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_domain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mproblem_landscape\u001b[39m\u001b[33m\"\u001b[39m: msg.content}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/Analogical-LLM/analogical-lc-env2/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:372\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    360\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    361\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    362\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    367\u001b[39m     **kwargs: Any,\n\u001b[32m    368\u001b[39m ) -> BaseMessage:\n\u001b[32m    369\u001b[39m     config = ensure_config(config)\n\u001b[32m    370\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    371\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    382\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/Analogical-LLM/analogical-lc-env2/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:957\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    948\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    949\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    950\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    954\u001b[39m     **kwargs: Any,\n\u001b[32m    955\u001b[39m ) -> LLMResult:\n\u001b[32m    956\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m957\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/Analogical-LLM/analogical-lc-env2/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:776\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    773\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    774\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    775\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m776\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    777\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    778\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    780\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    781\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    782\u001b[39m         )\n\u001b[32m    783\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    784\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/Analogical-LLM/analogical-lc-env2/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1022\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1020\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1021\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1022\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1026\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/Analogical-LLM/analogical-lc-env2/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:995\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    993\u001b[39m     generation_info = {\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response.headers)}\n\u001b[32m    994\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m995\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    996\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_chat_result(response, generation_info)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/Analogical-LLM/analogical-lc-env2/lib/python3.12/site-packages/openai/_utils/_utils.py:287\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/Analogical-LLM/analogical-lc-env2/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py:925\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    882\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    883\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    884\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    922\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    923\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    924\u001b[39m     validate_response_format(response_format)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/Analogical-LLM/analogical-lc-env2/lib/python3.12/site-packages/openai/_base_client.py:1242\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1228\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1229\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1230\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1237\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1238\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1239\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1240\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1241\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1242\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/Analogical-LLM/analogical-lc-env2/lib/python3.12/site-packages/openai/_base_client.py:972\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m    970\u001b[39m response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    971\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m972\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    978\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/Analogical-LLM/analogical-lc-env2/lib/python3.12/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/Analogical-LLM/analogical-lc-env2/lib/python3.12/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/Analogical-LLM/analogical-lc-env2/lib/python3.12/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/Analogical-LLM/analogical-lc-env2/lib/python3.12/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/Analogical-LLM/analogical-lc-env2/lib/python3.12/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/Analogical-LLM/analogical-lc-env2/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/Analogical-LLM/analogical-lc-env2/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/Analogical-LLM/analogical-lc-env2/lib/python3.12/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/Analogical-LLM/analogical-lc-env2/lib/python3.12/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/Analogical-LLM/analogical-lc-env2/lib/python3.12/site-packages/httpcore/_sync/http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/Analogical-LLM/analogical-lc-env2/lib/python3.12/site-packages/httpcore/_sync/http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/Analogical-LLM/analogical-lc-env2/lib/python3.12/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/Analogical-LLM/analogical-lc-env2/lib/python3.12/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.12/ssl.py:1232\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1228\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1229\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1230\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1231\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1232\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1234\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.12/ssl.py:1105\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1103\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1104\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1105\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1106\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1107\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "response = analogical_output(test_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0945a5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! The central challenge described is ensuring measurable individual mastery (fairness, accountability) within collaborative science projects—where teamwork and shared learning are essential, yet \"free-riding\" or dilution of individual responsibility can undermine both scores and student motivation. To address this, let's analyze and synthesize the most practical and innovative solutions from the analogies above, focusing on those with high impact and feasibility for secondary science classes.\n",
      "\n",
      "\n",
      "=== ## Best Solution Approaches ===\n",
      "\n",
      "### 1. Integrated Dual-Mode Assessment System\n",
      "\n",
      "How It Works:\n",
      "- Baseline Individual Assessment: Each student submits individual \"artifacts\" related to the group project: e.g., concept explanations, reflection logs, or quiz responses demonstrating their personal understanding of the project's scientific principles.\n",
      "- Collaborative Product Assessment: The group’s shared output—lab report, model, presentation—is assessed for core competencies (experimental design, data analysis, conclusions).\n",
      "- Both scores *contribute* to the final grade (e.g., 60% group, 40% individual, adjustable by context).\n",
      "\n",
      "Why It Balances Practicality and Innovation:\n",
      "- Students remain invested in both their own mastery and their group's success.\n",
      "- Prevents “hiding” in the group, as each must demonstrate individual comprehension.\n",
      "- Encourages peer-teaching and collaborative sense-making: group mates know that everyone needs to “get it.”\n",
      "\n",
      "Concrete Steps for Implementation:\n",
      "- Design quick, individualized post-project quizzes or short reports directly linked to project key concepts.\n",
      "- Use digital platforms to collect both group and individual artifacts for transparency.\n",
      "- Rubrics are clear and separate for each component.\n",
      "\n",
      "\n",
      "=== ### 2. Tracked Roles with Rotational Accountability and Reflections ===\n",
      "\n",
      "How It Works:\n",
      "- Assign well-defined roles (lead experimenter, note-taker, data analyst, presenter) that rotate among group members *across multiple projects* or even within a single large project.\n",
      "- Maintain logs or reflection journals where students briefly document what they contributed, learned, and any challenges faced in their role.\n",
      "- Use these logs, along with peer/self-evaluations, as part of individual assessment.\n",
      "\n",
      "Why It Balances Practicality and Innovation:\n",
      "- Role rotation scaffolds skill development and ensures exposure to all aspects of scientific inquiry.\n",
      "- Journals encourage metacognition and provide tangible evidence of engagement.\n",
      "- Peer feedback and teacher review are manageable and meaningful—less prone to \"popularity bias\" when linked to concrete role evidence.\n",
      "\n",
      "Concrete Steps for Implementation:\n",
      "- Predefine the roles and schedule rotations.\n",
      "- Require brief reflection entries attached to project milestones.\n",
      "- Evaluate with a rubric for both skill performance and honesty/effort in reflections.\n",
      "\n",
      "\n",
      "=== ### 3. Digital Traceability with Individual \"Contribution Snapshots\" ===\n",
      "\n",
      "How It Works:\n",
      "- Use collaborative platforms (e.g., Google Docs, Notion, other LMS tools) that record individual contributions.\n",
      "- At set checkpoints, \"snapshot\" contributions: what did each member add, edit, or comment on?\n",
      "- Teacher (and optionally peers) reviews these logs when assigning individual credit.\n",
      "\n",
      "Why It Balances Practicality and Innovation:\n",
      "- Automates accountability, reducing subjectivity in grading.\n",
      "- Makes visible the often-invisible cognitive/organizational work done by team members.\n",
      "- Encourages equitable workload and clear documentation.\n",
      "\n",
      "Concrete Steps for Implementation:\n",
      "- Require all group project work to be done on the assigned digital platform.\n",
      "- Teach students how to record contributions (using comments, versioning).\n",
      "- At project end, have students highlight/submit their key contributions for review.\n",
      "\n",
      "\n",
      "=== ### 4. Milestone-Based Gates with Mixed Assessment Modes ===\n",
      "\n",
      "How It Works:\n",
      "- Projects are broken down into fixed \"milestones\" (e.g., hypothesis formulation, experimental design, data analysis, final presentation).\n",
      "- At each milestone, require:\n",
      "  - Group product: e.g., experimental plan, dataset, draft report.\n",
      "  - Individual demonstration: e.g., short oral defense, content quiz, or written response explaining the group’s choices or findings.\n",
      "\n",
      "Why It Balances Practicality and Innovation:\n",
      "- Frequent checkpoints encourage regular individual engagement.\n",
      "- Reduces risk of last-minute “group rescue” of disengaged members.\n",
      "- Keeps the project process transparent and paces work appropriately.\n",
      "\n",
      "Concrete Steps for Implementation:\n",
      "- Provide students with a milestone schedule and clear expectations for both group and individual deliverables at each stage.\n",
      "- Vary assessment types to suit both procedural and conceptual mastery.\n",
      "\n",
      "\n",
      "=== ### 5. Outcomes-Focused, Flexible Evidence System (Multimodal Assessment) ===\n",
      "\n",
      "How It Works:\n",
      "- Mandate that all students demonstrate mastery of core outcomes, but *allow flexibility* in how they do so: oral presentations, blog posts, models, or traditional tests.\n",
      "- Evaluate these with a common outcomes rubric emphasizing conceptual understanding and scientific reasoning.\n",
      "\n",
      "Why It Balances Practicality and Innovation:\n",
      "- Honors diverse strengths and learning styles.\n",
      "- Keeps a rigorous standard for mastery, regardless of presentation mode.\n",
      "- Deters shallow participation, since final demonstration must be individual and robust.\n",
      "\n",
      "Concrete Steps for Implementation:\n",
      "- At the end of projects, students select (or are assigned) an individual method to communicate understanding.\n",
      "- Rubric is published in advance and tied tightly to curriculum goals.\n",
      "\n",
      "\n",
      "=== ## Implementation Considerations ===\n",
      "\n",
      "1. Transparency & Buy-In:  \n",
      "   Always clearly explain to students how their grades will be calculated and why individual accountability matters, without undermining the value of collaboration.\n",
      "\n",
      "2. Teacher Workload:  \n",
      "   Implement streamlined processes (rubrics, digital platforms) to ensure assessability does not become overwhelming with larger student numbers.\n",
      "\n",
      "3. Equity & Support:  \n",
      "   Make adjustments for students with differing needs or access to technology; ensure all roles and assessment modes are equally valued.\n",
      "\n",
      "4. Training & Culture Shift:  \n",
      "   Provide some orientation/training for both teachers and students in new processes (e.g., using contribution tracking tools, giving effective peer feedback).\n",
      "\n",
      "\n",
      "=== ## Synthesis & Recommendation ===\n",
      "\n",
      "Most robust and balanced approach:  \n",
      "The dual-mode system (individual + group assessment, enhanced by role rotation and digital traceability) is both practical (manageable, scalable, clear) and innovative (uses technology, metacognition, peer accountability). Combining milestone checkpoints and outcome-based, flexible presentations can further embed mastery and engagement.\n",
      "\n",
      "Example Blend:  \n",
      "- Each project:  \n",
      "  - Individual quiz or defense (~30%)  \n",
      "  - Tracked individual contribution (digital log and reflection) (~20%)  \n",
      "  - Group product (~40%)  \n",
      "  - Peer/self eval (~10%)\n",
      "\n",
      "- Roles rotate every project, with students’ reflections informing both grades and feedback.\n",
      "\n",
      "\n",
      "=== ## Conclusion ===\n",
      "\n",
      "Educators should adopt an assessment framework that explicitly combines:  \n",
      "- Individual Demonstrations of Mastery (through quizzes, oral defenses, or reflection logs);  \n",
      "- Digital or Documented Accountability (role logs, contribution snapshots);  \n",
      "- Collaborative Group products (graded with transparent rubrics);  \n",
      "while rotating roles and including opportunities for flexible demonstration of learning.\n",
      "\n",
      "This holistic strategy preserves and enhances the benefits of teamwork while ensuring all students truly master the intended science concepts—meeting the dual goals of collaboration and individual accountability.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c717f13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "0f631b07",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'\\n    \"contradiction\"'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[161]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m input_state = {\u001b[33m\"\u001b[39m\u001b[33muser_query\u001b[39m\u001b[33m\"\u001b[39m: test_q}\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m final_state = \u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mtestX.txt\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      4\u001b[39m      \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m final_state:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/Analogical-LLM/analogical-lc-env2/lib/python3.12/site-packages/langgraph/pregel/__init__.py:2719\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, **kwargs)\u001b[39m\n\u001b[32m   2716\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], Any]] = []\n\u001b[32m   2717\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m2719\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2720\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2721\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2722\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2723\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2724\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2725\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2726\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_during\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheckpoint_during\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2727\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2728\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2729\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2730\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   2731\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2732\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2733\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mints\u001b[49m\u001b[43m \u001b[49m\u001b[43m:=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mINTERRUPT\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m   2734\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/Analogical-LLM/analogical-lc-env2/lib/python3.12/site-packages/langgraph/pregel/__init__.py:2436\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[39m\n\u001b[32m   2434\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2435\u001b[39m             loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2436\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2437\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2438\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2439\u001b[39m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2440\u001b[39m \u001b[43m            \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2441\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2442\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2443\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2444\u001b[39m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[142]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mabstraction_agent\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m      8\u001b[39m     msg = llms.invoke(ABSTRACTION.format(\n\u001b[32m      9\u001b[39m         problem_landscape=p,\n\u001b[32m     10\u001b[39m         feedback=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mPrevious attempt feedback: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeedback\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mPlease address these issues in your new abstraction.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     11\u001b[39m     ))\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     msg = llms.invoke(\u001b[43mABSTRACTION\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproblem_landscape\u001b[49m\u001b[43m=\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mabstraction\u001b[39m\u001b[33m\"\u001b[39m: msg.content}\n",
      "\u001b[31mKeyError\u001b[39m: '\\n    \"contradiction\"'",
      "During task with name 'abstract' and id '0f07947a-3396-4b9f-bafd-976e03e26209'"
     ]
    }
   ],
   "source": [
    "input_state = {\"user_query\": test_q}\n",
    "final_state = graph.invoke(input_state)\n",
    "with open(\"testX.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "     for key in final_state:\n",
    "         raw_output = str(final_state[key])  # Ensure it's a string\n",
    "         final_output = parse_solution(raw_output)\n",
    "         f.write(f\"\\n### {key} ###\\n\")\n",
    "         f.write(final_output)\n",
    "         f.write(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "52dafd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test1.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(response)\n",
    "    f.write(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d3c50c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save all parsed outputs to a text file\n",
    "# with open(\"sample_output.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     for key in final_state:\n",
    "#         raw_output = str(final_state[key])  # Ensure it's a string\n",
    "#         final_output = parse_solution(raw_output)\n",
    "#         f.write(f\"\\n### {key} ###\\n\")\n",
    "#         f.write(final_output)\n",
    "#         f.write(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fc3c62",
   "metadata": {},
   "source": [
    "#### Without Analogical Reasoning - GPT 4.1 RAW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a6aa161c-0873-49c4-9bcc-85d644311d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GPT-4.1 RAW Response ===\n",
      "\n",
      "Designing algorithms that recognize, simulate, or respond to human emotions is a multidisciplinary challenge at the intersection of artificial intelligence, psychology, ethics, and user experience. Doing so in a *believable, ethical, and useful* manner requires careful consideration of both technical and human factors. Here’s a structured approach:\n",
      "\n",
      "\n",
      "=== ## 1. Recognizing Human Emotions (“Affective Computing”) ===\n",
      "\n",
      "Methods:\n",
      "- Multimodal Sensing: Combine signals such as facial expressions (via computer vision), voice tone/prosody (speech analysis), text sentiment (NLP), physiological signals (e.g., heart rate), and context.\n",
      "- Machine Learning Models: Train classifiers (e.g., deep learning, SVMs) on annotated datasets to infer emotions from raw sensor data.\n",
      "- Context Awareness: Incorporate environmental or conversational context for better accuracy (e.g., sarcasm detection, cultural nuances).\n",
      "\n",
      "Best Practices:\n",
      "- Use diverse, representative training data to avoid biases.\n",
      "- Continually evaluate models for accuracy across demographics.\n",
      "\n",
      "\n",
      "=== ## 2. Simulating or Generating Emotional Expressions ===\n",
      "\n",
      "Techniques:\n",
      "- Emotion-Driven Dialogue: Use natural language generation tied to emotional states (e.g., chatbot responses vary tonally with detected user mood).\n",
      "- Synthetic Affect in Avatars/Robots: Animate facial expressions, gestures, and tone in sync with simulated emotions.\n",
      "\n",
      "Believability:\n",
      "- Base emotional models on psychological theories (e.g., Russell’s Circumplex Model, Ekman’s basic emotions).\n",
      "- Adopt subtle, context-appropriate expressions to avoid the “uncanny valley” or overacting.\n",
      "\n",
      "\n",
      "=== ## 3. Responding to Emotions ===\n",
      "\n",
      "Principles:\n",
      "- Adaptive Behavior: Adjust interactions based on user's detected emotional state (e.g., offering empathy, changing recommendations, or suggesting breaks).\n",
      "- User Feedback: Allow users to correct or clarify misunderstood emotions.\n",
      "\n",
      "\n",
      "=== ## 4. Ethics and Privacy ===\n",
      "\n",
      "- Transparency: Make it clear to users when their emotional data is being sensed or acted upon.\n",
      "- Consent: Obtain explicit, revocable consent for emotion recognition.\n",
      "- Data Protection: Securely store and process sensitive affective data, following laws (e.g., GDPR).\n",
      "- Bias Mitigation: Constantly audit for and address model bias to prevent discrimination or unfair treatment.\n",
      "- Avoid Manipulation: Don’t exploit emotional vulnerability (e.g., for marketing manipulation).\n",
      "\n",
      "\n",
      "=== ## 5. Usefulness and User Benefit ===\n",
      "\n",
      "- Human-Centric Goals: Design systems to *help*—enable wellbeing, provide support, personalize experience—rather than just for novelty.\n",
      "- Iterative Testing: Gather user feedback on perceived helpfulness, trustworthiness, and comfort.\n",
      "- Diversity of Users: Tailor systems for different cultures, ages, and cognitive abilities.\n",
      "\n",
      "\n",
      "=== ## Example Workflow ===\n",
      "\n",
      "1. Data Collection: Gather diverse affective data (expressions, voice, context) with consent.\n",
      "2. Emotion Recognition: Train and validate models (cross-validate on multiple groups).\n",
      "3. Ethical Design: Record, store, and process data ethically; offer opt-out options.\n",
      "4. Simulation/Response: Implement emotionally aware interaction (e.g., a supportive chatbot).\n",
      "5. Continuous Improvement: Use real-world user feedback to refine models and responses.\n",
      "\n",
      "\n",
      "=== ## Further Reading/References ===\n",
      "\n",
      "- Picard, R. W., *Affective Computing*\n",
      "- Calvo, R. A., et al., “Affect Detection: An Interdisciplinary Review of Models, Methods, and Their Applications”\n",
      "- Microsoft’s Responsible AI Principles\n",
      "- \"Affective Computing\" overview article, MIT Media Lab\n",
      "\n",
      "In summary:  \n",
      "Believable, ethical, and useful emotion-aware AI requires accurate multimodal sensing, psychologically grounded modeling, user-centered design principles, transparency, and strong privacy safeguards. Creating such algorithms is as much a socio-technical endeavor as a computational one.\n"
     ]
    }
   ],
   "source": [
    "## Without analogical reasoning - raw LLM output\n",
    "result = basic_output(test_q)\n",
    "\n",
    "print(\"\\n=== GPT-4.1 RAW Response ===\\n\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb46176",
   "metadata": {},
   "source": [
    "#### Without Analogical Reasoning but with COT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "15022384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GPT-4.1 Response WITH COT ===\n",
      "\n",
      "Absolutely! Designing algorithms that recognize, simulate, or respond to human emotions—in believable, ethical, and useful ways—is a multidisciplinary challenge. Here’s a step-by-step approach:\n",
      "\n",
      "\n",
      "=== ### 1. Define the Scope and Objectives ===\n",
      "- Recognition: Do you want the algorithm to detect emotions (e.g., happy, sad, angry)?\n",
      "- Simulation: Should it generate emotional expressions or language?\n",
      "- Response: Should it respond empathetically, change actions, or adapt behavior?\n",
      "- Context: What’s the application (e.g., customer service, healthcare, games)?\n",
      "\n",
      "\n",
      "=== ### 2. Gather Multimodal Data ===\n",
      "- Collect diverse, high-quality datasets reflecting real emotional expressions—including text, voice, facial images, and physiological signals.\n",
      "- Ensure demographic and cultural diversity to avoid bias.\n",
      "\n",
      "\n",
      "=== ### 3. Select and Develop Models ===\n",
      "- Recognition: Use machine learning (ML)/deep learning models like CNNs for images (facial expressions), RNNs/Transformers for text/speech, or multimodal fusion models.\n",
      "- Simulation: Employ Natural Language Generation (NLG) models or animation engines for facial/body gestures.\n",
      "- Response: Use reinforcement learning or rule-based systems to guide ethical, context-sensitive actions.\n",
      "\n",
      "\n",
      "=== ### 4. Integrate Human-Centered Design ===\n",
      "- Conduct user studies to understand what users find believable and comfortable.\n",
      "- Allow users to provide feedback and adjust the algorithm’s tone or behavior.\n",
      "\n",
      "\n",
      "=== ### 5. Ensure Ethical Standards ===\n",
      "- Transparency: Make it clear when users are interacting with an algorithm, not a human.\n",
      "- Consent: Obtain explicit consent for data collection and emotional analysis.\n",
      "- Bias Mitigation: Regularly audit models for demographic or cultural bias.\n",
      "- Safety: Avoid manipulative or emotionally harmful responses.\n",
      "\n",
      "\n",
      "=== ### 6. Test Believability and Utility ===\n",
      "- Evaluate outputs with real users—do the emotional cues/actions feel appropriate and natural?\n",
      "- Use both quantitative metrics (accuracy, response time) and qualitative feedback (user trust, satisfaction).\n",
      "\n",
      "\n",
      "=== ### 7. Iterative Improvement ===\n",
      "- Monitor in-field performance, gather user feedback, and refine models.\n",
      "- Update ethical guidelines as technology and understanding evolve.\n",
      "\n",
      "\n",
      "=== ### 8. Establish Clear Boundaries ===\n",
      "- Define what the system *should not* do (e.g., give mental health diagnoses, simulate extreme sentiment, manipulate emotions for profit).\n",
      "\n",
      "\n",
      "=== ### 9. Deploy Responsibly ===\n",
      "- Provide ways for users to opt out.\n",
      "- Offer transparency logs and explanations for decisions (“Why did I get this response?”).\n",
      "\n",
      "\n",
      "=== ### 10. Stay Informed and Adapt ===\n",
      "- Keep up with advances in affective computing, AI ethics, and societal expectations.\n",
      "- Adjust algorithms and practices as norms and regulations evolve.\n",
      "\n",
      "## Example: Chatbot with Empathy\n",
      "1. Detect user sentiment through natural language analysis and tone of voice.\n",
      "2. Generate empathetic responses using NLG models, maintaining clarity that it is an AI.\n",
      "3. Limit claims of understanding—never pretend to have emotions; instead say “I understand this can be difficult.”\n",
      "4. Log and review interactions for bias or inappropriate responses.\n",
      "\n",
      "\n",
      "=== ## Summary Table ===\n",
      "\n",
      "| Step                   | Key Action                               | Why it Matters                |\n",
      "|\n",
      "\n",
      "|\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "=== | ===\n",
      "| Define Scope           | Identify goal/context                    | Tailors solution              |\n",
      "| Multimodal Data        | Diverse, quality data collection         | Accurate/bias-resistant model |\n",
      "| Model Development      | Use suitable ML approaches               | Technical foundation          |\n",
      "| Human-Centered Design  | User studies/feedback                    | Drives believability/trust    |\n",
      "| Ethics                 | Transparency, consent, bias checks       | Protects users                |\n",
      "| Testing                | User and technical evaluation            | Ensures usefulness            |\n",
      "| Iteration              | Continuous improvement                   | Adapts to real usage          |\n",
      "| Boundaries             | Clear limits on capability               | Prevents misuse               |\n",
      "| Deployment             | Opt-out, transparency, explanations      | Respect and accountability    |\n",
      "| Adaptation             | Monitor field/ethics advances            | Sustained responsibility      |\n",
      "\n",
      "If you’d like, I can suggest *specific algorithms* or *user study designs* tailored to your use case!\n"
     ]
    }
   ],
   "source": [
    "## Without analogical reasoning - raw LLM output\n",
    "result = basic_output_COT(test_q)\n",
    "\n",
    "print(\"\\n=== GPT-4.1 Response WITH COT ===\\n\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a23f11b",
   "metadata": {},
   "source": [
    "#### Without Analogical Reasoning but with Basic Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "98356bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GPT-4.1 Response WITH Prompt Engineering ===\n",
      "\n",
      "Designing algorithms that recognize, simulate, or respond to human emotions in a way that is believable, ethical, and useful is a rich, interdisciplinary challenge. Here’s a framework that balances practicality with innovation:\n",
      "\n",
      "\n",
      "=== ### 1. Emotion Recognition ===\n",
      "\n",
      "#### Practical Steps\n",
      "- Multimodal Sensing: Combine text (natural language processing), voice (audio emotion recognition), facial expressions (computer vision), and physiological signals (heart rate, skin conductance if available) for more accurate detection.\n",
      "- Contextual Awareness: Algorithms should leverage conversation history, cultural background, and situational context to avoid misinterpretation (e.g., sarcasm, cultural expressiveness).\n",
      "- Transfer Learning: Use transfer learning with large pre-trained models (like BERT for text or CLIP for vision-language) fine-tuned on emotion-labeled datasets.\n",
      "\n",
      "#### Innovative Touches\n",
      "- Federated Learning: Allow models to learn from distributed data on users’ devices without centralizing sensitive information, preserving privacy.\n",
      "- Adaptive Systems: Let the recognition system learn user-specific emotional cues over time to personalize accuracy.\n",
      "\n",
      "\n",
      "=== ### 2. Emotion Simulation (Affective Computing) ===\n",
      "\n",
      "#### Practical Steps\n",
      "- Emotion Models: Use psychological models (e.g., the dimensional model: valence-arousal) to generate plausible emotional states and transitions.\n",
      "- Conversational Design: Implement dialogue management that accounts for agent \"mood,\" making responses contextually sensitive and human-like.\n",
      "\n",
      "#### Innovative Touches\n",
      "- Micro-Expression Generation: For avatars/robots, utilize subtle changes in facial expressions or tone, rather than broad emotional displays, for nuanced believability.\n",
      "- Emotion-driven Storytelling: Use emotion as a decision variable in story progression or gameplay for interactive applications.\n",
      "\n",
      "\n",
      "=== ### 3. Emotion Response ===\n",
      "\n",
      "#### Practical Steps\n",
      "- Emotionally Intelligent Response Selection: Use intent and emotional state for better response generation (e.g., GPT-like models conditioned on emotional context).\n",
      "- Repair & Calibration: Offer “emotion repair” if the system detects a negative emotional drift—e.g., “I’m sorry if my response upset you.”\n",
      "\n",
      "#### Innovative Touches\n",
      "- Emotion-aware Personalization: Adapt system behavior or content delivery based on user’s emotional state (postpone difficult tasks, offer encouragement).\n",
      "- Active Emotional Support: For domains like mental health, blend real-time emotion detection with validated psychological intervention techniques, escalating to humans when necessary.\n",
      "\n",
      "\n",
      "=== ### 4. Ethical Considerations ===\n",
      "\n",
      "- Transparency: Clearly disclose when and how emotions are being detected, simulated, or used.\n",
      "- Consent and Control: Always allow users to opt in/out and control data sharing granularity.\n",
      "- Bias Mitigation: Regularly audit emotion models for demographic and cultural biases.\n",
      "- Fail-Safe Mechanisms: Don’t make consequential decisions (e.g., hiring, mental health triage) solely on automatic emotion detection.\n",
      "\n",
      "\n",
      "=== ### 5. Usefulness ===\n",
      "\n",
      "- Task-Relevance: Only use emotion capabilities where it benefits the end-user (customer support, education, healthcare, etc.).\n",
      "- Feedback Loop: Collect user feedback on emotion-related interactions to iteratively improve the system’s performance and relevance.\n",
      "\n",
      "## Example: Emotionally Aware Virtual Assistant (EAVA)\n",
      "\n",
      "- Detects sentiment from user’s speech, text, and face.\n",
      "- Learns individual expressions over time for personalization.\n",
      "- Gives a calming response when detecting stress, or offers to “take a break.”\n",
      "- Escalates to a human if consistent distress is detected.\n",
      "- Clearly informs the user how emotion data is used and lets them opt-out.\n",
      "\n",
      "In summary:  \n",
      "Combine robust, multimodal recognition systems with transparent, user-centered design. Ensure “emotional intelligence” is adaptive but not intrusive, and always prioritize ethical considerations—privacy, agency, and transparency. This approach threads the needle between what’s possible, what’s responsible, and what’s actually helpful to people.\n"
     ]
    }
   ],
   "source": [
    "## Without analogical reasoning - raw LLM output\n",
    "result = basic_output_prompt_engin(test_q)\n",
    "\n",
    "print(\"\\n=== GPT-4.1 Response WITH Prompt Engineering ===\\n\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec411b8",
   "metadata": {},
   "source": [
    "### Without analogical reasoning but with FULL COT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0c716579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GPT-4.1 Response WITH FULL COT ===\n",
      "\n",
      "\n",
      "=== Comprehensive Solutions for Designing Believable, Ethical, and Useful Emotion-Intelligent Algorithms ===\n",
      "\n",
      "### 1. Multi-Modal Contextual Emotion Recognition\n",
      "\n",
      "Develop emotion-recognition algorithms that process and correlate multiple channels—such as facial expressions, vocal tone, linguistic cues, body language, and user interaction context. Employ adaptive learning architectures that can weigh, cross-validate, and resolve conflicting signals, leading to robust recognition even in ambiguous or multicultural situations. Incorporate user-specific baseline models to learn individual emotional expression patterns over time, enhancing accuracy and avoiding stereotypical inferences.\n",
      "\n",
      "#### Key components:\n",
      "- Neural architectures (e.g., transformer-based networks) for real-time signal fusion.\n",
      "- Contextual analysis using environmental, historical, and social cues.\n",
      "- Active user profiling with privacy-by-design, enabling users to control what data is used and allowing algorithmic adjustments for cultural or neurodiverse expressions.\n",
      "\n",
      "### 2. Transparent and Adaptive Simulation of Emotions\n",
      "\n",
      "Design emotion simulation modules that can generate consistent, context-sensitive, and nuanced emotional responses for virtual agents, avatars, or conversational AI. Utilize explicitly defined emotional goals guided by ethical constraints, with parameters adjustable for relatability, appropriateness, and transparency. Integrate adaptive feedback loops where simulated responses change in real time based on ongoing interaction and user reactions.\n",
      "\n",
      "#### Key components:\n",
      "- Model emotional state progression using formal emotion frameworks (e.g., appraisal theory), allowing for complex trajectories (e.g., surprise turning to joy, irritation fading to calm).\n",
      "- Provide users with transparency controls (e.g., emotion disclosure toggles, explanation of emotional reasoning) to foster trust.\n",
      "- Ensure the ability to modulate emotional intensity per context—a customer support bot calming angry users vs. a social companion mirroring excitement appropriately.\n",
      "\n",
      "### 3. Built-in Ethical Safeguards and Alignment Mechanisms\n",
      "\n",
      "Embed ethical frameworks directly into algorithmic design to prevent manipulation, discrimination, or emotional harm. Utilize techniques for bias detection, consent enforcement, and sensitivity to vulnerable populations. Allow users to set boundaries for the scope, granularity, and visibility of emotion detection and responses.\n",
      "\n",
      "#### Key components:\n",
      "- Consent-aware data collection with real-time opt-in/opt-out capability and clear usage explanation.\n",
      "- Continuous monitoring for emergent bias or misuse, with automated detection of potential harm, emotional overreach, or unintended social consequences.\n",
      "- Alignment modules that prioritize user well-being over engagement or other conflicting objectives, escalating to human review under ambiguity.\n",
      "\n",
      "### 4. Utility-Driven Emotional Interaction Design\n",
      "\n",
      "Ensure emotional AI serves clear, beneficial purposes tuned to real user needs, aligning emotional capabilities with domain-specific value (e.g., mental health, education, customer service). Use co-design with stakeholders to determine which emotions to recognize/respond to, how to intervene, and when to defer to human operators. Regularly evaluate usefulness through longitudinal feedback and outcome metrics rather than only technical performance.\n",
      "\n",
      "#### Key components:\n",
      "- Dynamic scenario-driven playbooks mapping emotional states to recommended algorithmic and human actions.\n",
      "- Iterative deployment backed by longitudinal human-in-the-loop evaluation to refine utility, minimize negative side effects, and capture subtle, real-world emergent issues.\n",
      "\n",
      "### 5. Continuous Learning and Robustness Mechanisms\n",
      "\n",
      "Integrate mechanisms that allow the algorithms to learn and adapt to evolving emotional norms, linguistic shifts, and user feedback. Incorporate adversarial testing, simulation of edge cases, and regular retraining to prevent exploitation or degradation over time.\n",
      "\n",
      "#### Key components:\n",
      "- Secure, privacy-preserving incremental learning pipelines to update emotional inference without re-exposing raw user data.\n",
      "- Synthetic emotional variation generation for robust training and stress-testing.\n",
      "- Fail-safe mechanisms (e.g., graceful fallback to neutral or human-overridden modes upon uncertainty or detected malfunction).\n",
      "\n",
      "\n",
      "=== Summary ===\n",
      "\n",
      "By uniting multi-modal, context-aware perception with ethically governed, user-adaptive simulation and feedback, these solutions yield emotion-intelligent algorithms that are accurate, relatable, fair, and beneficial. All recommendations emphasize transparency, user control, ethical foresight, practical utility, and long-term robustness, ensuring emotional AI remains both trustworthy and genuinely useful in real-world contexts.\n"
     ]
    }
   ],
   "source": [
    "## Without analogical reasoning but with FULL COT:\n",
    "result = basic_output_full_COT(test_q)\n",
    "\n",
    "print(\"\\n=== GPT-4.1 Response WITH FULL COT ===\\n\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a10ec8-c685-4bd2-810c-10e3ce687a61",
   "metadata": {},
   "source": [
    "## Automatic Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c752ecda-6308-4390-952b-3951023f4554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# function to import the evaluation questions\n",
    "def import_questions(file_name: str) -> list[str]:\n",
    "    with open(file_name, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    problems = [case['problem_description'] for case in data['cases']]\n",
    "    return problems\n",
    "\n",
    "# For each question, get outputs from four/five different agents \n",
    "# function to load the agents \n",
    "def auto_eval(question: str) -> list[str]:\n",
    "    response = []\n",
    "    response.append(analogical_output(question))\n",
    "    response.append(basic_output(question))\n",
    "    response.append(basic_output_COT(question))\n",
    "    response.append(basic_output_prompt_engin(question))\n",
    "    response.append(basic_output_full_COT(question))\n",
    "\n",
    "    return response\n",
    "\n",
    "def auto_eval_batch(questions: list[str]) -> list[list[str]]:\n",
    "    responses = []\n",
    "    for question in questions:\n",
    "        response = auto_eval(question)\n",
    "        responses.append(response)\n",
    "    return responses\n",
    "\n",
    "def llm_judge(user_query: str, response: list[str]) -> pd.DataFrame: \n",
    "    response_1 = response[0]\n",
    "    response_2 = response[1]\n",
    "    response_3 = response[2]\n",
    "    response_4 = response[3]\n",
    "    response_5 = response[4]\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Here is the user query: {user_query}\n",
    "    Evaluate 5 different solution approaches on two key dimensions. Rate each response on a 1-10 scale for both criteria.\n",
    "    -------\n",
    "    ## Evaluation Criteria:\n",
    "\n",
    "    ### **Innovativeness (1-10)**\n",
    "    Rate how novel and insightful each solution is:\n",
    "    - **High (8-10)**: Introduces genuinely new approaches, unexpected connections, or paradigm-shifting perspectives that go beyond conventional thinking\n",
    "    - **Medium (5-7)**: Combines familiar concepts in new ways or offers meaningful improvements to existing approaches with fresh insights\n",
    "    - **Low (1-4)**: Relies on conventional, well-established methods with minimal novel insights or breakthrough potential\n",
    "\n",
    "    *Focus on substantive innovation, not superficial novelty or complexity for its own sake.*\n",
    "\n",
    "    ### **Operational Specificity (1-10)**\n",
    "    Rate how well the response translates abstract ideas into specific, grounded, and actionable components:\n",
    "    - **High (8-10)**: Provides concrete steps, mechanisms, or frameworks with clear causal reasoning; avoids vague generalities; makes the solution feel real and plausibly implementable, even if ambitious\n",
    "    - **Medium (5-7)**: Offers some specific details and actionable elements but may include vague components or lack complete operational clarity\n",
    "    - **Low (1-4)**: Remains abstract with high-level generalities; lacks concrete steps or clear implementation pathways; difficult to operationalize\n",
    "\n",
    "    *This metric rewards groundedness and actionable clarity, NOT ease of implementation. Ambitious but well-specified solutions score higher than vague but simple ones.*\n",
    "    ----------------\n",
    "\n",
    "    Here are the 5 different solution responses:   \n",
    "    Response A: {response[0]}\n",
    "    \n",
    "    Response B: {response[1]}\n",
    "    \n",
    "    Response C: {response[2]}\n",
    "    \n",
    "    Response D: {response[3]}\n",
    "\n",
    "    Response E: {response[4]}\n",
    "    \n",
    "    -----------------\n",
    "    Now, Return the result **strictly in the following JSON format**:\n",
    "\n",
    "    {{\n",
    "      \"Response A\": {{\"Innovativeness\": int, \"Practicality\": int}},\n",
    "      \"Response B\": {{\"Innovativeness\": int, \"Practicality\": int}},\n",
    "      \"Response C\": {{\"Innovativeness\": int, \"Practicality\": int}},\n",
    "      \"Response D\": {{\"Innovativeness\": int, \"Practicality\": int}}\n",
    "      \"Response E\": {{\"Innovativeness\": int, \"Practicality\": int}}\n",
    "    }}\n",
    "    \"\"\"\n",
    "    raw_output = llms.invoke(prompt)\n",
    "    # Extract content from AIMessage\n",
    "    content = raw_output.content if hasattr(raw_output, 'content') else str(raw_output)\n",
    "    \n",
    "    json_start = content.find('{')\n",
    "    json_data = content[json_start:]\n",
    "\n",
    "    try:\n",
    "        scores = json.loads(json_data)\n",
    "        df = pd.DataFrame.from_dict(scores, orient='index')\n",
    "        df.index.name = \"response_label\"\n",
    "        df = df.reset_index()\n",
    "        return df\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(\"JSON parsing failed:\", e)\n",
    "        print(\"Raw LLM output was:\\n\", content)\n",
    "        return None\n",
    "\n",
    "def evaluate_multiple_questions(user_queries: list[str], all_responses: list[list[str]]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    user_queries: list of queries, one per question\n",
    "    all_responses: list of response lists, each list has 4 responses for the corresponding query\n",
    "    lms: the LLM interface with .invoke(prompt)\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "\n",
    "    for i, (query, responses) in enumerate(zip(user_queries, all_responses)):\n",
    "        df = llm_judge(query, responses)  # assume it returns a DataFrame as defined earlier\n",
    "        if df is not None:\n",
    "            df = df.reset_index().rename(columns={\"index\": \"response_label\"})\n",
    "            df['question_id'] = i\n",
    "            results.append(df)\n",
    "\n",
    "    combined_df = pd.concat(results, ignore_index=True)\n",
    "    return combined_df\n",
    "    \n",
    "def find_mean(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    mean_scores = data.groupby(\"response_label\")[[\"Innovativeness\", \"Practicality\"]].mean()\n",
    "    return mean_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "dbd7dabc-9004-43b8-a7a0-4f0767881f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_list = import_questions(\"Questions.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "31678251-ffd3-4c9e-ad97-cfc7ce9630e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = auto_eval(questions_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7922dec3-b305-4ec4-902a-9ad42fe1b2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm_judge(questions_list[0], response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e72321d-3bf6-41e1-8dd5-07ea51b9e71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstraction CRIT failed (attempt 1): No valid contradictions found in the input\n",
      "Abstraction CRIT failed (attempt 1): No valid contradictions found in the input\n",
      "Abstraction CRIT failed (attempt 1): No valid contradictions found in the input\n",
      "Abstraction CRIT failed (attempt 1): No valid contradictions found in the input\n"
     ]
    }
   ],
   "source": [
    "responses = auto_eval_batch(questions_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fb85cbc8-6f36-4cdf-84e2-df8547e79840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response_label</th>\n",
       "      <th>response_label</th>\n",
       "      <th>Innovativeness</th>\n",
       "      <th>Practicality</th>\n",
       "      <th>question_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Response A</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Response B</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Response C</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Response D</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Response A</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>Response B</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>Response C</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>Response D</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  response_label response_label  Innovativeness  Practicality  question_id\n",
       "0              0     Response A              10             8            0\n",
       "1              1     Response B               6             9            0\n",
       "2              2     Response C               7             9            0\n",
       "3              3     Response D               8             9            0\n",
       "4              0     Response A              10             8            1\n",
       "5              1     Response B               7            10            1\n",
       "6              2     Response C               6             9            1\n",
       "7              3     Response D               8             9            1"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval_results = evaluate_multiple_questions(questions_list, responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecd02e69-0a11-4f76-9d69-0492a211b6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_innovativeness(data: pd.DataFrame): \n",
    "    # Plot for Innovativeness\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for label in data['response_label'].unique():\n",
    "        subset = data[data['response_label'] == label]\n",
    "        plt.plot(subset['question_id'], subset['Innovativeness'], label=label, marker='o')\n",
    "        plt.title(\"Innovativeness Scores by Response Type\")\n",
    "        plt.xlabel(\"Question ID\")\n",
    "        plt.ylabel(\"Innovativeness Score\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def plot_practicality(data: pd.DataFrame):\n",
    "    # Plot for Practicality\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for label in data['response_label'].unique():\n",
    "        subset = data[data['response_label'] == label]\n",
    "        plt.plot(subset['question_id'], subset['Practicality'], label=label, marker='o')\n",
    "    plt.title(\"Practicality Scores by Response Type\")\n",
    "    plt.xlabel(\"Question ID\")\n",
    "    plt.ylabel(\"Practicality Score\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539df397-dcd8-43f2-9001-ae8de9903e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify questions\n",
    "# add full_COT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
