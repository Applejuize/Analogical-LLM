{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f00062d-0f67-46b5-aca0-b7857d11c4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing import TypedDict\n",
    "import openai \n",
    "# from langgraph_supervisor import create_supervisor\n",
    "from langchain.chat_models import init_chat_model\n",
    "from textwrap import dedent\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9c75eb6-69bf-40c9-8ed4-8b96c4d1f285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts\n",
    "TARGET_DOMAIN = dedent(\"\"\"\n",
    "        As a Domain Analysis Specialist, extract all the core innovation domains from the user query. It could be a single one for a simple query, or multiple ones for a complex query.\n",
    "        Instructions:\n",
    "        1. Analyze the user's input\n",
    "        2. Identify the primary domain(s) requiring innovation\n",
    "        3. Classify it within standard innovation categories\n",
    "        Output Format:\n",
    "        Target Domain: [Clear, specific domain label]\n",
    "        Be very detailed and specific in your response and do not generalize. Respond ONLY with the name of the domain, do NOT include ANY other text like 'Target Domain:'.\n",
    "        User query: {user_query}\n",
    "\"\"\").strip()\n",
    "\n",
    "PROBLEM_LANDSCAPE = dedent(\"\"\"\n",
    "        You are a Problem Landscape Analyst. Your task is to map out the concrete challenges within the target domain identified.\n",
    "        Instructions:\n",
    "        1. Identify all the core problems or challenges currently present in these domains. Aim for at least 3 problems per domain\n",
    "        2. For each problem, provide:\n",
    "        - Problem: A short, clear title.\n",
    "        - Description: 2-3 sentences explaining what the problem is and why it matters.\n",
    "        - Context: Briefly state the circumstances or environment where this problem occurs.\n",
    "        - Stakeholders: List the main groups or individuals affected.\n",
    "        - Root Causes: Identify 1-3 underlying causes, if known.\n",
    "        - Impact: State the significance of the problem (e.g., social, economic, technical).\n",
    "        - Current Approaches: How is this problem currently addressed?\n",
    "        - Limitations: What are the shortcomings of current approaches?\n",
    "        - Success Metrics: How would you measure if this problem is solved?\n",
    "        - Interconnections: Note if this problem is linked to or influenced by other problems.\n",
    "        Output Format:\n",
    "        Present your findings as a structured list or JSON array, with each problem fully described as above.\n",
    "        Important:\n",
    "        - Focus on clarity and completeness.\n",
    "        - Avoid abstracting or generalizing; stay concrete and domain-specific.\n",
    "        - Do not propose solutions; only describe the current problem landscape.\n",
    "        Target domain: {target_domain}\n",
    "\"\"\").strip()\n",
    "\n",
    "ABSTRACTION = dedent(\"\"\"\n",
    "You are a TRIZ Methodology Expert. Transform domain-specific problems into universal contradictions.\n",
    "        Process:\n",
    "        1. Read up on TRIZ - the contradiction matrix, and the inventive principles\n",
    "        2. For each problem in the problem landscape:\n",
    "        - Abstract to universal parameters (what improves vs. what worsens)\n",
    "        - Express as 'When we improve X, Y worsens'\n",
    "        - Ensure parameters are domain-agnostic\n",
    "        3. Analyze all the abstracted universal parameters, and identify all the core TRIZ contradictions present:\n",
    "        - Select the most fundamental tensions\n",
    "        - Map to TRIZ contradiction matrix\n",
    "        - Note applicable inventive principles\n",
    "        Output:\n",
    "        # List all the core contradictions in form of:\n",
    "        - Improving [parameter] vs. Worsening [parameter]\n",
    "        - TRIZ Principles: [1-3 relevant principles]\n",
    "        - Innovation Potential: [High/Medium/Low]\n",
    "        Focus on contradictions that, if resolved, would create breakthrough value.\n",
    "\n",
    "        Problem landscape: {problem_landscape}\n",
    "\"\"\").strip()\n",
    "\n",
    "BASE_DOMAIN = dedent(\"\"\"\n",
    "        You are a Cross-Domain Search Specialist. Do the following:\n",
    "        - For each contradiction provided, identify 3 distinct source domains (fields or industries) where this contradiction has been successfully addressed.\n",
    "        - Experiment with different subsets of the list of contradictions, and see if you could identify 3 distinct source domains for each of these subsets identified as well. You should find at least 3 different subsets.\n",
    "        Note: The domains should have A CONCEPTUAL DISTANCE OF AT LEAST 3 DISTINCT HOPS FROM WHAT IMMEDIATELY COMES TO MIND. Be creative! It can be domains within spheres like natural, phsyical, social, artistic, or anything.\n",
    "        For each domain identified, briefly explain why it is relevant to the single contradiction or the subset of contradictions identified. Do not describe specific solutions just yet-only list the domains and your rationale.\n",
    "        Output:\n",
    "        A list for each contradiction and subset of contradictions identified, naming 3 relevant domains with a 2 sentence rationale for each.\n",
    "        Aim for a total of at least 20 relevant base domains. \n",
    "        Contradictions: {contradictions}\n",
    "\"\"\").strip()\n",
    "\n",
    "BASE_SOLUTIONS = dedent(\"\"\"\n",
    "        You are a Solution Pattern Extractor. You are provided with an input with 3 base domains identified per TRIZ (Theory of Inventive Problem Solving) contradiction or a set of contradictions, as well as the contradictions themselves.\n",
    "        For each of these identified base domains, identify one specific, well-documented solution pattern within the domain that effectively resolves the contradiction (or the set of contradictions).\n",
    "        For each solution pattern, return:\n",
    "        - Identify the base domain it's corresponding to\n",
    "        - Recall the contradiction or the set of contradictions that this base domain faces\n",
    "        - The name or label of the solution pattern for resolving these contradiction(s) in the base domain\n",
    "        - A detailed description of the core mechanism or principle involved and how it addressed the domain's contradiction(s)\n",
    "        - The context or situation in the domain where this pattern is applied\\n\"\n",
    "        Do not generalize or adapt the solution-simply describe how the contradiction is addressed within each source domain.\n",
    "        Output:\n",
    "        For each of the provided domain, list the base domain name, contradiction(s) faced, solution pattern name, the detailed description of the mechanism of the solution pattern, and the context in which it is used. Articulate the contradictions as problems and considerations faced, through framing them as a tension.\n",
    "\n",
    "        Input: {input}\n",
    "\"\"\").strip()\n",
    "\n",
    "ANALOGICAL_TRANSFER = dedent(\"\"\"\n",
    "        You are a very innovative Analogical Transfer Specialist.\n",
    "        You are provided with list the base domain name, tensions faced, solution pattern name, the detailed description of the mechanism of the solution pattern, and the context in which it is used.\n",
    "        Your task is to propose how solution patterns used to resolve these tensions in various base domains might inspire solution framings for the original target domain.\n",
    "\n",
    "        Input Overview:\n",
    "        1. A list the base domains identified, the tensions these domains faced, the name of solution patterns that helped addressed these tensions in these base domains, the detailed description of the mechanism of the solution pattern, and the context in which it is used.\n",
    "        2. The original target domain.\n",
    "\n",
    "        Instructions:\n",
    "        For each pair of base domain and the corresponding tensions identified, review the solution patterns that worked for the base domain. For each pattern:\n",
    "        - Analyze the core mechanism or principle behind the solution.\n",
    "        - Map and adapt this mechanism conceptually to the target domain, considering the specific context and needs of the target domain.\n",
    "        - Clearly describe how this analogical transfer could frame a potential solution in the target domain.\n",
    "        - Highlight any key adaptations, considerations, or limitations that would be relevant when applying this pattern to the target domain.\n",
    "\n",
    "        Your expected Output:\n",
    "        For each base domain, provide a comprehensive description of a proposed solution framing for the target domain, including:\n",
    "        - The original tension addressed\n",
    "        - The source domain and solution pattern\n",
    "        - A detailed explanation of how the pattern could inspire or inform a solution in the target domain\n",
    "        - Any important adaptations or considerations for successful transfer\n",
    "\n",
    "        Here are the actualinputs:\n",
    "        - A list the base domains identified, the tensions these domains faced, the name of solution patterns that helped addressed these tensions in these base domains, the detailed description of the mechanism of the solution pattern, and the context in which it is used.: {contradictions_solutions}\n",
    "        - Original target domain: {target_domain}\n",
    "\"\"\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e573d7e-e43f-4c7d-8752-ae11a6198c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for openai API\n",
    "def _set_if_undefined(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"Please provide your {var}\")\n",
    "\n",
    "\n",
    "_set_if_undefined(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8d2d7f38-3881-4d8d-80fc-542b5918d14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse_output\n",
    "import re\n",
    "\n",
    "def parse_solution(text):\n",
    "    # Remove Markdown formatting like \"**\" and \"\\n\"\n",
    "    clean_text = text.replace(\"**\", \"\").replace(\"\\\\n\", \"\\n\")\n",
    "\n",
    "    # Split into sections by horizontal rules (---)\n",
    "    sections = re.split(r'-{3,}', clean_text)\n",
    "\n",
    "    # Create a readable version of each section\n",
    "    readable_output = []\n",
    "    for section in sections:\n",
    "        section = section.strip()\n",
    "        if not section:\n",
    "            continue\n",
    "        # Optional: separate title and body if present\n",
    "        lines = section.split(\"\\n\")\n",
    "        if len(lines) > 1 and \":\" not in lines[0]:\n",
    "            # First line is likely a heading\n",
    "            heading = lines[0]\n",
    "            body = \"\\n\".join(lines[1:])\n",
    "            readable_output.append(f\"\\n=== {heading} ===\\n{body}\")\n",
    "        else:\n",
    "            readable_output.append(section)\n",
    "\n",
    "    return \"\\n\\n\".join(readable_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a188a9e3-c538-487f-9dde-8dfe295575db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph state: workflow\n",
    "class ReasoningState(TypedDict):\n",
    "    user_query: str\n",
    "    target_domain: str\n",
    "    problem_landscape: str\n",
    "    abstraction: str\n",
    "    abstraction_feedback: str\n",
    "    base_domain: str\n",
    "    base_solutions: str\n",
    "    analogical_transfer: str\n",
    "    transfer_feedback: str\n",
    "    solution: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "306439ab-4b94-4084-8a1c-8c53ecf1a0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can switch between different LLMs \n",
    "llms = init_chat_model(\"openai:gpt-4.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54ea64a9-96a1-42ab-bb3d-1a1c1d14b1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st Agent: Identify Target Domain from User Input\n",
    "def target_domain_agent(state: ReasoningState):\n",
    "    u = state['user_query']\n",
    "\n",
    "    msg = llms.invoke(TARGET_DOMAIN.format(user_query=u))\n",
    "\n",
    "    return {\"target_domain\": msg.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d327c06e-9618-4463-b19e-2be74a21d14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2st Agent: Conduct comprehensive research into problem landscape for target domain \n",
    "# (i.e. What specific challenges exist in this domain?)\n",
    "def problem_landscape_agent(state: ReasoningState):\n",
    "    t = state['target_domain']\n",
    "\n",
    "    msg = llms.invoke(PROBLEM_LANDSCAPE.format(target_domain=t))\n",
    "\n",
    "    return {\"problem_landscape\": msg.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26b96ec9-7bdb-4803-9bab-dcc82d6a4c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3rd Agent: Abstract Problems Identified into Generalized Principles and TRIZ Contradiction\n",
    "def abstraction_agent(state: ReasoningState):\n",
    "    p = state['problem_landscape']\n",
    "    feedback = state.get('abstraction_feedback', '')\n",
    "    \n",
    "    # If there's feedback, include it in the prompt\n",
    "    if feedback:\n",
    "        msg = llms.invoke(ABSTRACTION.format(\n",
    "            problem_landscape=p,\n",
    "            feedback=f\"\\nPrevious attempt feedback: {feedback}\\nPlease address these issues in your new abstraction.\"\n",
    "        ))\n",
    "    else:\n",
    "        msg = llms.invoke(ABSTRACTION.format(problem_landscape=p))\n",
    "    \n",
    "    return {\"abstraction\": msg.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b63464e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3.5th agent: CRIT Quality Control for TRIZ Abstraction\n",
    "def CRIT_Control_Abstraction(target_domain: str, problem_landscape: str, triz_contradictions: str) -> dict:\n",
    "    \"\"\"Quality control function that evaluates the abstraction output.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "        # CRITICAL READING INQUISITIVE TEMPLATE (CRIT) - TRIZ VALIDATION\n",
    "        You are a TRIZ ABSTRACTION VALIDATOR. Apply Socratic methods to evaluate contradiction quality.\n",
    "\n",
    "        ## INPUTS\n",
    "        Target Domain: {target_domain}\n",
    "        Problem Landscape: {problem_landscape}\n",
    "        TRIZ Contradictions: {triz_contradictions}\n",
    "\n",
    "        ## VALIDATION PROTOCOL [Pass Threshold: 7.0/10]\n",
    "\n",
    "        ### 1. METHOD OF DEFINITION\n",
    "        - Does each contradiction follow \"Improving [X] vs. Worsening [Y]\" format?\n",
    "        - Are parameters from Altshuller's 39 Engineering Parameters?\n",
    "        - Is abstraction at first-principles level (not solution-biased)?\n",
    "\n",
    "        ### 2. METHOD OF ELENCHUS (Cross-Examination)\n",
    "        - Trace each contradiction back to originating problem\n",
    "        - Verify causal chain: Problem → Root Cause → Contradiction\n",
    "        - Rate abstraction mapping strength (1-10)\n",
    "\n",
    "        ### 3. METHOD OF DIALECTIC (Counter-Arguments)\n",
    "        - Generate 2-3 alternative contradictions for same problem\n",
    "        - Compare which better captures essential tension\n",
    "        - Eliminate weaker contradictions\n",
    "\n",
    "        ### 4. SCORING CRITERIA\n",
    "        - **TRIZ Compliance (40%):** Format, parameter accuracy\n",
    "        - **Abstraction Quality (35%):** First-principles, non-solution-biased\n",
    "        - **Problem Coverage (25%):** All major problems represented\n",
    "\n",
    "        ### OUTPUT FORMAT\n",
    "        {{\n",
    "            \"score\": float (1.0-10.0),\n",
    "            \"pass\": boolean,\n",
    "            \"reason\": str (brief explanation if fail)\n",
    "        }}\n",
    "\n",
    "        **FAILURE CONDITIONS:** Format violations, non-TRIZ parameters, solution bias, logical gaps, incomplete coverage.\n",
    "        \n",
    "        IMPORTANT: Your response must be a valid JSON object with the exact keys shown above.\n",
    "    \"\"\"\n",
    "    response = llms.invoke(prompt)\n",
    "    \n",
    "    # Parse the response into a dictionary\n",
    "    try:\n",
    "        # Extract JSON from the response\n",
    "        response_text = response.content.strip()\n",
    "        # Find the JSON object in the response\n",
    "        start_idx = response_text.find('{')\n",
    "        end_idx = response_text.rfind('}') + 1\n",
    "        if start_idx >= 0 and end_idx > start_idx:\n",
    "            json_str = response_text[start_idx:end_idx]\n",
    "            result = json.loads(json_str)\n",
    "            return result\n",
    "        else:\n",
    "            # If no JSON found, return a default fail response\n",
    "            return {\n",
    "                \"score\": 0.0,\n",
    "                \"pass\": False,\n",
    "                \"reason\": \"Invalid response format from validator\"\n",
    "            }\n",
    "    except json.JSONDecodeError:\n",
    "        # If JSON parsing fails, return a default fail response\n",
    "        return {\n",
    "            \"score\": 0.0,\n",
    "            \"pass\": False,\n",
    "            \"reason\": \"Failed to parse validator response\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a230bf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3.5th Agent Logic Flow: if quality control passes, continue to next agent. Else, loop back to retry TRIZ abstraction (taking rationale for why it failed as additional input)\n",
    "def should_continue_to_base(state: ReasoningState) -> str:\n",
    "    \"\"\"Determine whether to continue to base domain or retry abstraction based on CRIT score.\"\"\"\n",
    "    # Get the CRIT score from the state\n",
    "    crit_result = CRIT_Control_Abstraction(\n",
    "        state['target_domain'],\n",
    "        state['problem_landscape'],\n",
    "        state['abstraction']\n",
    "    )\n",
    "    \n",
    "    # Parse the result to get the score and pass status\n",
    "    score = crit_result.get('score', 0)\n",
    "    passed = crit_result.get('pass', False)\n",
    "    \n",
    "    if passed:\n",
    "        return \"base\"  # Continue to base domain agent\n",
    "    else:\n",
    "        # Add the feedback to the state for the abstraction agent to use\n",
    "        state['abstraction_feedback'] = crit_result.get('reason', '')\n",
    "        return \"abstract\"  # Retry abstraction agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e95a5d2f-1773-4339-ba89-995d185885c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4th Agent: Search for Appropriate Base Domains\n",
    "def base_domain_agent(state: ReasoningState):\n",
    "    a = state['abstraction']\n",
    "\n",
    "    msg = llms.invoke(BASE_DOMAIN.format(contradictions=a))\n",
    "\n",
    "    return {\"base_domain\": msg.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7db11f3-e388-48b4-8661-cde636b1ba50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5th Agent: Identify Solution in Base Domain\n",
    "def base_solution_agent(state: ReasoningState):\n",
    "    b = state['base_domain']\n",
    "\n",
    "    msg = llms.invoke(BASE_SOLUTIONS.format(input=b))\n",
    "\n",
    "    return {\"base_solutions\": msg.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5dc1b395-b154-493e-9c57-7e5a6926cf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6th Agent: Base Domain Solution Informing Target Domain Solution\n",
    "def analogical_transfer_agent(state: ReasoningState):\n",
    "    if \"base_solutions\" not in state:\n",
    "        raise ValueError(\"Missing 'base_solutions' key. Check if previous node returned it.\")\n",
    "    b = state['base_solutions']\n",
    "    t = state['target_domain']\n",
    "    feedback = state.get('transfer_feedback', '')\n",
    "    \n",
    "    # If there's feedback, include it in the prompt\n",
    "    if feedback:\n",
    "        msg = llms.invoke(ANALOGICAL_TRANSFER.format(\n",
    "            contradictions_solutions=b,\n",
    "            target_domain=t,\n",
    "            feedback=f\"\\nPrevious attempt feedback: {feedback}\\nPlease address these issues in your new analogical transfer.\"\n",
    "        ))\n",
    "    else:\n",
    "        msg = llms.invoke(ANALOGICAL_TRANSFER.format(\n",
    "            contradictions_solutions=b,\n",
    "            target_domain=t\n",
    "        ))\n",
    "    \n",
    "    return {\"analogical_transfer\": msg.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8dc7e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.5th Agent: CRIT Quality Control for Analogical Reasoning \n",
    "def CRIT_Control_Analogical_Transfer(base_solutions: str, analogical_transfers: str, original_contradictions: str, target_domain: str) -> dict:\n",
    "    prompt = f\"\"\"\n",
    "    # CRITICAL READING INQUISITIVE TEMPLATE (CRIT) - ANALOGICAL TRANSFER VALIDATION\n",
    "    You are an ANALOGICAL TRANSFER VALIDATOR. Apply Socratic methods to evaluate transfer quality.\n",
    "\n",
    "    ## INPUTS\n",
    "    Base Solutions: {base_solutions}\n",
    "    Analogical Transfers: {analogical_transfers}\n",
    "    Original Contradictions: {original_contradictions}\n",
    "    Target Domain: {target_domain}\n",
    "\n",
    "    ## VALIDATION PROTOCOL [Pass Threshold: 7.5/10]\n",
    "\n",
    "    ### 1. METHOD OF ELENCHUS (Cross-Examination)\n",
    "    - Does each transfer preserve the CORE MECHANISM from base domain?\n",
    "    - Trace: Base Solution → Transfer Logic → Target Application\n",
    "    - Rate mechanism preservation fidelity (1-10)\n",
    "\n",
    "    ### 2. METHOD OF DIALECTIC (Counter-Arguments)\n",
    "    - Generate 2-3 alternative transfer approaches for same base solution\n",
    "    - Identify potential failure modes in target domain\n",
    "    - Test against target domain constraints\n",
    "\n",
    "    ### 3. METHOD OF COUNTERFACTUAL\n",
    "    - \"What if this transfer were applied in [alternative context]?\"\n",
    "    - Predict 2-3 unintended consequences in target domain\n",
    "    - Validate scalability and implementation feasibility\n",
    "\n",
    "    ### 4. SCORING CRITERIA\n",
    "    - **Mechanism Fidelity (35%):** Core solution pattern preserved\n",
    "    - **Domain Adaptation (30%):** Properly adapted to target constraints  \n",
    "    - **Contradiction Resolution (25%):** Addresses original TRIZ tensions\n",
    "    - **Implementation Feasibility (10%):** Realistic in target context\n",
    "\n",
    "    ### OUTPUT FORMAT\n",
    "    {{\n",
    "        \"score\": float (1.0-10.0),\n",
    "        \"pass\": boolean,\n",
    "        \"reason\": str (brief explanation if fail)\n",
    "    }}\n",
    "\n",
    "    **FAILURE CONDITIONS:** Mechanism distortion, poor domain adaptation, contradiction mismatch, implementation impossibility.\n",
    "    \n",
    "    IMPORTANT: Your response must be a valid JSON object with the exact keys shown above.\n",
    "    \"\"\"\n",
    "    response = llms.invoke(prompt)\n",
    "    \n",
    "    # Parse the response into a dictionary\n",
    "    try:\n",
    "        # Extract JSON from the response\n",
    "        response_text = response.content.strip()\n",
    "        # Find the JSON object in the response\n",
    "        start_idx = response_text.find('{')\n",
    "        end_idx = response_text.rfind('}') + 1\n",
    "        if start_idx >= 0 and end_idx > start_idx:\n",
    "            json_str = response_text[start_idx:end_idx]\n",
    "            result = json.loads(json_str)\n",
    "            return result\n",
    "        else:\n",
    "            # If no JSON found, return a default fail response\n",
    "            return {\n",
    "                \"score\": 0.0,\n",
    "                \"pass\": False,\n",
    "                \"reason\": \"Invalid response format from validator\"\n",
    "            }\n",
    "    except json.JSONDecodeError:\n",
    "        # If JSON parsing fails, return a default fail response\n",
    "        return {\n",
    "            \"score\": 0.0,\n",
    "            \"pass\": False,\n",
    "            \"reason\": \"Failed to parse validator response\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "faecca01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.5th Agent Logic Flow: \n",
    "def should_continue_to_synthesis(state: ReasoningState) -> str:\n",
    "    \"\"\"Determine whether to continue to synthesis or retry analogical transfer based on CRIT score.\"\"\"\n",
    "    crit_result = CRIT_Control_Analogical_Transfer(\n",
    "        state['base_solutions'],\n",
    "        state['analogical_transfer'], \n",
    "        state['abstraction'],\n",
    "        state['target_domain']\n",
    "    )\n",
    "\n",
    "    score = crit_result.get('score', 0)\n",
    "    passed = crit_result.get('pass', False)\n",
    "    \n",
    "    if passed:\n",
    "        return \"synthesis\"  # Continue to final synthesis\n",
    "    else:\n",
    "        state['transfer_feedback'] = crit_result.get('reason', '')\n",
    "        return \"analogical_transfer\"  # Retry transfer with feedback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3626041-fcd5-4bf3-a32b-f164352e788f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7th Agent: Summarize everything and respond to the question\n",
    "def synthesis_agent(state: ReasoningState):\n",
    "    msg = llms.invoke(\n",
    "        f\"Evaluate the proposed analogical solutions. Find the best ones that balances practicality with innovation. Then, provide a detailed, well-structured response that addresses all aspects of the query.\\n\\n\"\n",
    "        f\"Problem: {state['user_query']}\\n\"\n",
    "        f\"Analogical Solutions: {state['analogical_transfer']}\"\n",
    "        f\"In your output, remember to abstract away the analogy itself such that it is focused on responding to the user input.\"\n",
    "        f\"Also, check if the users are requesting a specific number of possible solutions. Make sure to answer the user's query in full and provide what is requested.\"\n",
    "    )\n",
    "    return {\"solution\": msg.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68994430-5388-4c63-84be-919d5978d5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the workflow \n",
    "workflow = StateGraph(ReasoningState)\n",
    "\n",
    "workflow.add_node(\"target\", target_domain_agent)\n",
    "workflow.add_node(\"landscape\", problem_landscape_agent)\n",
    "workflow.add_node(\"abstract\", abstraction_agent)\n",
    "workflow.add_node(\"base\", base_domain_agent)\n",
    "workflow.add_node(\"base_soln\", base_solution_agent)\n",
    "workflow.add_node(\"analogy\", analogical_transfer_agent)\n",
    "workflow.add_node(\"synthesis\", synthesis_agent)\n",
    "\n",
    "workflow.set_entry_point(\"target\")\n",
    "\n",
    "# Define edges\n",
    "workflow.add_edge(\"target\", \"landscape\")\n",
    "workflow.add_edge(\"landscape\", \"abstract\")\n",
    "# workflow.add_edge(\"abstract\", \"base\")\n",
    "\n",
    "# Define conditional edges - Use CRIT TO control quality for Abstraction TRIZ\n",
    "workflow.add_conditional_edges(\n",
    "    \"abstract\",\n",
    "    should_continue_to_base,\n",
    "    path_map=[\"base\", \"abstract\"]\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"base\", \"base_soln\")\n",
    "workflow.add_edge(\"base_soln\", \"analogy\")\n",
    "#workflow.add_edge(\"analogy\", \"synthesis\")\n",
    "\n",
    "# Define conditional edges - Use CRIT TO control quality for Analogical Transfer\n",
    "workflow.add_conditional_edges(\n",
    "    \"analogy\",\n",
    "    should_continue_to_synthesis,\n",
    "    path_map=[\"synthesis\", \"analogy\"]\n",
    ")\n",
    "\n",
    "workflow.set_finish_point(\"synthesis\")\n",
    "\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b178c9ea-3768-44c1-8461-01c0e93ef1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogical_output(user_input: str) -> str:\n",
    "    input_state = {\"user_query\": user_input}\n",
    "\n",
    "    final_state = graph.invoke(input_state)\n",
    "\n",
    "    for key in final_state:\n",
    "        raw_output = str(final_state[key])  # Ensure it's a string\n",
    "        final_output = parse_solution(raw_output)\n",
    "\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea24580",
   "metadata": {},
   "source": [
    "### Initialize Different Functions for Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1ed3fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Output\n",
    "def basic_output(user_input: str):\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    {user_input}\n",
    "    \"\"\"\n",
    "\n",
    "    response = llms.invoke(prompt) \n",
    "\n",
    "    # Parse and format the response\n",
    "    formatted_response = parse_solution(response.content)\n",
    "\n",
    "    return(formatted_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "174da814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Output with COT\n",
    "## TO EDIT WITH FULL PROMPT?\n",
    "def basic_output_COT(user_input: str):\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    {user_input}\n",
    "    Think step by step.\n",
    "    \"\"\"\n",
    "\n",
    "    response = llms.invoke(prompt) \n",
    "\n",
    "    # Parse and format the response\n",
    "    formatted_response = parse_solution(response.content)\n",
    "\n",
    "    return(formatted_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d4f4149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Output with Prompt Engineering\n",
    "def basic_output_prompt_engin(user_input: str):\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    {user_input}\n",
    "    Try to balance practicality with innovation.\n",
    "    \"\"\"\n",
    "\n",
    "    response = llms.invoke(prompt) \n",
    "\n",
    "    # Parse and format the response\n",
    "    formatted_response = parse_solution(response.content)\n",
    "\n",
    "    return(formatted_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fe3b3f1-ed91-4fb9-a391-215466a0ccae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_output_full_COT(user_input: str):\n",
    "    prompt = f\"\"\"\n",
    "    {user_input}\n",
    "    Step one: As a Domain Analysis Specialist, extract all the core innovation domains from the user query. It could be a single one for a simple query, or multiple ones for a complex query.\n",
    "        Instructions:\n",
    "        1. Analyze the user's input\n",
    "        2. Identify the primary domain(s) requiring innovation\n",
    "        3. Classify it within standard innovation categories\n",
    "        Output Format:\n",
    "        Target Domain: [Clear, specific domain label]\n",
    "        Be very detailed and specific in your response and do not generalize. Respond ONLY with the name of the domain, do NOT include ANY other text like 'Target Domain:'.\n",
    "    Step two: You are a Problem Landscape Analyst. Your task is to map out the concrete challenges within the target domain identified.\n",
    "        Instructions:\n",
    "        1. Identify all the core problems or challenges currently present in these domains. Aim for at least 3 problems per domain\n",
    "        2. For each problem, provide:\n",
    "        - Problem: A short, clear title.\n",
    "        - Description: 2-3 sentences explaining what the problem is and why it matters.\n",
    "        - Context: Briefly state the circumstances or environment where this problem occurs.\n",
    "        - Stakeholders: List the main groups or individuals affected.\n",
    "        - Root Causes: Identify 1-3 underlying causes, if known.\n",
    "        - Impact: State the significance of the problem (e.g., social, economic, technical).\n",
    "        - Current Approaches: How is this problem currently addressed?\n",
    "        - Limitations: What are the shortcomings of current approaches?\n",
    "        - Success Metrics: How would you measure if this problem is solved?\n",
    "        - Interconnections: Note if this problem is linked to or influenced by other problems.\n",
    "        Output Format:\n",
    "        Present your findings as a structured list or JSON array, with each problem fully described as above.\n",
    "        Important:\n",
    "        - Focus on clarity and completeness.\n",
    "        - Avoid abstracting or generalizing; stay concrete and domain-specific.\n",
    "        - Do not propose solutions; only describe the current problem landscape.\n",
    "    Step three: You are a TRIZ Methodology Expert. Transform domain-specific problems into universal contradictions.\n",
    "        Process:\n",
    "        1. Read up on TRIZ - the contradiction matrix, and the inventive principles\n",
    "        2. For each problem in the problem landscape:\n",
    "        - Abstract to universal parameters (what improves vs. what worsens)\n",
    "        - Express as 'When we improve X, Y worsens'\n",
    "        - Ensure parameters are domain-agnostic\n",
    "        3. Analyze all the abstracted universal parameters, and identify all the core TRIZ contradictions present:\n",
    "        - Select the most fundamental tensions\n",
    "        - Map to TRIZ contradiction matrix\n",
    "        - Note applicable inventive principles\n",
    "        Output:\n",
    "        # List all the core contradictions in form of:\n",
    "        - Improving [parameter] vs. Worsening [parameter]\n",
    "        - TRIZ Principles: [1-3 relevant principles]\n",
    "        - Innovation Potential: [High/Medium/Low]\n",
    "        Focus on contradictions that, if resolved, would create breakthrough value.\n",
    "    Step four: You are a Cross-Domain Search Specialist. Do the following:\n",
    "        - For each contradiction provided, identify 3 distinct source domains (fields or industries) where this contradiction has been successfully addressed.\n",
    "        - Experiment with different subsets of the list of contradictions, and see if you could identify 3 distinct source domains for each of these subsets identified as well. You should find at least 3 different subsets.\n",
    "        Note: The domains should have A CONCEPTUAL DISTANCE OF AT LEAST 3 DISTINCT HOPS FROM WHAT IMMEDIATELY COMES TO MIND. Be creative! It can be domains within spheres like natural, phsyical, social, artistic, or anything.\n",
    "        For each domain identified, briefly explain why it is relevant to the single contradiction or the subset of contradictions identified. Do not describe specific solutions just yet-only list the domains and your rationale.\n",
    "        Output:\n",
    "        A list for each contradiction and subset of contradictions identified, naming 3 relevant domains with a 2 sentence rationale for each.\n",
    "        Aim for a total of at least 20 relevant base domains. \n",
    "    Step five: You are a Solution Pattern Extractor. You are provided with an input with 3 base domains identified per TRIZ (Theory of Inventive Problem Solving) contradiction or a set of contradictions, as well as the contradictions themselves.\n",
    "        For each of these identified base domains, identify one specific, well-documented solution pattern within the domain that effectively resolves the contradiction (or the set of contradictions).\n",
    "        For each solution pattern, return:\n",
    "        - Identify the base domain it's corresponding to\n",
    "        - Recall the contradiction or the set of contradictions that this base domain faces\n",
    "        - The name or label of the solution pattern for resolving these contradiction(s) in the base domain\n",
    "        - A detailed description of the core mechanism or principle involved and how it addressed the domain's contradiction(s)\n",
    "        - The context or situation in the domain where this pattern is applied\\n\"\n",
    "        Do not generalize or adapt the solution-simply describe how the contradiction is addressed within each source domain.\n",
    "        Output:\n",
    "        For each of the provided domain, list the base domain name, contradiction(s) faced, solution pattern name, the detailed description of the mechanism of the solution pattern, and the context in which it is used. Articulate the contradictions as problems and considerations faced, through framing them as a tension.\n",
    "    Step six:         You are a very innovative Analogical Transfer Specialist.\n",
    "        You are provided with list the base domain name, tensions faced, solution pattern name, the detailed description of the mechanism of the solution pattern, and the context in which it is used.\n",
    "        Your task is to propose how solution patterns used to resolve these tensions in various base domains might inspire solution framings for the original target domain.\n",
    "\n",
    "        Input Overview:\n",
    "        1. A list the base domains identified, the tensions these domains faced, the name of solution patterns that helped addressed these tensions in these base domains, the detailed description of the mechanism of the solution pattern, and the context in which it is used.\n",
    "        2. The original target domain.\n",
    "\n",
    "        Instructions:\n",
    "        For each pair of base domain and the corresponding tensions identified, review the solution patterns that worked for the base domain. For each pattern:\n",
    "        - Analyze the core mechanism or principle behind the solution.\n",
    "        - Map and adapt this mechanism conceptually to the target domain, considering the specific context and needs of the target domain.\n",
    "        - Clearly describe how this analogical transfer could frame a potential solution in the target domain.\n",
    "        - Highlight any key adaptations, considerations, or limitations that would be relevant when applying this pattern to the target domain.\n",
    "\n",
    "        Your expected Output:\n",
    "        For each base domain, provide a comprehensive description of a proposed solution framing for the target domain, including:\n",
    "        - The original tension addressed\n",
    "        - The source domain and solution pattern\n",
    "        - A detailed explanation of how the pattern could inspire or inform a solution in the target domain\n",
    "        - Any important adaptations or considerations for successful transfer\n",
    "    Step seven: Evaluate the proposed analogical solutions. Find the best ones that balances practicality with innovation. Then, provide a detailed, well-structured response that addresses all aspects of the query.\n",
    "        In your output, remember to abstract away the analogy itself such that it is focused on responding to the user input.\n",
    "        Also, check if the users are requesting a specific number of possible solutions. Make sure to answer the user's query in full and provide what is requested.   \n",
    "    \"\"\"\n",
    "    response = llms.invoke(prompt) \n",
    "\n",
    "    # Parse and format the response\n",
    "    formatted_response = parse_solution(response.content)\n",
    "\n",
    "    return(formatted_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7fe14c",
   "metadata": {},
   "source": [
    "### Test prompt 1 - Education"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6fb723",
   "metadata": {},
   "source": [
    "#### User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "848e42be",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_q = \"Teachers introduced structured group projects using peer evaluations and rotating roles to boost collaboration in science classes. Despite these measures, anonymous student surveys revealed that individual quiz scores post-project dropped compared to solo assignments, signaling diluted accountability. How can educators redesign design a system to ensure measurable individual mastery while preserving the benefits of teamwork?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cd473f",
   "metadata": {},
   "source": [
    "#### With Analogical Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3414cb06-b846-4979-99ef-be697a56101b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = analogical_output(test_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d3c50c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save all parsed outputs to a text file\n",
    "# with open(\"sample_output.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     for key in final_state:\n",
    "#         raw_output = str(final_state[key])  # Ensure it's a string\n",
    "#         final_output = parse_solution(raw_output)\n",
    "#         f.write(f\"\\n### {key} ###\\n\")\n",
    "#         f.write(final_output)\n",
    "#         f.write(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fc3c62",
   "metadata": {},
   "source": [
    "#### Without Analogical Reasoning - GPT 4.1 RAW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a6aa161c-0873-49c4-9bcc-85d644311d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GPT-4.1 RAW Response ===\n",
      "\n",
      "This is a thoughtful and common challenge in collaborative learning: preserving the benefits of teamwork (communication, collaboration, deeper thinking) and ensuring individual mastery and accountability.\n",
      "\n",
      "Here are targeted strategies educators can implement:\n",
      "\n",
      "\n",
      "=== ### 1. Hybrid Grading Approach ===\n",
      "- Dual Components: Make the final project grade a combination of group and individual assessments (e.g., 50% group outcome, 50% individual evidence).\n",
      "- Individual Quizzes or Reflections: Require each student to complete a quiz, written reflection, or oral explanation showing their understanding of the key science concepts behind the project.\n",
      "\n",
      "### 2. Role-Specific Deliverables\n",
      "- When rotating roles, assign each student role-specific, individual deliverables. For example, if one student is the \"researcher,\" they must individually summarize sources or findings.\n",
      "- Collect and grade these outputs separately to verify individual understanding.\n",
      "\n",
      "### 3. Regular Formative Check-ins\n",
      "- Schedule short, individual check-ins (exit tickets, clicker questions, journal entries) throughout the project timeline on fundamental concepts.\n",
      "- Use these low-stakes assessments to track progress and address gaps early.\n",
      "\n",
      "### 4. Randomized Oral or Written Defense\n",
      "- After project completion, randomly select students (or all students, if feasible) to give a brief, individual explanation of a portion of the project or answer follow-up questions.\n",
      "- This encourages personal responsibility for understanding the entirety of the project.\n",
      "\n",
      "### 5. Peer Teaching Moments\n",
      "- Include sessions where students must teach a segment of the project material to a peer or small group and are assessed on clarity and accuracy. Peer-teaching boosts accountability for individual learning.\n",
      "\n",
      "### 6. Transparent Criteria\n",
      "- Use rubrics that clearly separate teamwork skills (collaboration, contribution, communication) from content mastery, and make expectations explicit to students.\n",
      "\n",
      "### 7. Self and Peer Assessments Weighted for Accountability\n",
      "- Use calibrated self- and peer-evaluations (ideally, structured and not just subjective ratings) as just one part of the overall grade, not the entirety.\n",
      "- Combine with teacher verification or evidence-based checkpoints.\n",
      "\n",
      "Sample Redesigned System:\n",
      "| Assessment Type             | Weight | Description                                                                            |\n",
      "|\n",
      "\n",
      "|\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "=== | ===\n",
      "| Group Project Outcome       | 40%    | Graded as a team: product demonstrates effective collaboration and solid science        |\n",
      "| Individual Quiz/Reflection  | 30%    | Each student completes an assessment on key project science concepts                   |\n",
      "| Role-Specific Deliverable   | 20%    | Each student submits evidence of their unique contribution                             |\n",
      "| Peer/Self Assessment        | 10%    | Structured feedback on teamwork, professionalism, responsibility                       |\n",
      "\n",
      "Bottom Line:  \n",
      "Assess individual learning in parallel with group achievements.  \n",
      "This balance preserves collaboration’s power while guaranteeing everyone is learning. Consider explaining to students *why* both teamwork and individual mastery matter—explicit alignment can foster buy-in.\n",
      "\n",
      "If you’d like, share more about the exact subject/project, and I can provide a more tailored set of sample assignments or assessment formats!\n"
     ]
    }
   ],
   "source": [
    "## Without analogical reasoning - raw LLM output\n",
    "result = basic_output(test_q)\n",
    "\n",
    "print(\"\\n=== GPT-4.1 RAW Response ===\\n\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb46176",
   "metadata": {},
   "source": [
    "#### Without Analogical Reasoning but with COT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "15022384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GPT-4.1 Response WITH COT ===\n",
      "\n",
      "Absolutely—this is a great, nuanced question about balancing collaboration with individual mastery. Here’s a step-by-step approach educators can use:\n",
      "\n",
      "\n",
      "=== ### 1. Analyze the Problem ===\n",
      "\n",
      "- Observation: Teamwork improved via group projects, but individual accountability dropped—reflected by lower post-project quiz scores.\n",
      "- Challenge: Group work risks “social loafing” (some students do less), hiding who hasn’t mastered the content.\n",
      "\n",
      "\n",
      "=== ### 2. Clarify Goals ===\n",
      "\n",
      "- Maintain benefits of teamwork (communication, peer learning, problem-solving).\n",
      "- Ensure each student achieves and can demonstrate individual mastery.\n",
      "\n",
      "\n",
      "=== ### 3. Redesign Steps ===\n",
      "\n",
      "#### Step 1: Incorporate Regular Individual Accountability Within Groups\n",
      "\n",
      "- Frequent Individual Checks: Mix in *individual* exit tickets, checkpoints, or short quizzes during the project phase—not just at the end. This keeps everyone engaged and signals that individual mastery is always required.\n",
      "- Randomized Individual Interview or Defense: Occasionally (and unpredictably), have students individually defend or explain a project aspect to you or the class. This encourages participation and preparation.\n",
      "\n",
      "#### Step 2: Structure Projects for Individual Products Within a Collaborative Framework\n",
      "\n",
      "- Hybrid Deliverables: Design group projects with both a group product *and* an individual component (individual reflections, mini-reports, or analysis sections).  \n",
      "    - Example: Group develops and presents an experiment; each student writes their own analysis or conclusion.\n",
      "\n",
      "#### Step 3: Use Peer and Self-Assessment Strategically\n",
      "\n",
      "- Peer Assessment Tied to Learning Goals: Structure peer evaluations so they specifically address each member’s contribution and understanding of core concepts, not just effort/helpfulness.\n",
      "- Self-Assessment: Ask students to assess their learning and contributions, helping them self-identify gaps.\n",
      "\n",
      "#### Step 4: Scaffold Team Roles Towards Individual Accountability\n",
      "\n",
      "- Rotate Roles With Individual Ownership: Assign roles requiring each student to master certain topics, then test or quiz on that area individually.\n",
      "- Role-Based Quizzing: If a student was the “data analyst” in the group, quiz them specifically about data analysis methods used.\n",
      "\n",
      "#### Step 5: Separate Group and Individual Grading\n",
      "\n",
      "- Weighted Grades: Make a significant portion of the project grade individual (based on quizzes, reflections, interviews), and a portion group-based (final product, collaboration).\n",
      "    - Example: 60% individual, 40% group.\n",
      "\n",
      "#### Step 6: Build in Post-Project Individual Assessment and Remediation\n",
      "\n",
      "- Immediate Post-Project Quiz: Right after a project ends, give an individual quiz on the underlying concepts.\n",
      "- Targeted Remediation: For students who underperform, require additional (brief) individual assignments/reflection to close the gap.\n",
      "\n",
      "\n",
      "=== ### 4. Monitor and Adjust ===\n",
      "\n",
      "- Use Ongoing Data: Regularly compare individual assessments before and after implementing new systems to see if changes are effective.\n",
      "- Solicit Feedback: Keep open anonymous feedback channels for students about their sense of fairness and responsibility.\n",
      "\n",
      "\n",
      "=== ### Concrete Example Implementation ===\n",
      "\n",
      "Imagine a group builds a model rocket (project).  \n",
      "- During the build: Each student submits quick individual “explain what we’re doing and why” logs.\n",
      "- After the build: The group submits one launch report.\n",
      "- But: Each student writes *their own* analysis of why their launch did/didn’t work, linking to scientific principles—and takes a post-project quiz.\n",
      "- Grades: 40% group build/report, 60% own analysis + quiz.\n",
      "\n",
      "\n",
      "=== ### Summary Table ===\n",
      "\n",
      "| Step                        | Action                                                    |\n",
      "|\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "=== | ===\n",
      "| Individual Accountability   | Short in-class quizzes, explanations, reflections         |\n",
      "| Hybrid Deliverables         | Group AND individual assignment components                |\n",
      "| Role and Peer Assessment    | Tie roles and peer evals to content, not just participation |\n",
      "| Weighted Grading            | Clear split between individual/group grades               |\n",
      "| Post-Project Assessment     | Individual quiz + remediation as needed                   |\n",
      "\n",
      "Final Thought:  \n",
      "Collaboration and accountability aren’t mutually exclusive. Well-structured hybrid systems make teamwork rewarding *and* ensure every student achieves mastery.\n"
     ]
    }
   ],
   "source": [
    "## Without analogical reasoning - raw LLM output\n",
    "result = basic_output_COT(test_q)\n",
    "\n",
    "print(\"\\n=== GPT-4.1 Response WITH COT ===\\n\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a23f11b",
   "metadata": {},
   "source": [
    "#### Without Analogical Reasoning but with Basic Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "98356bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GPT-4.1 Response WITH Prompt Engineering ===\n",
      "\n",
      "You’ve surfaced a classic tension in group work: collaboration boosts engagement, but can mask—or even reduce—individual learning gains. To strike a balance, consider a redesign that weaves together individual accountability and collaborative skill-building. Here are practical-yet-innovative strategies:\n",
      "\n",
      "\n",
      "=== ### 1. Hybrid Grading Models ===\n",
      "\n",
      "- Individual & Group Scores Blend  \n",
      "  Assign a percentage of the project grade to individual contributions (e.g., quiz/performance, reflection, or portfolio) and another to the group product. For instance, 60% individual, 40% group. Make this transparent to students.  \n",
      "  *Why it works:* Students have personal “skin in the game,” incentivizing accountability.\n",
      "\n",
      "- Team Contract with Individual Goals  \n",
      "  At project kickoff, require each student to articulate learning goals and anticipated contributions. Check-in mid-project on these.  \n",
      "  *Why it works:* Promotes self-awareness and specific expectations, which you can reference during assessment.\n",
      "\n",
      "\n",
      "=== ### 2. Intertwined Solo and Group Tasks ===\n",
      "\n",
      "- “Jigsaw” Method Plus Solo Synthesis  \n",
      "  Each student becomes an expert on one aspect, teaches it to the group, then completes a related solo assessment (quiz, short essay) to demonstrate personal mastery.  \n",
      "  *Why it works:* Builds deep understanding and ensures each student internalizes content.\n",
      "\n",
      "- Rotating “Lead Scientist” Roles  \n",
      "  For each major activity, one student is responsible for leading, explaining concepts, and submitting a summary. Rotate this role so all have leadership moments evaluated — both by teacher and group.\n",
      "\n",
      "\n",
      "=== ### 3. Individual Reflection and Metacognition ===\n",
      "\n",
      "- Reflective Journals or Video Diaries  \n",
      "  Students regularly (e.g. after key milestones) reflect on both their and their group’s learning, hurdles, and the application of core concepts, citing specific evidence from the project.  \n",
      "  *Why it works:* Deepens content retention and gives you direct insight into what *each* student has learned.\n",
      "\n",
      "\n",
      "=== ### 4. Low-Stakes, Frequent Individual Checks ===\n",
      "\n",
      "- Mini-Quizzes & Concept Maps  \n",
      "  Short, scaffolded individual assessments at intervals during the project. Consider quick in-class polling, digital quizzes, or having students draw concept maps in response to a prompt.  \n",
      "  *Why it works:* Frequent feedback for both student and teacher, allowing you to catch gaps early.\n",
      "\n",
      "\n",
      "=== ### 5. Transparent Peer Feedback With Evidence ===\n",
      "\n",
      "- \"Show Your Work\" Peer Assessments  \n",
      "  Require students to cite concrete examples (e.g., “Sam researched X and explained Y during the discussion”) in peer evaluations. Pair this feedback with your observational notes.  \n",
      "  *Why it works:* Reduces bias and encourages students to value substantive contributions.\n",
      "\n",
      "\n",
      "=== ### Combining These Approaches ===\n",
      "\n",
      "For best results, layer these methods. For example:  \n",
      "1. Launch with a team contract and division of labor.\n",
      "2. Alternate between group work and individual “jigsaw” quizzes.\n",
      "3. Integrate reflection checkpoints.\n",
      "4. Conclude with both group product and individual synthesis tasks.\n",
      "\n",
      "Bottom line:  \n",
      "Delivering direct, measurable individual assessments at key points within a collaborative structure ensures accountability without sacrificing the social construction of knowledge. Students learn together but must also independently demonstrate mastery—both process and product are valued.\n",
      "\n",
      "Bonus: Involve students in co-designing parts of the system. This fosters buy-in and often surfaces creative accountability ideas you’d never consider alone!\n",
      "\n",
      "References for further reading:  \n",
      "- Slavin, R. E. (1996). Research on Cooperative Learning and Achievement: What We Know, What We Need to Know.  \n",
      "- Oakley, B., Felder, R. M., Brent, R., & Elhajj, I. (2004). Turning Student Groups into Effective Teams.\n",
      "\n",
      "Let me know if you want sample rubrics or quick templates!\n"
     ]
    }
   ],
   "source": [
    "## Without analogical reasoning - raw LLM output\n",
    "result = basic_output_prompt_engin(test_q)\n",
    "\n",
    "print(\"\\n=== GPT-4.1 Response WITH Prompt Engineering ===\\n\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a10ec8-c685-4bd2-810c-10e3ce687a61",
   "metadata": {},
   "source": [
    "## Automatic Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c752ecda-6308-4390-952b-3951023f4554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# function to import the evaluation questions\n",
    "def import_questions(file_name: str) -> list[str]:\n",
    "    with open(file_name, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    problems = [case['problem_description'] for case in data['cases']]\n",
    "    return problems\n",
    "\n",
    "# For each question, get outputs from four/five different agents \n",
    "# function to load the agents \n",
    "def auto_eval(question: str) -> list[str]:\n",
    "    response = []\n",
    "    response.append(analogical_output(question))\n",
    "    response.append(basic_output(question))\n",
    "    response.append(basic_output_COT(question))\n",
    "    response.append(basic_output_prompt_engin(question))\n",
    "    response.append(basic_output_full_COT(question))\n",
    "\n",
    "    return response\n",
    "\n",
    "def auto_eval_batch(questions: list[str]) -> list[list[str]]:\n",
    "    responses = []\n",
    "    for question in questions:\n",
    "        response = auto_eval(question)\n",
    "        responses.append(response)\n",
    "    return responses\n",
    "\n",
    "def llm_judge(user_query: str, response: list[str]) -> pd.DataFrame: \n",
    "    response_1 = response[0]\n",
    "    response_2 = response[1]\n",
    "    response_3 = response[2]\n",
    "    response_4 = response[3]\n",
    "    response_5 = response[4]\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Here is the user query: {user_query}\n",
    "    I will give you 5 sample responses generated using different reasoning methods. I need you to evaluate them side by side on 2 criteria: innovativeness (insightfulness) and practicality. \n",
    "    Score each criterion from 1 to 10, and return the result **strictly in the following JSON format**:\n",
    "\n",
    "    {{\n",
    "      \"Response A\": {{\"Innovativeness\": int, \"Practicality\": int}},\n",
    "      \"Response B\": {{\"Innovativeness\": int, \"Practicality\": int}},\n",
    "      \"Response C\": {{\"Innovativeness\": int, \"Practicality\": int}},\n",
    "      \"Response D\": {{\"Innovativeness\": int, \"Practicality\": int}}\n",
    "      \"Response E\": {{\"Innovativeness\": int, \"Practicality\": int}}\n",
    "    }}\n",
    "\n",
    "    User Query: {user_query}\n",
    "    \n",
    "    Response A: {response[0]}\n",
    "    \n",
    "    Response B: {response[1]}\n",
    "    \n",
    "    Response C: {response[2]}\n",
    "    \n",
    "    Response D: {response[3]}\n",
    "\n",
    "    Response E: {response[3]}\n",
    "    \"\"\"\n",
    "    raw_output = llms.invoke(prompt)\n",
    "    # Extract content from AIMessage\n",
    "    content = raw_output.content if hasattr(raw_output, 'content') else str(raw_output)\n",
    "    \n",
    "    json_start = content.find('{')\n",
    "    json_data = content[json_start:]\n",
    "\n",
    "    try:\n",
    "        scores = json.loads(json_data)\n",
    "        df = pd.DataFrame.from_dict(scores, orient='index')\n",
    "        df.index.name = \"response_label\"\n",
    "        df = df.reset_index()\n",
    "        return df\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(\"JSON parsing failed:\", e)\n",
    "        print(\"Raw LLM output was:\\n\", content)\n",
    "        return None\n",
    "\n",
    "def evaluate_multiple_questions(user_queries: list[str], all_responses: list[list[str]]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    user_queries: list of queries, one per question\n",
    "    all_responses: list of response lists, each list has 4 responses for the corresponding query\n",
    "    lms: the LLM interface with .invoke(prompt)\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "\n",
    "    for i, (query, responses) in enumerate(zip(user_queries, all_responses)):\n",
    "        df = llm_judge(query, responses)  # assume it returns a DataFrame as defined earlier\n",
    "        if df is not None:\n",
    "            df = df.reset_index().rename(columns={\"index\": \"response_label\"})\n",
    "            df['question_id'] = i\n",
    "            results.append(df)\n",
    "\n",
    "    combined_df = pd.concat(results, ignore_index=True)\n",
    "    return combined_df\n",
    "    \n",
    "def find_mean(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    mean_scores = data.groupby(\"response_label\")[[\"Innovativeness\", \"Practicality\"]].mean()\n",
    "    return mean_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dbd7dabc-9004-43b8-a7a0-4f0767881f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_list = import_questions(\"Questions.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "31678251-ffd3-4c9e-ad97-cfc7ce9630e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = auto_eval(questions_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7922dec3-b305-4ec4-902a-9ad42fe1b2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm_judge(questions_list[0], response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7e72321d-3bf6-41e1-8dd5-07ea51b9e71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = auto_eval_batch(questions_list[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fb85cbc8-6f36-4cdf-84e2-df8547e79840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response_label</th>\n",
       "      <th>response_label</th>\n",
       "      <th>Innovativeness</th>\n",
       "      <th>Practicality</th>\n",
       "      <th>question_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Response A</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Response B</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Response C</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Response D</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Response A</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>Response B</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>Response C</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>Response D</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  response_label response_label  Innovativeness  Practicality  question_id\n",
       "0              0     Response A              10             8            0\n",
       "1              1     Response B               6             9            0\n",
       "2              2     Response C               7             9            0\n",
       "3              3     Response D               8             9            0\n",
       "4              0     Response A              10             8            1\n",
       "5              1     Response B               7            10            1\n",
       "6              2     Response C               6             9            1\n",
       "7              3     Response D               8             9            1"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_multiple_questions(questions_list[:2], responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecd02e69-0a11-4f76-9d69-0492a211b6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_innovativeness(data: pd.DataFrame): \n",
    "    # Plot for Innovativeness\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for label in data['response_label'].unique():\n",
    "        subset = data[combined_df['response_label'] == label]\n",
    "        plt.plot(subset['question_id'], subset['Innovativeness'], label=label, marker='o')\n",
    "        plt.title(\"Innovativeness Scores by Response Type\")\n",
    "        plt.xlabel(\"Question ID\")\n",
    "        plt.ylabel(\"Innovativeness Score\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def plot_practicality(data: pd.DataFrame):\n",
    "    # Plot for Practicality\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for label in data['response_label'].unique():\n",
    "        subset = data[combined_df['response_label'] == label]\n",
    "        plt.plot(subset['question_id'], subset['Practicality'], label=label, marker='o')\n",
    "    plt.title(\"Practicality Scores by Response Type\")\n",
    "    plt.xlabel(\"Question ID\")\n",
    "    plt.ylabel(\"Practicality Score\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539df397-dcd8-43f2-9001-ae8de9903e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify questions\n",
    "# add full_COT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
